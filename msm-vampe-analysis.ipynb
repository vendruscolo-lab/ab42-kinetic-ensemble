{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "In this notebook we perform most of the analysis and all the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gc\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "import os\n",
    "from typing import List, Tuple, Sequence\n",
    "import warnings\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, ticker\n",
    "from matplotlib.colors import ListedColormap\n",
    "from msmtools.analysis import stationary_distribution, mfpt\n",
    "from msmtools.flux import tpt\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import pyemma as pe\n",
    "from scipy.linalg import eig\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot settings\n",
    "sns.set_palette(\"husl\", 8)\n",
    "rc(\"font\", **{\"family\": \"Helvetica\",\n",
    "              \"sans-serif\": [\"Helvetica\"]})\n",
    "rc(\"svg\", **{\"fonttype\": \"none\"})\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "hv.extension(\"matplotlib\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "The version of Keras we're using unfortunately doesn't have `restore_best_weights` implemented, so I copied this from a newer version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(source: np.ndarray, lengths: List[int]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Takes an array and returns a list of arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    source\n",
    "        Array to be unflattened.\n",
    "    lengths\n",
    "        List of integers giving the length of each subarray.\n",
    "        Must sum to the length of source.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unflat\n",
    "        List of arrays.\n",
    "    \n",
    "    \"\"\"\n",
    "    conv = []\n",
    "    lp = 0\n",
    "    for arr in lengths:\n",
    "        arrconv = []\n",
    "        for le in arr:\n",
    "            arrconv.append(source[lp:le + lp])\n",
    "            lp += le\n",
    "        conv.append(arrconv)\n",
    "    ccs = list(itertools.chain(*conv))\n",
    "    return ccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_lengths(flatlengths: Sequence[int], shapes: Sequence[int]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Takes a list of lengths and returns a list of lists of lengths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    flatlengths\n",
    "        List of lengths\n",
    "    shapes\n",
    "        List of shapes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lengths\n",
    "        List of lists of lengths\n",
    "    \n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "    i = 0\n",
    "    for n in shapes:\n",
    "        arr = []\n",
    "        for _ in range(n):\n",
    "            arr.append(flatlengths[i])\n",
    "            i += 1\n",
    "        lengths.append(arr)\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triu_inverse(x: np.ndarray, n: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts flattened upper-triangular matrices into full symmetric matrices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        Flattened matrices\n",
    "    n\n",
    "        Size of the n * n matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mat\n",
    "        Array of shape (length, n, n)\n",
    "    \n",
    "    \"\"\"\n",
    "    length = x.shape[0]\n",
    "    mat = np.zeros((length, n, n))\n",
    "    a, b = np.triu_indices(n, k=1)\n",
    "    mat[:, a, b] = x\n",
    "    mat += mat.swapaxes(1, 2)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statdist(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the equilibrium distribution of a transition matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        Row-stochastic transition matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mu\n",
    "        Stationary distribution, i.e. the left\n",
    "        eigenvector associated with eigenvalue 1.\n",
    "    \n",
    "    \"\"\"\n",
    "    ev, evec = eig(X, left=True, right=False)\n",
    "    mu = evec.T[ev.argmax()]\n",
    "    mu /= mu.sum()\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_sorting(rmsd: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sorts a matrix of RMSD values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rmsd\n",
    "        Array of shape (n, n) with interstate differences.\n",
    "        This matrix should be acquired by calculating the RMSD\n",
    "        between a reference decomposition and a trial decomposition.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sorter\n",
    "        Array of sorted indices\n",
    "    \n",
    "    \"\"\"\n",
    "    size = rmsd.shape[0]\n",
    "    \n",
    "    # -1 is not yet assigned\n",
    "    sorter = np.full(size, -1, dtype=np.int8)\n",
    "    sorted_idx = rmsd.argsort(axis=None)\n",
    "    \n",
    "    # We walk through the sorted RMSDs from low to high and assign the 2D indices.\n",
    "    # If one is already assigned, we just jump to the next one, which will be the next-lowest RMSD.\n",
    "    for i, j in zip(*np.unravel_index(sorted_idx, (size, size))):\n",
    "        if sorter[i] < 0 and j not in sorter:\n",
    "            sorter[i] = j\n",
    "    return sorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_traj(idx: int, lengths: List[int]) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Given a trajectory index, find the round and trajectory file number.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    idx\n",
    "        Trajectory index\n",
    "    lengths\n",
    "        Length of each round\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    round, number\n",
    "        Simulation round and corresponding trajectory number\n",
    "    \n",
    "    \"\"\"\n",
    "    lengths = np.array(lengths)\n",
    "    lcs = lengths.cumsum()\n",
    "    if idx >= lengths[0]:\n",
    "        nr = idx - lcs[lcs < idx][-1]\n",
    "        i = np.arange(len(lengths))[lcs > idx][0]\n",
    "    else:\n",
    "        i, nr = 0, idx\n",
    "    return i, nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msmtools doesn't think our matrices are normalized enough,\n",
    "# so we normalize until we hit the default tolerance requirement.\n",
    "def renormalize(mat, tol=1e-12, axis=1):\n",
    "    n = mat.shape[0]\n",
    "    while abs(np.ones(n) - mat.sum(axis=axis)).max() > tol:\n",
    "        mat = abs(mat) / abs(mat).sum(axis=axis)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_its(its, lags, dt=1.0):\n",
    "    multi = its.ndim == 3\n",
    "    nits, nlags = its.shape[-2], its.shape[-1]\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    if multi:\n",
    "        itsm = its.mean(axis=0)\n",
    "        cfl, cfu = np.percentile(its, q=(2.5, 97.5), axis=0)\n",
    "    else:\n",
    "        itsm = its\n",
    "    \n",
    "    ax.semilogy(lags * dt, lags * dt, color=\"k\")\n",
    "    ax.fill_between(lags * dt, ax.get_ylim()[0] * np.ones(len(lags)),\n",
    "                    lags * dt, color=\"k\", alpha=0.2)\n",
    "    for i in range(nits):\n",
    "        ax.plot(lags * dt, itsm[i], marker=\"o\",\n",
    "                    linestyle=\"dashed\", linewidth=1.5, color=colors[-(i + 2)])\n",
    "        ax.plot(lags * dt, itsm[i], marker=\"o\", linewidth=1.5, color=colors[-(i + 2)])\n",
    "        if multi:\n",
    "            ax.fill_between(lags * dt, cfl[i], cfu[i],\n",
    "                            interpolate=True, color=colors[-(i + 2)], alpha=0.2)\n",
    "    loc = ticker.LogLocator(base=10.0, subs=(0.2, 0.4, 0.6, 0.8), numticks=12)\n",
    "    ax.set_ylim(1, 100000)\n",
    "    ax.set_yticks(10 ** np.arange(6))\n",
    "    ax.yaxis.set_minor_locator(loc)\n",
    "    ax.yaxis.set_minor_formatter(ticker.NullFormatter())\n",
    "    ax.set_xlabel(r\"$\\tau$ [ns]\", fontsize=24)\n",
    "    ax.set_ylabel(r\"$t_i$ [ns]\", fontsize=24)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    sns.despine(ax=ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ck(cke, ckp, lag):\n",
    "    multi = cke.ndim == 4\n",
    "    n = cke.shape[-2]\n",
    "    steps = cke.shape[-1]\n",
    "    \n",
    "    if multi:\n",
    "        ckem = cke.mean(axis=0)\n",
    "        ckpm = ckp.mean(axis=0)\n",
    "        ckep = np.percentile(cke, q=(2.5, 97.5), axis=0)\n",
    "        ckpp = np.percentile(ckp, q=(2.5, 97.5), axis=0)\n",
    "    else:\n",
    "        ckem = cke\n",
    "        ckpm = ckp\n",
    "    \n",
    "    fig, axes = plt.subplots(n, n, figsize=(4 * n, 4 * n), sharex=True)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax = axes[i, j]\n",
    "            x = np.arange(0, steps * lag, lag)\n",
    "            if multi:\n",
    "                ax.errorbar(x, ckpm[i, j], yerr=[ckpm[i, j] - ckpp[0, i, j], ckpp[1, i, j] - ckpm[i, j]],\n",
    "                            linewidth=2, elinewidth=2)\n",
    "                ax.fill_between(x, ckep[0, i, j], ckep[1, i, j],\n",
    "                                alpha=0.2, interpolate=True, color=colors[1])\n",
    "            else:\n",
    "                ax.plot(x, ckpm[i, j], linestyle=\"-\", color=colors[0], linewidth=2)\n",
    "            ax.plot(x, ckem[i, j], linestyle=\"--\", color=colors[1], linewidth=2)\n",
    "            \n",
    "            if i == j:\n",
    "                ax.set_ylim(0.78, 1.02)\n",
    "                ax.text(0, 0.8, r\"{0} $\\to$ {1}\".format(i, j), fontsize=24, verticalalignment=\"center\")\n",
    "            else:\n",
    "                ax.set_ylim(-0.02, 0.22)\n",
    "                ax.text(0, 0.2, r\"{0} $\\to$ {1}\".format(i, j), fontsize=24, verticalalignment=\"center\")\n",
    "            ax.set_xticks(np.arange(0, steps * lag, lag), minor=True)\n",
    "            ax.set_xticks(np.arange(0, steps * lag, 2 * lag))\n",
    "            ax.set_xticklabels((np.arange(0, steps * lag, 2 * lag) * dt).astype(int))\n",
    "            ax.tick_params(labelsize=24)\n",
    "    fig.text(0.5, 0.01 * 1.5 * n, r\"$\\tau$ [ns]\", ha=\"center\", fontsize=24)\n",
    "    fig.text(0.01 * 1.5 * n, 0.5, r\"$P$\", va=\"center\", rotation=\"vertical\", fontsize=24)\n",
    "    fig.subplots_adjust(wspace=0.25)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in five rounds of 1024 simulations each, totalling 5119 runs (one simulation failed to run) at 278 K in the $NVT$ ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs = sorted(glob(\"trajectories/red/r?/traj*.xtc\"))\n",
    "top = \"trajectories/red/topol.gro\"\n",
    "KBT = 2.311420 # 278 K\n",
    "traj_rounds = [1024, 2047, 3071, 4095, 5119]\n",
    "nres = 42\n",
    "\n",
    "# This is only really necessary for the residues in the plots\n",
    "topo = md.load_topology(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat = pe.coordinates.featurizer(top)\n",
    "feat.add_residue_mindist()\n",
    "inpcon = pe.coordinates.source(trajs, feat)\n",
    "\n",
    "# Switch for full version:\n",
    "# lengths = sort_lengths(inpcon.trajectory_lengths(), [1024, 1023, 1024, 1024, 1024])\n",
    "lengths = [inpcon.trajectory_lengths()]\n",
    "nframes = inpcon.trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trajectories: {0}\".format(len(trajs)))\n",
    "print(\"Frames: {0}\".format(nframes))\n",
    "print(\"Time: {0:5.3f} µs\".format(inpcon.trajectory_lengths().sum() * 0.00025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental\n",
    "Chemical shifts were acquired at 278 K. We have C$\\alpha$, C$\\beta$, C', H$\\alpha$, HN, and N shifts available. The chemical shifts were backcalculated from the trajectory using Camshift [2], the errors are given by the test-set errors.\n",
    "\n",
    "[2]\tKohlhoff, K. J., Robustelli, P., Cavalli, A., Salvatella, X. & Vendruscolo, M. Fast and accurate predictions of protein NMR chemical shifts from interatomic distances. J. Am. Chem. Soc. 131, 13894–13895 (2009)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csraw = sorted(glob(\"trajectories/red/r?cs/cs*.dat\"))\n",
    "cs = [np.loadtxt(f)[:, 1:207] for f in csraw]\n",
    "csexp = np.loadtxt(csraw[0])[0, 207:]\n",
    "\n",
    "err = {\"NH\": 3.01, \"HN\": 0.56, \"HA\": 0.28, \"CA\": 1.3, \"CB\": 1.36, \"CO\": 1.38}\n",
    "nerr = {\"CA\": 37, \"CB\": 27, \"CO\": 37, \"HA\": 31, \"HN\": 37, \"NH\": 37}\n",
    "experr = np.array([err[\"CA\"]] * 37 + [err[\"CB\"]] * 27 + [err[\"CO\"]] * 37 +\n",
    "                  [err[\"HA\"]] * 31 + [err[\"HN\"]] * 37 + [err[\"NH\"]] * 37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAMPNet\n",
    "VAMPNet[1] is composed of two lobes, one reading the system features $\\mathbf{x}$ at a timepoint $t$ and the other after some lag time $\\tau$. In this case the network reads all minimum inter-residue distances (780 values) and sends them through 5 layers with 256 nodes each. The final layer uses between 2 and 8 *softmax* outputs to yield a state assignment vector $\\chi: \\mathbb{R}^m \\to \\Delta^{n}$ where $\\Delta^{n} = \\{ s \\in \\mathbb{R}^n \\mid 0 \\le s_i \\le 1, \\sum_i^n s_i = 1 \\}$ representing the probability of a state assignment. One lobe thus transforms a system state into a state occupation probability. We can also view this value as a kind of reverse ambiguity, i.e. how sure the network is that the system is part of a certain cluster. These outputs are then used as the input for the VAMP scoring function. We use the new enhanced version with physical constraints[2], particularly the ones for positive entries and reversibility.\n",
    "\n",
    "[1] Mardt, A., Pasquali, L., Wu, H. & Noé, F. VAMPnets for deep learning of molecular kinetics. Nat Comms 1–11 (2017). doi:10.1038/s41467-017-02388-1\n",
    "\n",
    "[2] Mardt, A., Pasquali, L., Noé, F. & Wu, H. Deep learning Markov and Koopman models with physical constraints. arXiv:1912.07392 [physics] (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) for the neural network, but remove the 2nd and 3rd off-diagonals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"intermediate/mindist-780-red.npy\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing file for ensemble: {0}\".format(filename))\n",
    "    input_flat = np.load(filename)\n",
    "else:\n",
    "    print(\"No mindist file for ensemble, calculating from scratch...\")\n",
    "    input_flat = np.vstack(inpcon.get_output())\n",
    "    np.save(filename, input_flat)\n",
    "input_data = unflatten(input_flat, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the full minimum inter-residue distances for some analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpairs = np.asarray(list(itertools.combinations(range(nres), 2)))\n",
    "filename = \"intermediate/mindist-all-red.npy\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing file for ensemble: {0}\".format(filename))\n",
    "    mindist_flat = np.load(filename)\n",
    "else:\n",
    "    print(\"No mindist file for ensemble, calculating from scratch...\")\n",
    "    feat = pe.coordinates.featurizer(top)\n",
    "    feat.add_residue_mindist(residue_pairs=allpairs)\n",
    "    inpmindist = pe.coordinates.source(trajs, feat)\n",
    "    mindist_flat = np.vstack(inpmindist.get_output())\n",
    "    np.save(filename, mindist_flat)\n",
    "mindist = unflatten(mindist_flat, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters\n",
    "To allow for a larger hyperparameter search space, we use the self-normalizing neural network approach by Klambauer *et al.* [2], thus using SELU units, `AlphaDropout` and normalized `LeCun` weight initialization. The other hyperparameters are defined at the beginning of this notebook.\n",
    "\n",
    "[2] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-Normalizing Neural Networks. arXiv.org cs.LG, (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 50                         # Lag time\n",
    "n_dims = input_data[0].shape[1]  # Input dimension\n",
    "nres = 42                        # Number of residues\n",
    "dt = 0.25                        # Trajectory timestep in ns\n",
    "steps = 6                        # CK test steps\n",
    "bs_frames = 900000               # Number of frames in the bootstrap sample\n",
    "attempts = 20                    # Number of times to run\n",
    "\n",
    "outsizes = np.array([2, 3, 4, 5, 6])\n",
    "lags = np.array([1, 2, 5, 10, 20, 50, 100])\n",
    "\n",
    "# Comment for full version:\n",
    "bs_frames = nframes\n",
    "attempts = 2\n",
    "outsizes = np.array([4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Model validation\n",
    "We load the previously trained neural network models and calculated implied timescales, Chapman-Kolmogorov test, and Koopman operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State sorting\n",
    "As every run of the neural network will generate a different ordering of (mostly the same) classes, we need to reorder them to be internally consistent. We do this by calculating the root-mean-square deviation between all average inter-residue minimum distance matrices for the individual states and matching the lowest values. We also make sure the sorting is unique, i.e. all states are accounted for, even if a certain RMSD is lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to run this if you have the data.hdf5 file from the repo\n",
    "with h5py.File(\"intermediate/data.hdf5\") as store:\n",
    "    for n in outsizes:\n",
    "        # First attempt is the reference dataset\n",
    "        ref_group = store[\"red/{0}/{1}\".format(0, n)]\n",
    "        ref = ref_group[\"full\"][:, :n]\n",
    "        \n",
    "        # This is the average contact map for every\n",
    "        # state for the first training attempt\n",
    "        conwref = (ref / ref.sum(axis=0)).T @ mindist_flat\n",
    "        conwrefbt = np.broadcast_to(conwref, (n, n, mindist_flat.shape[1])).swapaxes(0, 1)\n",
    "        ref_group.create_dataset(\"full_sorted\", data=ref)\n",
    "        ref_group.create_dataset(\"bootstrap_sorted\", data=ref_group[\"bootstrap\"][:, :n])\n",
    "        \n",
    "        for i in range(1, attempts):\n",
    "            print(\"Processing n={0} i={1}...\".format(n, i), end=\"\\r\")\n",
    "            group = store[\"red/{0}/{1}\".format(i, n)]\n",
    "            pf = group[\"full\"][:, :n]\n",
    "            \n",
    "            # Average contact map for each state for attempt `i`\n",
    "            conw = (pf / pf.sum(axis=0)).T @ mindist_flat\n",
    "            conwbt = np.broadcast_to(conw, (n, n, mindist_flat.shape[1]))\n",
    "            \n",
    "            # Calculate the RMSD to our reference attempt\n",
    "            rmsd = np.sqrt(((conwbt - conwrefbt) ** 2).sum(axis=-1))\n",
    "            \n",
    "            # Get a permutation vector and store it\n",
    "            sorter = unique_sorting(rmsd)\n",
    "            group.create_dataset(\"sorter\", data=sorter)\n",
    "            group.create_dataset(\"full_sorted\", data=pf[:, sorter])\n",
    "            group.create_dataset(\"bootstrap_sorted\", data=group[\"bootstrap\"][:, :n][:, sorter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data\n",
    "with h5py.File(\"intermediate/data.hdf5\") as store:\n",
    "    with h5py.File(\"intermediate/data-mini.hdf5\", \"w\") as write:\n",
    "        for k in (\"red\", \"ox\"):\n",
    "            ens = write.create_group(k)\n",
    "            for i in [0, 1]:\n",
    "                att = ens.create_group(str(i))\n",
    "                for n in [4]:\n",
    "                    out = att.create_group(str(n))\n",
    "                    for key in (\"k\", \"its\", \"cke\", \"ckp\"):\n",
    "                        out.create_dataset(key, data=store[\"{0}/{1}/{2}/{3}\".format(k, i, n, key)][:], compression=9)\n",
    "                    for key in (\"mu\", \"bootstrap\", \"full\", \"bootstrap_sorted\", \"full_sorted\", \"sorter\"):\n",
    "                        out.create_dataset(key, data=store[\"{0}/{1}/{2}/{3}\".format(k, i, n, key)][:sum(lengths[0][:6])], compression=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorters = {n: np.empty((attempts, n), dtype=int) for n in outsizes}\n",
    "pfs = {n: np.empty((attempts, nframes, n)) for n in outsizes}\n",
    "pfsn = {n: np.empty((attempts, nframes, n)) for n in outsizes}\n",
    "pfs_boot = {n: np.empty((attempts, bs_frames, n)) for n in outsizes}\n",
    "pfsn_boot = {n: np.empty((attempts, bs_frames, n)) for n in outsizes}\n",
    "koops = {n: np.empty((attempts, n, n)) for n in outsizes}\n",
    "pis = {n: np.empty((attempts, n)) for n in outsizes}\n",
    "with h5py.File(\"intermediate/data-mini.hdf5\") as read:\n",
    "    store = read[\"red\"]\n",
    "    for i in range(attempts):\n",
    "        for n in outsizes:\n",
    "            sorters[n][i] = store[\"{0}/{1}/sorter\".format(i, n)][:]\n",
    "            pfs[n][i] = store[\"{0}/{1}/full_sorted\".format(i, n)][:]\n",
    "            pfsn[n][i] = pfs[n][i] / pfs[n][i].sum(axis=0)\n",
    "            pfs_boot[n][i] = store[\"{0}/{1}/bootstrap_sorted\".format(i, n)][:]\n",
    "            pfsn_boot[n][i] = pfs_boot[n][i] / pfs_boot[n][i].sum(axis=0)\n",
    "            koops[n][i] = store[\"{0}/{1}/k\".format(i, n)][:][sorters[n][i]][:, sorters[n][i]]\n",
    "            pis[n][i] = statdist(koops[n][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate an equilibrium weighting vector, analogous to discrete Markov models:\n",
    "\n",
    "\\begin{equation}\n",
    "w_t = \\frac{\\langle \\chi(\\mathbf{x}_t) | \\mathbf{\\pi} \\rangle}{\\sum_t^N \\langle \\chi(\\mathbf{x}_t) | \\mathbf{\\pi} \\rangle}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "for n in outsizes:\n",
    "    weights[n] = np.empty((attempts, nframes))\n",
    "    for i in range(attempts):\n",
    "        w = pfs[n][i] @ pis[n][i]\n",
    "        weights[n][i] = w / w.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global sorting\n",
    "Because state assignments will not only be different within a choice of number of states, but also globally, we sort the states based on the previous (coarser) state decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods = {}\n",
    "for n in outsizes[1:]:\n",
    "    prods[n] = np.einsum(\"jk,jl->kl\", pfs[n - 1].mean(axis=0), pfs[n].mean(axis=0))\n",
    "\n",
    "global_sorter = {2: np.arange(2, dtype=int)}\n",
    "for n in outsizes[1:]:\n",
    "    global_sorter[n] = np.full(n, -1, dtype=int)\n",
    "    mass = prods[n][global_sorter[n - 1]]\n",
    "    for i in range(n - 1):\n",
    "        sort = mass[i].argsort()[::-1]\n",
    "        for j in range(n):\n",
    "            if sort[j] not in global_sorter[n]:\n",
    "                global_sorter[n][i] = sort[j]\n",
    "                break\n",
    "    global_sorter[n][-1] = [i for i in range(n) if i not in global_sorter[n][:-1]][0]\n",
    "\n",
    "if outsizes.shape[0] == 1:\n",
    "    n = outsizes[0]\n",
    "    global_sorter[n] = np.arange(n, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implied timescales\n",
    "The implied timescales are computed from the eigenvalues $\\lambda_i$ of the Koopman operator $\\mathbf{K}$ and the selected lag time $\\tau$:\n",
    "$$ t_i = \\frac{-\\tau}{\\log | \\lambda_i(\\tau) |} $$\n",
    "We compute the implied timescales for several lag times $\\tau$. Ideally, we want to choose $\\tau$ so that we're in a regime where the timescales are mostly independent of $\\tau$. This is the case for lag times longer than $\\tau = 12.5\\,\\mathrm{ns}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"intermediate/data-mini.hdf5\") as store:\n",
    "    for n in outsizes:\n",
    "        its = np.stack(store[\"red/{0}/{1}/its\".format(i, n)] for i in range(attempts))\n",
    "        fig = plot_its(its, lags, dt=dt)\n",
    "        plt.savefig(\"figs/its-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "        plt.savefig(\"figs/its-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapman-Kolmogorov Test\n",
    "Now that we have found a Koopman operator $\\mathbf{K}$, how do we know if it adequately describes the dynamical system?\n",
    "\n",
    "There are a number of tests we can perform to ensure we have found a good approximation. One of the more stringent ones is the Chapman Kolmogorov (CK-test). It is based on the assumption that multiple applications of our operator to our system state at time $t$ should be equivalent to estimating the operator at a longer lag time $\\tau$:\n",
    "\n",
    "$$ \\mathbf{K}(\\tau)^n \\approx \\mathbf{K}(n\\tau) $$\n",
    "\n",
    "I.e., multiple applications of the operator should produce the same result as using an operator with a multiple of a certain lag time, within an error. This is a more stringent test of model quality than the implied timescales test described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"intermediate/data-mini.hdf5\") as store:\n",
    "    for n in outsizes:\n",
    "        cke = np.stack(store[\"red/{0}/{1}/cke\".format(i, n)][:][sorters[n][i]][:, sorters[n][i]]\n",
    "                       for i in range(attempts))[:, global_sorter[n]][:, :, global_sorter[n]]\n",
    "        ckp = np.stack(store[\"red/{0}/{1}/ckp\".format(i, n)][:][sorters[n][i]][:, sorters[n][i]]\n",
    "                       for i in range(attempts))[:, global_sorter[n]][:, :, global_sorter[n]]\n",
    "        fig = plot_ck(cke, ckp, lag=50)\n",
    "        plt.savefig(\"figs/ck-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "        plt.savefig(\"figs/ck-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental backcalculation\n",
    "We calculate the root-mean-square deviation of backcalculated values (using CamShift) to experimental observables for the whole ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"CA\": ((0, 37), (30, 70)),\n",
    "    \"CB\": ((37, 64), (10, 70)),\n",
    "    \"CO\": ((64, 101), (165, 185)),\n",
    "    \"HA\": ((101, 132), (0, 6)),\n",
    "    \"HN\": ((132, 169), (0, 12)),\n",
    "    \"NH\": ((169, 206), (90, 140))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "csm = np.einsum(\"ij,jk->ik\", weights[n], np.vstack(cs))\n",
    "sqdev = (csm - csexp) ** 2\n",
    "rmsd = {k: np.sqrt(sqdev[:, v[0][0]:v[0][1]].mean(axis=1)) for k, v in labels.items()}\n",
    "rmsderr = [(k, rmsd[k].mean(axis=0), np.percentile(rmsd[k], (2.5, 97.5))) for k in sorted(labels.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(np.arange(6), [d[1] for d in sorted(tuple(err.items()))], capsize=8,\n",
    "       color=colors[5], alpha=0.5, label=\"Camshift Error\")\n",
    "ax.bar(np.arange(6), [d[1] for d in rmsderr],\n",
    "       capsize=8, color=colors[5], label=\"Simulation Error\")\n",
    "ax.set_xticks(np.arange(6))\n",
    "ax.set_xticklabels([d[0] for d in rmsderr])\n",
    "ax.set_ylabel(\"RMSD [ppm]\", fontsize=24)\n",
    "ax.set_ylim(0, 3.5)\n",
    "ax.tick_params(axis=\"x\", length=0, pad=10, labelsize=24)\n",
    "ax.tick_params(axis=\"y\", labelsize=24)\n",
    "sns.despine(ax=ax)\n",
    "ax.legend(fontsize=16)\n",
    "plt.savefig(\"figs/rmsd-tot.pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.savefig(\"figs/rmsd-tot.svg\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "We would ideally like to see how converged our ensemble is with respect to the timescales and stationary distribution given by our model. We thus build trial models with different numbers of trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "k_conv = np.load(\"intermediate/k-conv-red-{0}.npy\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timescales\n",
    "We can examine how the timescales change when estimating with more trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its_conv = np.empty((len(traj_rounds), attempts, n - 1))\n",
    "for j, _ in enumerate(traj_rounds):\n",
    "    for i in range(attempts):\n",
    "        ev, _ = eig(k_conv[j, i], left=True, right=False)\n",
    "        order = np.abs(ev).argsort()[::-1]\n",
    "        its_conv[j, i] = -50 * dt / np.log(np.real(ev[order][1:]))\n",
    "its_conv_m = its_conv.mean(axis=1)\n",
    "its_conv_p = np.percentile(its_conv, (2.5, 97.5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.array(traj_rounds)\n",
    "for i in range(n - 1):\n",
    "    ax.semilogy(x, its_conv_m[:, i], color=colors[i + 4], linewidth=2, marker=\"o\")\n",
    "    ax.fill_between(x, its_conv_p[0, :, i], its_conv_p[1, :, i],\n",
    "                    interpolate=True, color=colors[i + 4], alpha=0.2)\n",
    "\n",
    "loc = ticker.LogLocator(base=10.0, subs=(0.2, 0.4, 0.6, 0.8), numticks=12)\n",
    "ax.set_ylim(1, 100000)\n",
    "ax.set_yticks(10 ** np.arange(6))\n",
    "ax.yaxis.set_minor_locator(loc)\n",
    "ax.yaxis.set_minor_formatter(ticker.NullFormatter())\n",
    "ax.set_xticks(x)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(\"# Trajectories\", fontsize=24)\n",
    "ax.set_ylabel(\"$t$\", fontsize=24)\n",
    "sns.despine(ax=ax)\n",
    "plt.savefig(\"figs/its-conv-coarse-{0}.pdf\".format(n), transparent=True, bbox_inches=\"tight\")\n",
    "plt.savefig(\"figs/its-conv-coarse-{0}.svg\".format(n), transparent=True, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metainference ensemble\n",
    "#### Radius of gyration\n",
    "We calculate the radius of gyration $R_\\mathrm{gyr}$ of a previously performed metainference metadynamics simulation [3] to compare to our kinetic ensemble:\n",
    "\n",
    "[3] Heller, G. T. et al. Small molecule sequestration of amyloid-β as a drug discovery strategy for Alzheimer’s disease. bioRxiv 729392 (2019) doi:10.1101/729392."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pe.coordinates.featurizer(top)\n",
    "feat.add_custom_func(lambda t: md.compute_rg(t).astype(np.float32).reshape(-1, 1), 1)\n",
    "inp = pe.coordinates.source(traj_mi, feat)\n",
    "gyrmi = np.vstack(inp.get_output())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gyrs_mi, _ = np.histogram(\n",
    "    gyrmi.flatten(), bins=nbins,\n",
    "    range=(xmin, xmax), weights=weights_mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State decomposition\n",
    "By looking at how states split when choosing finer state decompositions, we can indirectly get a feeling for the spectral gaps, and also see the consistency of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods = {}\n",
    "for n in outsizes[1:]:\n",
    "    prods[n] = np.einsum(\"jk,jl->kl\", pfs[n - 1].mean(axis=0)[:, global_sorter[n - 1]],\n",
    "                         pfs[n].mean(axis=0)[:, global_sorter[n]])\n",
    "\n",
    "# This is just formatting for holoviews\n",
    "prods_format = {}\n",
    "for n in outsizes[1:]:\n",
    "    prods_format[n] = {}\n",
    "    for u in range(n - 1):\n",
    "        for v in range(n):\n",
    "            prods_format[n][(u, v)] = prods[n][u, v]\n",
    "            \n",
    "prods_format = {n: {(int(\"{0}{1}\".format(n - 1, u)),\n",
    "                     int(\"{0}{1}\".format(n, v))): val\n",
    "                    for (u, v), val in prods_format[n].items()}\n",
    "                for n in outsizes[1:]}\n",
    "\n",
    "sankeydata = sorted(list(itertools.chain(\n",
    "    *[[(u, v, val) for (u, v), val in prods_format[n].items()]\n",
    "      for n in outsizes[1:]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = ListedColormap(list(itertools.chain(*[sns.color_palette(\"husl\", 8)[:n] for n in outsizes])))\n",
    "sank = hv.Sankey(sankeydata)\n",
    "sank = sank.options(node_width=30, cmap=cmap, edge_alpha=0.35, node_size=0.1, edge_linewidth=0, node_linewidth=0)\n",
    "fig = hv.render(sank)\n",
    "\n",
    "ax = fig.axes[0]\n",
    "for t in ax.texts:\n",
    "    text = t.get_text()\n",
    "    t.set_text(text[1])\n",
    "    t.set_fontsize(16)\n",
    "fig.set_dpi(600)\n",
    "fig.savefig(\"figs/sankey-ab-6.pdf\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "### TICA\n",
    "Time-lagged independent component analysis is a special case of Koopman operator estimation using a linear projection [4]. We solve the following generalized eigenvalue problem:\n",
    "\n",
    "$$ \\mathbf{C}_{01}v = \\lambda \\mathbf{C}_{00} v $$\n",
    "\n",
    "The eigenvectors encode the slowest dynamics of the system, and we use them as a convenient visualization technique.\n",
    "\n",
    "[4]\tPérez-Hernández, G., Paul, F., Giorgino, T., De Fabritiis, G. & Noé, F. Identification of slow molecular order parameters for Markov model construction. The Journal of Chemical Physics 139, 015102–14 (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticacon = pe.coordinates.tica(mindist, lag=4, dim=-1, kinetic_map=True)\n",
    "ticscon = ticacon.get_output()\n",
    "ycon = np.vstack(ticscon)\n",
    "\n",
    "print(\"tIC Dimensions: {0}\".format(ycon.shape[1]))\n",
    "print(\"Required dimensions for 90 %: {0}\".format(ticacon.cumvar[ticacon.cumvar < 0.9].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Free energy surface\n",
    "We also show the free energy surface projected onto the two slowest tICs in the form of a kernel density estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gaussian_kde(ycon[::10, :2].T)\n",
    "xmin, ymin, *_ = ycon.min(axis=0)\n",
    "xmax, ymax, *_ = ycon.max(axis=0)\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "posi = np.vstack((X.ravel(), Y.ravel()))\n",
    "Z = kernel(posi).reshape(X.shape)\n",
    "mat = np.rot90(Z.copy())\n",
    "mat[mat < 0.01] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "cmap = \"plasma\"\n",
    "F = -KBT * np.log(Z)\n",
    "F -= F.min()\n",
    "ax.contourf(X, Y, F, np.arange(0.0, 10, 1), cmap=cmap)\n",
    "ax.contour(X, Y, F, np.arange(0.0, 10, 1), cmap=cmap, linewidth=10)\n",
    "ax.tick_params(labelsize=24)\n",
    "ax.set_xlabel(r\"tIC 1\", fontsize=24, labelpad=10)\n",
    "ax.set_ylabel(r\"tIC 2\", fontsize=24, labelpad=10)\n",
    "sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contact maps\n",
    "We calculate contact maps from inter-residue minimum distance matrices, using the bootstrapped state assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.8\n",
    "inds = unflatten(np.arange(nframes).reshape(-1, 1), lengths)\n",
    "contacts = {n: np.empty((attempts, n, nres, nres)) for n in outsizes}\n",
    "for i in range(attempts):\n",
    "    generator = DataGenerator.from_state(inds, \"models/model-idx-red-{0}.hdf5\".format(i))\n",
    "    idx = generator(n=2, lag=20).trains[0].flatten().astype(int)\n",
    "    for n in outsizes:\n",
    "        print(\"Processing n={0} i={1}...\".format(n, i), end=\"\\r\")\n",
    "        con = (np.einsum(\"jk,jl->kl\", pfs[n][i, idx], (mindist_flat[idx] < cutoff)) /\n",
    "               pfs[n][i, idx].sum(axis=0).reshape(-1, 1))\n",
    "        contacts[n][i] = np.asarray([triu_inverse(con[j], nres)[0] for j in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a more stringent cutoff of 0.45 nm to compare to Meng's ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.45\n",
    "n = 4\n",
    "inds = unflatten(np.arange(nframes).reshape(-1, 1), lengths)\n",
    "enscon = np.empty((attempts, nres, nres))\n",
    "for i in range(attempts):\n",
    "    generator = DataGenerator.from_state(inds, \"models/model-idx-red-{0}.hdf5\".format(i))\n",
    "    idx = generator(n=2, lag=20).trains[0].flatten().astype(int)\n",
    "    print(\"Processing i={0}...\".format(i), end=\"\\r\")\n",
    "    con = np.einsum(\"j,jk->k\", weights[n][i, idx] / weights[n][i, idx].sum(), (mindist_flat[idx] < cutoff))\n",
    "    enscon[i] = triu_inverse(con, nres)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n",
    "for ax, dat, title in zip(axes.flatten(),\n",
    "                          [enscon.mean(axis=0), enscon.std(axis=0)],\n",
    "                          [\"$P$\", \"$\\sigma(P)$\"]):\n",
    "    cmap = sns.light_palette(colors[4], as_cmap=True)\n",
    "    im = ax.matshow(dat, vmin=0.0, vmax=1.0, interpolation=\"nearest\", cmap=cmap)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    ax.set_xticks(np.arange(0, 50, 10))\n",
    "    ax.set_xticks(np.arange(0, 42, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(0, 50, 10))\n",
    "    ax.set_yticks(np.arange(0, 42, 1), minor=True)\n",
    "    ax.set_xticklabels(np.arange(0, 50, 10))\n",
    "    ax.set_xlim(0, 41)\n",
    "    ax.set_ylim(41, 0)\n",
    "    ax.tick_params(labelsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ASP1 - ALA42 contact ({0:4.2f} nm) probability: {1:4.4f} +/- {2:4.4f}\".format(\n",
    "    cutoff, enscon.mean(axis=0)[0, 41], enscon.std(axis=0)[0, 41]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary structure\n",
    "Secondary structure assignment with DSSP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom func makes this a lot easier\n",
    "dssptable = str.maketrans(\"HBEGITS \", \"01234567\")\n",
    "def dssp_enc(traj):\n",
    "    raw = md.compute_dssp(traj.atom_slice(range(627)), simplified=False)\n",
    "    return np.char.translate(raw, table=dssptable).astype(np.float32)\n",
    "\n",
    "feat = pe.coordinates.featurizer(top)\n",
    "feat.add_custom_func(dssp_enc, dim=42)\n",
    "inp = pe.coordinates.source(trajs, feat)\n",
    "dssp = np.vstack(inp.get_output()).astype(np.int32)\n",
    "\n",
    "# One-hot encoding\n",
    "nvals = dssp.max() + 1\n",
    "dsspoh = np.eye(nvals, dtype=np.int32)[dssp]\n",
    "\n",
    "# We could use the simplified DSSP scheme, but this gives us a bit more flexibility\n",
    "dssplow = np.empty((nframes, nres, 4))\n",
    "dssplow[:, :, 0] = dsspoh[:, :, [0, 3, 4]].sum(axis=-1)\n",
    "dssplow[:, :, 1] = dsspoh[:, :, [1, 2]].sum(axis=-1)\n",
    "dssplow[:, :, 2] = dsspoh[:, :, [5, 6]].sum(axis=-1)\n",
    "dssplow[:, :, 3] = dsspoh[:, :, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = unflatten(np.arange(nframes).reshape(-1, 1), lengths)\n",
    "sec = {n: np.empty((attempts, n, nres, 4)) for n in outsizes}\n",
    "for i in range(attempts):\n",
    "    generator = DataGenerator.from_state(inds, \"models/model-idx-red-{0}.hdf5\".format(i))\n",
    "    idx = generator(n=2, lag=20).trains[0].flatten().astype(int)\n",
    "    for n in outsizes:\n",
    "        print(\"Processing n={0} i={1}...\".format(n, i), end=\"\\r\")\n",
    "        sec[n][i] = (np.einsum(\"ij,ikl->jkl\", pfs[n][i, idx], dssplow[idx]) /\n",
    "                     pfs[n][i, idx].sum(axis=0).reshape(-1, 1, 1))\n",
    "\n",
    "secm = {n: sec[n].mean(axis=0)[global_sorter[n]] for n in outsizes}\n",
    "secs = {n: np.percentile(sec[n], q=(2.5, 97.5), axis=0)[:, global_sorter[n]] for n in outsizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    fig, axes = plt.subplots(4, n, figsize=(4 * n, 7.5), sharex=True,\n",
    "                             gridspec_kw={\"height_ratios\": [5, 5, 15, 1], \"width_ratios\": [3] * n})\n",
    "    for i, axs in enumerate(axes.T):\n",
    "        for j, ax in enumerate(axs[:2]):\n",
    "            ax.bar(np.arange(nres), secm[n][i, :, j], yerr=[secm[n][i, :, j] - secs[n][0, i, :, j],\n",
    "                                                            secs[n][1, i, :, j] - secm[n][i, :, j]],\n",
    "                   capsize=2, color=colors[i])\n",
    "            sns.despine(ax=ax)\n",
    "            ax.set_ylim(0, 1)\n",
    "            if i == 0:\n",
    "                ax.set_ylabel([r\"$P(\\alpha)$\", r\"$P(\\beta)$\"][j], fontsize=24)\n",
    "            else:\n",
    "                ax.set_yticklabels([])\n",
    "            ax.set_yticks([0, 0.5, 1.0])\n",
    "            ax.set_xticks(np.arange(0, 50, 10))\n",
    "            ax.set_xticks(np.arange(0, 42, 1), minor=True)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.tick_params(labelsize=24)\n",
    "            ax.tick_params(axis=\"x\", length=0, labelsize=12)\n",
    "\n",
    "        ax = axs[2]\n",
    "        dat = contacts[n].mean(axis=0)[global_sorter[n]][i]\n",
    "        cmap = sns.light_palette(colors[i], as_cmap=True)\n",
    "        im = ax.matshow(dat, vmin=0.0, vmax=1.0, interpolation=\"nearest\", cmap=cmap)\n",
    "        ax.xaxis.tick_bottom()\n",
    "        ax.set_xticks(np.arange(0, 50, 10))\n",
    "        ax.set_xticks(np.arange(0, 42, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(0, 50, 10))\n",
    "        ax.set_yticks(np.arange(0, 42, 1), minor=True)\n",
    "        ax.set_xticklabels(np.arange(0, 50, 10))\n",
    "        if i != 0:\n",
    "            ax.set_yticklabels([])\n",
    "        ax.set_xlim(0, 41)\n",
    "        ax.set_ylim(41, 0)\n",
    "        ax.tick_params(labelsize=24)\n",
    "\n",
    "        cax = axs[3]\n",
    "        shared = cax.get_shared_x_axes()\n",
    "        for a in shared.get_siblings(cax):\n",
    "            shared.remove(a)\n",
    "        fig.colorbar(im, cax=cax, fraction=0.046, pad=0.04, orientation=\"horizontal\")\n",
    "        cax.xaxis.set_ticks(np.arange(0, 1.5, 0.5))\n",
    "        cax.set_xticklabels(np.arange(0, 1.5, 0.5))\n",
    "        cax.tick_params(labelsize=16)\n",
    "        ax.set_xticks(np.arange(0, 50, 10))\n",
    "        ax.set_xticks(np.arange(0, 42, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(0, 50, 10))\n",
    "        ax.set_yticks(np.arange(0, 42, 1), minor=True)\n",
    "        ax.set_xticklabels(np.arange(0, 50, 10))\n",
    "\n",
    "        cax.xaxis.set_ticks(np.arange(0, 1.5, 0.5))\n",
    "        cax.set_xticklabels(np.arange(0, 1.5, 0.5))\n",
    "        cax.tick_params(labelsize=16)\n",
    "    plt.savefig(\"figs/structure-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/structure-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radius of gyration\n",
    "We can calculate the radius of gyration $R_\\mathrm{gyr}$ for the whole ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pe.coordinates.featurizer(top)\n",
    "feat.add_custom_func(lambda t: md.compute_rg(t).astype(np.float32).reshape(-1, 1), 1)\n",
    "inp = pe.coordinates.source(trajs, feat)\n",
    "gyr = np.vstack(inp.get_output())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 100\n",
    "xmin, xmax = gyr.min(), gyr.max()\n",
    "inds = unflatten(np.arange(nframes).reshape(-1, 1), lengths)\n",
    "gyrs = {n: np.empty((attempts, nbins)) for n in outsizes}\n",
    "for i in range(attempts):\n",
    "    generator = DataGenerator.from_state(inds, \"models/model-idx-red-{0}.hdf5\".format(i))\n",
    "    idx = generator(n=2, lag=20).trains[0].flatten().astype(int)\n",
    "    for n in outsizes:\n",
    "        print(\"Processing n={0} i={1}...\".format(n, i), end=\"\\r\")\n",
    "        gyrs[n][i], edges = np.histogram(\n",
    "            gyr.flatten()[idx], bins=nbins,\n",
    "            range=(xmin, xmax), weights=weights[n][i, idx] / weights[n][i, idx].sum())\n",
    "\n",
    "gyrm = {n: gyrs[n].mean(axis=0) for n in outsizes}\n",
    "gyrs = {n: np.percentile(gyrs[n], q=(2.5, 97.5), axis=0) for n in outsizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "x = 0.5 * (edges[:-1] + edges[1:])\n",
    "ax.plot(x, gyrm[n], linewidth=2, label=\"MSM\", color=colors[4])\n",
    "ax.fill_between(x, gyrm[n], alpha=0.3, color=colors[4])\n",
    "\n",
    "ax.plot(x, gyrs_mi, linewidth=2, label=\"MI\", color=colors[6])\n",
    "ax.fill_between(x, gyrs_mi, alpha=0.3, color=colors[6])\n",
    "\n",
    "ax.set_xlim(0.9, 2.1)\n",
    "ax.set_xlabel(r\"$R_\\mathrm{gyr}$ [nm]\", fontsize=24)\n",
    "ax.set_ylabel(r\"Density\", fontsize=24)\n",
    "ax.tick_params(labelsize=24)\n",
    "ax.legend(fontsize=16)\n",
    "sns.despine(ax=ax)\n",
    "plt.savefig(\"figs/gyr.pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.savefig(\"figs/gyr.svg\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = unflatten(np.arange(nframes).reshape(-1, 1), lengths)\n",
    "gyrs_av = {n: np.empty((attempts, n)) for n in outsizes}\n",
    "for i in range(attempts):\n",
    "    generator = DataGenerator.from_state(inds, \"models/model-idx-red-{0}.hdf5\".format(i))\n",
    "    idx = generator(n=2, lag=20).trains[0].flatten().astype(int)\n",
    "    for n in outsizes:\n",
    "        print(\"Processing n={0} i={1}...\".format(n, i), end=\"\\r\")\n",
    "        gyrs_av[n][i] = (pfs[n][i, idx].T @ gyr.flatten()[idx]) / pfs[n][i, idx].sum(axis=0)\n",
    "\n",
    "gyr_avm = {n: gyrs_av[n].mean(axis=0) for n in outsizes}\n",
    "gyr_avs = {n: np.percentile(gyrs_av[n], q=(2.5, 97.5), axis=0) for n in outsizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n), gyr_avm[n][global_sorter[n]],\n",
    "           yerr=[gyr_avm[n][global_sorter[n]] - gyr_avs[n][0][global_sorter[n]],\n",
    "                 gyr_avs[n][1][global_sorter[n]] - gyr_avm[n][global_sorter[n]]],\n",
    "           color=colors, capsize=8)\n",
    "    for i in range(n):\n",
    "        ax.text(i, gyr_avs[n][1][global_sorter[n]][i] + 0.02, \"{:.2f}\".format(gyr_avm[n][global_sorter[n]][i]),\n",
    "                fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(1, 1.3)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(r\"$R_\\mathrm{gyr}$ [nm]\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/gyr-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/gyr-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinetics\n",
    "### Koopman operators\n",
    "We can look at the Koopman operators and their errors directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax, mat, title in zip(axes, (koops[n].mean(axis=0)[global_sorter[n]][:, global_sorter[n]],\n",
    "                                 koops[n].std(axis=0)[global_sorter[n]][:, global_sorter[n]]),\n",
    "                          (\"$P$\", r\"$\\sigma(P)$\")):\n",
    "    ax.matshow(mat, vmin=0.0, vmax=0.02, interpolation=\"none\", cmap=\"GnBu\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(j, i, \"{0:2.4f}\".format(mat[i, j]), ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(title, fontsize=24)\n",
    "    ax.tick_params(length=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean first passage times\n",
    "We calculate mean first-passage times from our Koopman matrix $\\mathbf{K}(\\tau)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfpts, rates = {}, {}\n",
    "for n in outsizes:\n",
    "    mfpts[n] = np.zeros((attempts, n, n))\n",
    "    rates[n] = np.zeros((attempts, n, n))\n",
    "    for i in range(attempts):\n",
    "        for u in range(n):\n",
    "            for v in range(n):\n",
    "                if u == v:\n",
    "                    continue\n",
    "                koop = renormalize(koops[n][i])\n",
    "                f = tpt(koop, [u], [v])\n",
    "                rates[n][i, u, v] = f.rate\n",
    "                mfpts[n][i, u, v] = f.mfpt * 50 * dt * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax, mat, title in zip(axes, (mfpts[n].mean(axis=0)[global_sorter[n]][:, global_sorter[n]],\n",
    "                                 mfpts[n].std(axis=0)[global_sorter[n]][:, global_sorter[n]]),\n",
    "                          (r\"$\\mathrm{MFPT}$ [µs]\", r\"$\\sigma(\\mathrm{MFPT})$\")):\n",
    "    im = ax.matshow(mat, vmin=0.0, vmax=60, interpolation=\"nearest\", cmap=\"GnBu\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(j, i, \"{0:2.2f}\".format(mat[i, j]), ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(title, fontsize=24)\n",
    "    ax.tick_params(length=0)\n",
    "plt.savefig(\"figs/mfpt-red-tmp-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "plt.savefig(\"figs/mfpt-red-tmp-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition rates\n",
    "The transition rates are the inverse of the mean first-passage times (MFPTs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "perms = 1e6 * rates[n] / (50 * dt)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax, mat, title in zip(axes, (perms.mean(axis=0)[global_sorter[n]][:, global_sorter[n]],\n",
    "                                 perms.std(axis=0)[global_sorter[n]][:, global_sorter[n]]),\n",
    "                          [r\"$k_{ij}$ [1/ms]\", \"$\\sigma(k_{ij})$ [1/ms]\"]):\n",
    "    ax.matshow(mat, vmin=0.0, vmax=1000, interpolation=\"none\", cmap=\"GnBu\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(j, i, \"{0:2.2f}\".format(mat[i, j]), ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(title, fontsize=24)\n",
    "    ax.tick_params(length=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifetimes\n",
    "The lifetimes are dependent on the probability of transition into the same state, they are given by:\n",
    "\n",
    "$$ \\overline{t}_i = \\frac{-\\tau}{\\log K_{ii}} $$\n",
    "\n",
    "where $K_{ii}$ are the diagonal elements of the transition matrix and $\\tau$ is the model lagtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetimes = {}\n",
    "for n in outsizes:\n",
    "    lt = np.empty((attempts, n))\n",
    "    for i in range(attempts):\n",
    "        lt[i] = -lag * dt / np.log(np.diag(koops[n][i])) * 1e-3\n",
    "    lifetimes[n] = lt.mean(axis=0)[global_sorter[n]], *(np.percentile(lt, q=(2.5, 97.5), axis=0)[:, global_sorter[n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylimit = 6\n",
    "for n in outsizes:\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n), lifetimes[n][0], yerr=[lifetimes[n][0] - lifetimes[n][1],\n",
    "                                                lifetimes[n][2] - lifetimes[n][0]],\n",
    "           color=colors, capsize=8)\n",
    "    for i in range(n):\n",
    "        if lifetimes[n][2][i] < ylimit:\n",
    "            ax.text(i, lifetimes[n][2][i] + 0.5, \"{:.2f}\".format(lifetimes[n][0][i]),\n",
    "                    fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, ylimit)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(r\"$\\overline{t}_i$ [µs]\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/lifetime-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/lifetime-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timescales\n",
    "We just plot the relaxation timescales here for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescales = {}\n",
    "with h5py.File(\"intermediate/data.hdf5\") as read:\n",
    "    store = read[\"red\"]\n",
    "    for n in outsizes:\n",
    "        its = np.stack(store[\"{0}/{1}/its\".format(i, n)] for i in range(attempts))[:, ::-1] * 1e-3\n",
    "        timescales[n] = (its[:, :, -2].mean(axis=0), *(np.percentile(its[:, :, -2], q=(2.5, 97.5), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    fig = plt.figure(figsize=(n - 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n - 1), timescales[n][0], yerr=[timescales[n][0] - timescales[n][1],\n",
    "                                                     timescales[n][2] - timescales[n][0]],\n",
    "           color=colors[4:], capsize=8)\n",
    "    for i in range(n - 1):\n",
    "        ax.text(i, timescales[n][2][i] + 0.2, \"{:.2f}\".format(timescales[n][0][i]),\n",
    "                fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_xticks(np.arange(n - 1))\n",
    "    ax.set_ylabel(r\"$t_i$ [µs]\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/timescales-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/timescales-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equilibrium distribution\n",
    "We can look at the equilibrium distributions $\\mathbf{\\pi}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    pm = pis[n].mean(axis=0)[global_sorter[n]]\n",
    "    pv = np.percentile(pis[n], q=(2.5, 97.5), axis=0)[:, global_sorter[n]]\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n), pm, yerr=[pm - pv[0], pv[1] - pm], color=colors, capsize=8)\n",
    "    for i in range(n):\n",
    "        ax.text(i, pv[1, i] + 0.05, \"{:.2f}\".format(pm[i]), fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(\"Probability\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/pops-{0}.pdf\".format(n), transparent=True, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"figs/pops-{0}.svg\".format(n), transparent=True, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "pm = pis[n].mean(axis=0)[global_sorter[n]]\n",
    "pv = np.percentile(pis[n], q=(2.5, 97.5), axis=0)[:, global_sorter[n]]\n",
    "for i in range(n):\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cols = [(0.8, 0.8, 0.8)] * n\n",
    "    cols[i] = colors[i]\n",
    "    ax.bar(np.arange(n), pm, yerr=[pm - pv[0], pv[1] - pm], color=cols, capsize=8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.0, 0.5, 1.0])\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(\"P\", fontsize=32, labelpad=10)\n",
    "    ax.tick_params(labelsize=32)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    ax.text(0.7, 0.9, \"{0}: {1:2.0f} %\".format(i, pm[i] * 100), fontsize=32)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/pops-fine-{0}-{1}.pdf\".format(n, i), transparent=True, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"figs/pops-fine-{0}-{1}.svg\".format(n, i), transparent=True, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "We can get some idea of the \"entropy\" of each state by calculating the information entropy $S_i = -\\sum_t^N \\chi_i(\\mathbf{x}_t) \\log_2(\\chi_i(\\mathbf{x}_t)) $. In some sense, this encodes ambiguity in the state assignment, or how \"wide\" the state is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = {}\n",
    "for i, n in enumerate(outsizes):\n",
    "    ent = -np.nansum(pfsn_boot[n] * np.log2(pfsn_boot[n]) / np.log2(pfsn_boot[n].shape[1]), axis=1)\n",
    "    ents[n] = np.array([ent.mean(axis=0)[global_sorter[n]],\n",
    "                        *(np.percentile(ent, (2.5, 97.5), axis=0))[:, global_sorter[n]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n), ents[n][0], yerr=[ents[n][0] - ents[n][1], ents[n][2] - ents[n][0]],\n",
    "           color=colors, capsize=8)\n",
    "    for i in range(n):\n",
    "        ax.text(i, ents[n][2, i] + 0.05, \"{:.2f}\".format(ents[n][0, i]), fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(\"Normalized entropy\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph\n",
    "We will now look at the model in the classic graph format. A good way of projecting the states is on the space of the two slowest time-lagged independent components (tICs), as they separate the states very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = {}\n",
    "for n in outsizes:\n",
    "    state_tic = np.einsum(\"ijk,jl->ikl\", pfsn[n], ycon[:, :2])\n",
    "    pos[n] = state_tic.mean(axis=0)[global_sorter[n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mask out transition probabilities below a certain threshold, and define the crispness as $\\mathscr{c}_i := S_i^{-1}$, i.e. a crisper state is less ambiguous in it's state assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "minflux = 3e-4\n",
    "ps = np.empty((attempts, n, n))\n",
    "for i in range(attempts):\n",
    "    p = koops[n][i][global_sorter[n]][:, global_sorter[n]].copy()\n",
    "    u, v = np.where((np.diag(pis[n][i][global_sorter[n]]) @ p) < minflux)\n",
    "#     p[u, v] = 0.0\n",
    "    ps[i, :, :] = p\n",
    "\n",
    "crisp = 1 / ents[n][0]\n",
    "crisp /= crisp.max()\n",
    "posi = pos[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just to get the arrow thickness proportional to the flux,\n",
    "# when using external software like Illustrator...\n",
    "psm = ps.mean(axis=0)\n",
    "pmin, pmax = psm[psm > 0].min(), psm[psm < 0.9].max()\n",
    "psm[(psm == 0.0) | (psm > 0.9)] = np.nan\n",
    "psm -= pmin\n",
    "psm /= pmax\n",
    "psm * 3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "fig, posi = pe.plots.plot_network(\n",
    "    ps.mean(axis=0), pos=posi, state_sizes=pis[n].mean(axis=0)[global_sorter[n]], show_frame=True, ax=ax, arrow_curvature=1.2)\n",
    "plt.close();\n",
    "\n",
    "figpadding = 0.2\n",
    "xmin = posi[:, 0].min()\n",
    "xmax = posi[:, 0].max()\n",
    "Dx = xmax - xmin\n",
    "xmin -= Dx * figpadding\n",
    "xmax += Dx * figpadding\n",
    "Dx *= 1 + figpadding\n",
    "ymin = posi[:, 0].min()\n",
    "ymax = posi[:, 0].max()\n",
    "Dy = ymax - ymin\n",
    "ymin -= Dy * figpadding\n",
    "ymax += Dy * figpadding\n",
    "Dy *= 1 + figpadding\n",
    "sizes = min(Dx, Dy) ** 2 * 0.5 * pis[n].mean(axis=0)[global_sorter[n]] / (pis[n].mean(axis=0).max() * n)\n",
    "crispness = min(Dx, Dy) ** 2 * 0.5 * crisp\n",
    "\n",
    "# We need to redraw these to be able to show the crispness\n",
    "ax.artists = []\n",
    "for i in range(n):\n",
    "    ax.add_artist(plt.Circle(posi[i], radius=0.5 * np.sqrt(0.5 * sizes[i]),\n",
    "                             facecolor=colors[i], edgecolor=colors[i], alpha=0.5))\n",
    "    ax.add_artist(plt.Circle(posi[i], radius=0.1 * np.sqrt(0.5 * crispness[i]), facecolor=colors[i]))\n",
    "ax.tick_params(labelsize=24)\n",
    "ax.set_xticks(np.arange(-2, 3, 1))\n",
    "ax.set_yticks(np.arange(-2, 3, 1))\n",
    "ax.set_xticks(np.arange(-2, 2.1, 0.1), minor=True)\n",
    "ax.set_yticks(np.arange(-2, 2.1, 0.1), minor=True)\n",
    "ax.set_xlim(-2.1, 2.1)\n",
    "ax.set_ylim(-2.1, 2.1)\n",
    "ax.set_xlabel(r\"tIC 0\", fontsize=24, labelpad=10)\n",
    "ax.set_ylabel(r\"tIC 1\", fontsize=24, labelpad=10)\n",
    "sns.despine(ax=ax)\n",
    "fig.savefig(\"figs/graph-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "fig.savefig(\"figs/graph-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to unconstrained model\n",
    "By comparing the unconstrained model, estimated directly from the state assignments $\\chi(\\mathbf{x}_t)$, with the constrained model, we can get an idea of the impact of constraining reversibility and positive matrix elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_koopman(data: List[np.ndarray], lag: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimate the Koopman matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        List of state vector trajectories\n",
    "    lag\n",
    "        Lag time for estimating the matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    K\n",
    "        The Koopman matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    cl = pe.coordinates.covariance_lagged(\n",
    "            data=data, lag=lag, weights=\"empirical\",\n",
    "            reversible=True, bessel=True)\n",
    "    return np.linalg.pinv(cl.C00_) @ cl.C0t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukoops = {}\n",
    "for n in outsizes:\n",
    "    ukoops[n] = np.empty((attempts, n, n))\n",
    "    for i in range(attempts):\n",
    "        ppf = unflatten(pfs[n][i], lengths)\n",
    "        ukoops[n][i] = estimate_koopman(ppf, lag=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition matrix comparison\n",
    "We compare the transition matrices from both the constrained and unconstrained VAMPNets. We would expect the shorter timescales in the constrained case as the eigenfunction estimation becomes hampered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax, mat, title in zip(axes, (koops[n].mean(axis=0)[global_sorter[n]][:, global_sorter[n]],\n",
    "                                 ukoops[n].mean(axis=0)[global_sorter[n]][:, global_sorter[n]]),\n",
    "                          (\"Constrained $P$\", r\"Unconstrained $P$\")):\n",
    "    ax.matshow(mat, vmin=0.0, vmax=0.02, interpolation=\"nearest\", cmap=\"GnBu\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            col = \"white\" if i == j else \"black\"\n",
    "            ax.text(j, i, \"{0:2.4f}\".format(mat[i, j]), ha=\"center\", va=\"center\", fontsize=12, color=col)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(title, fontsize=24)\n",
    "    ax.tick_params(length=0)\n",
    "    fig.savefig(\"figs/k-comp-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    fig.savefig(\"figs/k-comp-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timescales\n",
    "Timescales are a bit easier to interpret than raw matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlags = lags.shape[0]\n",
    "mitss = {}\n",
    "for n in outsizes:\n",
    "    mitss[n] = np.empty((attempts, n - 1, nlags))\n",
    "    for i in range(attempts):\n",
    "        print(\"Reading {0}/{1}  Size {2}...\".format(i, attempts, n), end=\"\\r\")\n",
    "        ppf = unflatten(pfs[n][i], lengths)\n",
    "        for t, tau in enumerate(lags):\n",
    "            koop = estimate_koopman(ppf, tau)\n",
    "            mitss[n][i, :, t] = -tau * dt / np.log(np.sort(np.linalg.eigvals(koop))[::-1])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescales_uncon = {}\n",
    "for n in outsizes:\n",
    "    _its = mitss[n][:, :, 5] * 1e-3\n",
    "    t_uncon_m = _its.mean(axis=0)\n",
    "    t_uncon_p = np.percentile(_its, q=(2.5, 97.5), axis=0)\n",
    "    timescales_uncon[n] = t_uncon_m, *t_uncon_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    fig = plt.figure(figsize=(n - 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n - 1), timescales_uncon[n][0], yerr=[timescales_uncon[n][0] - timescales_uncon[n][1],\n",
    "                                                           timescales_uncon[n][2] - timescales_uncon[n][0]],\n",
    "           color=colors[4:], capsize=8)\n",
    "    for i in range(n - 1):\n",
    "        ax.text(i, timescales_uncon[n][2][i] + 0.2, \"{:.2f}\".format(timescales_uncon[n][0][i]),\n",
    "                fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_xticks(np.arange(n - 1))\n",
    "    ax.set_ylabel(r\"$t_i$ [µs]\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/timescales-uncon-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/timescales-uncon-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    fig = plot_its(mitss[n], lags)\n",
    "    fig.savefig(\"figs/its-uncon-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    fig.savefig(\"figs/its-uncon-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifetimes\n",
    "The lifetimes are dependent on the probability of transition into the same state, they are given by:\n",
    "\n",
    "$$ \\overline{t}_i = \\frac{-\\tau}{\\log K_{ii}} $$\n",
    "\n",
    "where $K_{ii}$ are the diagonal elements of the transition matrix and $\\tau$ is the model lagtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetimes_uncon = {}\n",
    "for n in outsizes:\n",
    "    lt = np.empty((attempts, n))\n",
    "    for i in range(attempts):\n",
    "        lt[i] = -lag * dt / np.log(np.diag(ukoops[n][i])) * 1e-3\n",
    "    lifetimes_uncon[n] = lt.mean(axis=0)[global_sorter[n]], *(np.percentile(lt, q=(2.5, 97.5), axis=0)[:, global_sorter[n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylimit = 6\n",
    "for n in outsizes:\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n), lifetimes_uncon[n][0], yerr=[lifetimes_uncon[n][0] - lifetimes_uncon[n][1],\n",
    "                                                      lifetimes_uncon[n][2] - lifetimes_uncon[n][0]],\n",
    "           color=colors, capsize=8)\n",
    "    for i in range(n):\n",
    "        if lifetimes_uncon[n][2][i] < ylimit:\n",
    "            ax.text(i, lifetimes_uncon[n][2][i] + 0.5, \"{:.2f}\".format(lifetimes_uncon[n][0][i]),\n",
    "                    fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, ylimit)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(r\"$\\overline{t}_i$ [µs]\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/lifetime-uncon-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/lifetime-uncon-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean first passage times\n",
    "We calculate mean first-passage times from our Koopman matrix $\\mathbf{K}(\\tau)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfpts_uncon, rates_uncon = {}, {}\n",
    "for n in outsizes:\n",
    "    mfpts_uncon[n] = np.zeros((attempts, n, n))\n",
    "    rates_uncon[n] = np.zeros((attempts, n, n))\n",
    "    for i in range(attempts):\n",
    "        for u in range(n):\n",
    "            for v in range(n):\n",
    "                if u == v:\n",
    "                    continue\n",
    "                koop = renormalize(ukoops[n][i])\n",
    "                f = tpt(koop, [u], [v])\n",
    "                rates_uncon[n][i, u, v] = f.rate\n",
    "                mfpts_uncon[n][i, u, v] = f.mfpt * 50 * dt * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax, mat, title in zip(axes, (mfpts[n].mean(axis=0)[global_sorter[n]][:, global_sorter[n]],\n",
    "                                 mfpts_uncon[n].mean(axis=0)[global_sorter[n]][:, global_sorter[n]]),\n",
    "                          (r\"Constr. $\\mathrm{MFPT}$ [µs]\", r\"Unconstr. $\\mathrm{MFPT}$ [µs]\")):\n",
    "    ax.matshow(mat, vmin=0.0, vmax=50, interpolation=\"none\", cmap=\"GnBu\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(j, i, \"{0:2.2f}\".format(mat[i, j]), ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(title, fontsize=24)\n",
    "    ax.tick_params(length=0)\n",
    "    plt.savefig(\"figs/mfpt-comp-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/mfpt-comp-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectories\n",
    "We can visualize the state occupation over time, at least for a few trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "flatlengths = np.fromiter(itertools.chain(*lengths), dtype=np.int32)\n",
    "borders = flatlengths.cumsum()\n",
    "\n",
    "t0, t1 = 0, 10000\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(t1 - t0)\n",
    "y = pfs[n].mean(axis=0)[t0:t1]\n",
    "ax.matshow(y.T, interpolation=\"none\", aspect=200, cmap=\"Blues\")\n",
    "for i, b in enumerate(borders[(t0 < borders) & (borders < t1)]):\n",
    "    ax.plot(np.repeat(b - t0, 2), [-0.5, 3.5], linewidth=0.5, color=\"#333333\", alpha=0.5)\n",
    "    ax.text(b - t0 - 100, 4.5, str(i), fontsize=16)\n",
    "\n",
    "ax.set_xlabel(\"Time [steps]\", fontsize=24, labelpad=35)\n",
    "ax.set_ylabel(\"$P$\", fontsize=24)\n",
    "ax.tick_params(labelsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transitions\n",
    "We look for trajectories with full state transitions with some simple heuristics. This can be cool for visualisation with VMD, Chimera, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "upf = unflatten(pfs[n].mean(axis=0)[:, global_sorter[n]], lengths=lengths)\n",
    "window, frac, minn = 10, 0.8, 5\n",
    "transitions = defaultdict(list)\n",
    "for i, t in enumerate(upf):\n",
    "    instate = (t[:window].max(axis=1) > frac).sum() > minn and (t[-window:].max(axis=1) > frac).sum() > minn\n",
    "    samestate = t[0].argmax() == t[-1].argmax()\n",
    "    if instate and not samestate:\n",
    "        transitions[(t[0].argmax(), t[-1].argmax())].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_traj(transitions[(1, 0)][0], lengths=[1024, 1023, 1024, 1024, 1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example structures\n",
    "We extract representative structures for each state, for example just the ones with highest weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatlengths = np.array(list(itertools.chain(*lengths)))\n",
    "for n in outsizes:\n",
    "    spfs = pfs[n].mean(axis=0)[:, global_sorter[n]]\n",
    "    sids = spfs.argsort(axis=0)[-50:]\n",
    "    for s in range(n):\n",
    "        trjind = []\n",
    "        for ind in sids[:, s]:\n",
    "            pdiff = flatlengths.cumsum() - ind\n",
    "            trjidx = np.where(pdiff > 0)[0][0]\n",
    "            trjnr = pdiff[trjidx]\n",
    "            trjind.append((trjidx, trjnr - 1))\n",
    "\n",
    "        frames = md.join(md.load_frame(trajs[trjidx], trjnr, top=top)\n",
    "                         for i, (trjidx, trjnr) in enumerate(trjind))\n",
    "        frames.save_pdb(\"structures/state-{0}-{1}-top.pdb\".format(n, s))\n",
    "        np.savetxt(\"structures/state-{0}-{1}-top-p.dat\".format(n, s), spfs[:, s][sids[:, s]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or by sampling randomly based on these weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 50\n",
    "flatlengths = np.array(list(itertools.chain(*lengths)))\n",
    "allinds = np.arange(nframes, dtype=np.int64)\n",
    "for n in outsizes:\n",
    "    spfs = pfs[n].mean(axis=0)[:, global_sorter[n]]\n",
    "    for s in range(n):\n",
    "        p = spfs[:, s] / spfs[:, s].sum()\n",
    "        sids = np.random.choice(allinds, size=nsamples, replace=False, p=p)\n",
    "        trjind = []\n",
    "        for ind in sids:\n",
    "            pdiff = flatlengths.cumsum() - ind\n",
    "            trjidx = np.where(pdiff > 0)[0][0]\n",
    "            trjnr = pdiff[trjidx]\n",
    "            trjind.append((trjidx, trjnr - 1))\n",
    "\n",
    "        frames = md.join(md.load_frame(trajs[trjidx], trjnr, top=top)\n",
    "                         for i, (trjidx, trjnr) in enumerate(trjind))\n",
    "        frames.save_pdb(\"structures-alt/state-{0}-{1}-top.pdb\".format(n, s))\n",
    "        np.savetxt(\"structures-alt/state-{0}-{1}-top-p.dat\".format(n, s), spfs[:, s][sids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aβ42-MetSO Analysis\n",
    "\n",
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in five rounds of 1024 simulations each, totalling 5119 runs (one simulation failed to run) at 278 K in the $NVT$ ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_ox = sorted(glob(\"trajectories/ox/r?/traj*.xtc\"))\n",
    "top_ox = \"trajectories/ox/topol.gro\"\n",
    "traj_rounds_ox = [1024, 2047, 3071]\n",
    "outsizes = [3, 4]\n",
    "\n",
    "# This is only really necessary for the residues in the plots\n",
    "topo_ox = md.load_topology(top_ox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat = pe.coordinates.featurizer(top_ox)\n",
    "feat.add_residue_mindist()\n",
    "inpcon_ox = pe.coordinates.source(trajs_ox, feat)\n",
    "\n",
    "lengths_ox = sort_lengths(inpcon_ox.trajectory_lengths(), [1024, 1023, 1024])\n",
    "nframes_ox = inpcon_ox.trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trajectories: {0}\".format(len(trajs_ox)))\n",
    "print(\"Frames: {0}\".format(nframes_ox))\n",
    "print(\"Time: {0:5.3f} µs\".format(inpcon_ox.trajectory_lengths().sum() * 0.00025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) for the neural network, but remove the 2nd and 3rd off-diagonals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"intermediate/mindist-780-ox.npy\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing file for ensemble: {0}\".format(filename))\n",
    "    input_flat_ox = np.load(filename)\n",
    "else:\n",
    "    print(\"No mindist file for ensemble, calculating from scratch...\")\n",
    "    input_flat_ox = np.vstack(inpcon_ox.get_output())\n",
    "    np.save(filename, input_flat_ox)\n",
    "input_data_ox = unflatten(input_flat_ox, lengths_ox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the full minimum inter-residue distances for some analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpairs = np.asarray(list(itertools.combinations(range(nres), 2)))\n",
    "filename = \"intermediate/mindist-all-ox.npy\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing file for ensemble: {0}\".format(filename))\n",
    "    mindist_flat_ox = np.load(filename)\n",
    "else:\n",
    "    print(\"No mindist file for ensemble, calculating from scratch...\")\n",
    "    feat = pe.coordinates.featurizer(top_ox)\n",
    "    feat.add_residue_mindist(residue_pairs=allpairs)\n",
    "    inpmindist_ox = pe.coordinates.source(trajs_ox, feat)\n",
    "    mindist_flat_ox = np.vstack(inpmindist_ox.get_output())\n",
    "    np.save(filename, mindist_flat_ox)\n",
    "mindist_ox = unflatten(mindist_flat_ox, lengths_ox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "We load the previously trained neural network models and calculate the implied timescales, Chapman-Kolmogorov test, and the Koopman operators. This can take a long time, as the constraint vectors have to be re-estimated for every lag time, so we save the intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State sorting\n",
    "As every run of the neural network will generate a different ordering of (mostly the same) classes, we need to reorder them to be internally consistent. We do this by calculating the root-mean-square deviation between all average inter-residue minimum distance matrices for the individual states and matching the lowest values. We also make sure the sorting is unique, i.e. all states are accounted for, even if a certain RMSD is lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"intermediate/data.hdf5\") as store:\n",
    "    for n in outsizes:\n",
    "        # First attempt is the reference dataset\n",
    "        ref_group = store[\"ox/{0}/{1}\".format(0, n)]\n",
    "        ref = ref_group[\"full\"][:, :n]\n",
    "\n",
    "        # This is the average contact map for every\n",
    "        # state for the first training attempt\n",
    "        conwref = (ref / ref.sum(axis=0)).T @ mindist_flat_ox\n",
    "        conwrefbt = np.broadcast_to(conwref, (n, n, mindist_flat_ox.shape[1])).swapaxes(0, 1)\n",
    "        ref_group.create_dataset(\"sorter\", data=np.arange(n, dtype=\"int8\"))\n",
    "        ref_group.create_dataset(\"full_sorted\", data=ref)\n",
    "        ref_group.create_dataset(\"bootstrap_sorted\", data=ref_group[\"bootstrap\"][:, :n])\n",
    "\n",
    "        for i in range(1, attempts):\n",
    "            print(\"Processing n={0} i={1}...\".format(n, i), end=\"\\r\")\n",
    "            group = store[\"ox/{0}/{1}\".format(i, n)]\n",
    "            pf = group[\"full\"][:, :n]\n",
    "\n",
    "            # Average contact map for each state for attempt `i`\n",
    "            conw = (pf / pf.sum(axis=0)).T @ mindist_flat_ox\n",
    "            conwbt = np.broadcast_to(conw, (n, n, mindist_flat_ox.shape[1]))\n",
    "\n",
    "            # Calculate the RMSD to our reference attempt\n",
    "            rmsd = np.sqrt(((conwbt - conwrefbt) ** 2).sum(axis=-1))\n",
    "\n",
    "            # Get a permutation vector and store it\n",
    "            sorter = unique_sorting(rmsd)\n",
    "            group.create_dataset(\"sorter\", data=sorter)\n",
    "            group.create_dataset(\"full_sorted\", data=pf[:, sorter])\n",
    "            group.create_dataset(\"bootstrap_sorted\", data=group[\"bootstrap\"][:, :n][:, sorter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorters = {n: np.empty((attempts, n), dtype=int) for n in outsizes}\n",
    "pfs = {n: np.empty((attempts, nframes_ox, n)) for n in outsizes}\n",
    "pfsn = {n: np.empty((attempts, nframes_ox, n)) for n in outsizes}\n",
    "pfs_boot = {n: np.empty((attempts, bs_frames, n)) for n in outsizes}\n",
    "pfsn_boot = {n: np.empty((attempts, bs_frames, n)) for n in outsizes}\n",
    "koops = {n: np.empty((attempts, n, n)) for n in outsizes}\n",
    "pis = {n: np.empty((attempts, n)) for n in outsizes}\n",
    "with h5py.File(\"intermediate/data.hdf5\") as read:\n",
    "    store = read[\"ox\"]\n",
    "    for i in range(attempts):\n",
    "        for n in outsizes:\n",
    "            sorters[n][i] = store[\"{0}/{1}/sorter\".format(i, n)][:]\n",
    "            pfs[n][i] = store[\"{0}/{1}/full_sorted\".format(i, n)][:]\n",
    "            pfsn[n][i] = pfs[n][i] / pfs[n][i].sum(axis=0)\n",
    "            pfs_boot[n][i] = store[\"{0}/{1}/bootstrap_sorted\".format(i, n)][:]\n",
    "            pfsn_boot[n][i] = pfs_boot[n][i] / pfs_boot[n][i].sum(axis=0)\n",
    "            koops[n][i] = store[\"{0}/{1}/k\".format(i, n)][:][sorters[n][i]][:, sorters[n][i]]\n",
    "            pis[n][i] = statdist(koops[n][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate an equilibrium weighting vector, analogous to discrete Markov models:\n",
    "\n",
    "\\begin{equation}\n",
    "w_t = \\frac{\\langle \\chi(\\mathbf{x}_t) | \\mathbf{\\pi} \\rangle}{\\sum_t^N \\langle \\chi(\\mathbf{x}_t) | \\mathbf{\\pi} \\rangle}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "for n in outsizes:\n",
    "    weights[n] = np.empty((attempts, nframes_ox))\n",
    "    for i in range(attempts):\n",
    "        w = pfs[n][i] @ pis[n][i]\n",
    "        weights[n][i] = w / w.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global sorting\n",
    "Because state assignments will not only be different within a choice of number of states, but also globally, we sort the states based on the previous (coarser) state decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_sorter = {n: pis[n].mean(axis=0).argsort()[::-1] for n in outsizes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implied timescales\n",
    "The implied timescales are computed from the eigenvalues $\\lambda_i$ of the Koopman operator $\\mathbf{K}$ and the selected lag time $\\tau$:\n",
    "$$ t_i = \\frac{-\\tau}{\\log | \\lambda_i(\\tau) |} $$\n",
    "We compute the implied timescales for several lag times $\\tau$. Ideally, we want to choose $\\tau$ so that we're in a regime where the timescales are mostly independent of $\\tau$. This is the case for lag times longer than $\\tau = 12.5\\,\\mathrm{ns}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"intermediate/data.hdf5\") as store:\n",
    "    for n in outsizes:\n",
    "        its = np.stack(store[\"ox/{0}/{1}/its\".format(i, n)] for i in range(attempts))\n",
    "        fig = plot_its(its, lags, dt=dt)\n",
    "        plt.savefig(\"figs/its-ox-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "        plt.savefig(\"figs/its-ox-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapman-Kolmogorov Test\n",
    "Now that we have found a Koopman operator $\\mathbf{K}$, how do we know if it adequately describes the dynamical system?\n",
    "\n",
    "There are a number of tests we can perform to ensure we have found a good approximation. One of the more stringent ones is the Chapman Kolmogorov (CK-test). It is based on the assumption that multiple applications of our operator to our system state at time $t$ should be equivalent to estimating the operator at a longer lag time $\\tau$:\n",
    "\n",
    "$$ \\mathbf{K}(\\tau)^n \\approx \\mathbf{K}(n\\tau) $$\n",
    "\n",
    "I.e., multiple applications of the operator should produce the same result as using an operator with a multiple of a certain lag time, within an error. This is a more stringent test of model quality than the implied timescales test described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"intermediate/data.hdf5\") as store:\n",
    "    for n in outsizes:\n",
    "        cke = np.stack(store[\"ox/{0}/{1}/cke\".format(i, n)][:][sorters[n][i]][:, sorters[n][i]]\n",
    "                       for i in range(attempts))[:, global_sorter[n]][:, :, global_sorter[n]]\n",
    "        ckp = np.stack(store[\"ox/{0}/{1}/ckp\".format(i, n)][:][sorters[n][i]][:, sorters[n][i]]\n",
    "                       for i in range(attempts))[:, global_sorter[n]][:, :, global_sorter[n]]\n",
    "        fig = plot_ck(cke, ckp, lag=50)\n",
    "        plt.savefig(\"figs/ck-ox-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "        plt.savefig(\"figs/ck-ox-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "We would ideally like to see how converged our ensemble is with respect to the timescales and stationary distribution given by our model. We thus build trial models with different numbers of trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "k = \"ox\"\n",
    "k_conv = np.load(\"intermediate/k-conv-{0}-{1}.npy\".format(k, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timescales\n",
    "We can examine how the timescales change when estimating with more trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its_conv = np.empty((len(traj_rounds_ox), attempts, n - 1))\n",
    "for j, _ in enumerate(traj_rounds_ox):\n",
    "    for i in range(attempts):\n",
    "        ev, _ = eig(k_conv[j, i], left=True, right=False)\n",
    "        order = np.abs(ev).argsort()[::-1]\n",
    "        its_conv[j, i] = -50 * dt / np.log(np.real(ev[order][1:]))\n",
    "its_conv_m = its_conv.mean(axis=1)\n",
    "its_conv_p = np.percentile(its_conv, (2.5, 97.5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.array(traj_rounds_ox)\n",
    "for i in range(n - 1):\n",
    "    ax.semilogy(x, its_conv_m[:, i], color=colors[i + 4], linewidth=2, marker=\"o\")\n",
    "    ax.fill_between(x, its_conv_p[0, :, i], its_conv_p[1, :, i],\n",
    "                    interpolate=True, color=colors[i + 4], alpha=0.2)\n",
    "\n",
    "loc = ticker.LogLocator(base=10.0, subs=(0.2, 0.4, 0.6, 0.8), numticks=12)\n",
    "ax.set_ylim(1, 100000)\n",
    "ax.set_yticks(10 ** np.arange(6))\n",
    "ax.yaxis.set_minor_locator(loc)\n",
    "ax.yaxis.set_minor_formatter(ticker.NullFormatter())\n",
    "ax.set_xticks(x)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(\"# Trajectories\", fontsize=24)\n",
    "ax.set_ylabel(\"$t$\", fontsize=24)\n",
    "sns.despine(ax=ax)\n",
    "plt.savefig(\"figs/its-conv-coarse-ox-{0}.pdf\".format(n), transparent=True, bbox_inches=\"tight\")\n",
    "plt.savefig(\"figs/its-conv-coarse-ox-{0}.svg\".format(n), transparent=True, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "### TICA\n",
    "Time-lagged independent component analysis is a special case of Koopman operator estimation using a linear projection [4]. We solve the following generalized eigenvalue problem:\n",
    "\n",
    "$$ \\mathbf{C}_{01}v = \\lambda \\mathbf{C}_{00} v $$\n",
    "\n",
    "The eigenvectors encode the slowest dynamics of the system, and we use them as a convenient visualization technique.\n",
    "\n",
    "[4]\tPérez-Hernández, G., Paul, F., Giorgino, T., De Fabritiis, G. & Noé, F. Identification of slow molecular order parameters for Markov model construction. The Journal of Chemical Physics 139, 015102–14 (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticacon = pe.coordinates.tica(mindist_ox, lag=4, dim=-1, kinetic_map=True)\n",
    "ticscon = ticacon.get_output()\n",
    "ycon = np.vstack(ticscon)\n",
    "\n",
    "print(\"tIC Dimensions: {0}\".format(ycon.shape[1]))\n",
    "print(\"Required dimensions for 90 %: {0}\".format(ticacon.cumvar[ticacon.cumvar < 0.9].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Free energy surface\n",
    "We also show the free energy surface projected onto the two slowest tICs in the form of a kernel density estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gaussian_kde(ycon[::10, :2].T)\n",
    "xmin, ymin, *_ = ycon.min(axis=0)\n",
    "xmax, ymax, *_ = ycon.max(axis=0)\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "posi = np.vstack((X.ravel(), Y.ravel()))\n",
    "Z = kernel(posi).reshape(X.shape)\n",
    "mat = np.rot90(Z.copy())\n",
    "mat[mat < 0.01] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "cmap = \"plasma\"\n",
    "F = -KBT * np.log(Z)\n",
    "F -= F.min()\n",
    "ax.contourf(X, Y, F, np.arange(0.0, 10, 1), cmap=cmap)\n",
    "ax.contour(X, Y, F, np.arange(0.0, 10, 1), cmap=cmap, linewidth=10)\n",
    "ax.tick_params(labelsize=24)\n",
    "ax.set_xlabel(r\"tIC 1\", fontsize=24, labelpad=10)\n",
    "ax.set_ylabel(r\"tIC 2\", fontsize=24, labelpad=10)\n",
    "sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contact maps\n",
    "We calculate contact maps from inter-residue minimum distance matrices, using the bootstrapped state assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.8\n",
    "inds = unflatten(np.arange(nframes_ox).reshape(-1, 1), lengths_ox)\n",
    "contacts = {n: np.empty((attempts, n, nres, nres)) for n in outsizes}\n",
    "for i in range(attempts):\n",
    "    generator = DataGenerator.from_state(inds, \"models/model-idx-ox-{0}.hdf5\".format(i))\n",
    "    idx = generator(n=2, lag=20).trains[0].flatten().astype(int)\n",
    "    for n in outsizes:\n",
    "        print(\"Processing n={0} i={1}...\".format(n, i), end=\"\\r\")\n",
    "        con = (np.einsum(\"jk,jl->kl\", pfs[n][i, idx], (mindist_flat_ox[idx] < cutoff)) /\n",
    "               pfs[n][i, idx].sum(axis=0).reshape(-1, 1))\n",
    "        contacts[n][i] = np.asarray([triu_inverse(con[j], nres)[0] for j in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary structure\n",
    "Secondary structure assignment with DSSP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom func makes this a lot easier\n",
    "dssptable = str.maketrans(\"HBEGITS \", \"01234567\")\n",
    "def dssp_enc(traj):\n",
    "    raw = md.compute_dssp(traj.atom_slice(range(627)), simplified=False)\n",
    "    return np.char.translate(raw, table=dssptable).astype(np.float32)\n",
    "\n",
    "feat = pe.coordinates.featurizer(top_ox)\n",
    "feat.add_custom_func(dssp_enc, dim=42)\n",
    "inp = pe.coordinates.source(trajs_ox, feat)\n",
    "dssp = np.vstack(inp.get_output()).astype(np.int32)\n",
    "\n",
    "# One-hot encoding\n",
    "nvals = dssp.max() + 1\n",
    "dsspoh = np.eye(nvals, dtype=np.int32)[dssp]\n",
    "\n",
    "# We could use the simplified DSSP scheme, but this gives us a bit more flexibility\n",
    "dssplow = np.empty((nframes_ox, nres, 4))\n",
    "dssplow[:, :, 0] = dsspoh[:, :, [0, 3, 4]].sum(axis=-1)\n",
    "dssplow[:, :, 1] = dsspoh[:, :, [1, 2]].sum(axis=-1)\n",
    "dssplow[:, :, 2] = dsspoh[:, :, [5, 6]].sum(axis=-1)\n",
    "dssplow[:, :, 3] = dsspoh[:, :, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = unflatten(np.arange(nframes_ox).reshape(-1, 1), lengths_ox)\n",
    "sec = {n: np.empty((attempts, n, nres, 4)) for n in outsizes}\n",
    "for i in range(attempts):\n",
    "    generator = DataGenerator.from_state(inds, \"models/model-idx-ox-{0}.hdf5\".format(i))\n",
    "    idx = generator(n=2, lag=20).trains[0].flatten().astype(int)\n",
    "    for n in outsizes:\n",
    "        print(\"Processing n={0} i={1}...\".format(n, i), end=\"\\r\")\n",
    "        sec[n][i] = (np.einsum(\"ij,ikl->jkl\", pfs[n][i, idx], dssplow[idx]) /\n",
    "                     pfs[n][i, idx].sum(axis=0).reshape(-1, 1, 1))\n",
    "\n",
    "secm = {n: sec[n].mean(axis=0)[global_sorter[n]] for n in outsizes}\n",
    "secs = {n: np.percentile(sec[n], q=(2.5, 97.5), axis=0)[:, global_sorter[n]] for n in outsizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    fig, axes = plt.subplots(4, n, figsize=(4 * n, 7.5), sharex=True,\n",
    "                             gridspec_kw={\"height_ratios\": [5, 5, 15, 1], \"width_ratios\": [3] * n})\n",
    "    for i, axs in enumerate(axes.T):\n",
    "        for j, ax in enumerate(axs[:2]):\n",
    "            ax.bar(np.arange(nres), secm[n][i, :, j], yerr=[secm[n][i, :, j] - secs[n][0, i, :, j],\n",
    "                                                            secs[n][1, i, :, j] - secm[n][i, :, j]],\n",
    "                   capsize=2, color=colors[i])\n",
    "            sns.despine(ax=ax)\n",
    "            ax.set_ylim(0, 1)\n",
    "            if i == 0:\n",
    "                ax.set_ylabel([r\"$P(\\alpha)$\", r\"$P(\\beta)$\"][j], fontsize=24)\n",
    "            else:\n",
    "                ax.set_yticklabels([])\n",
    "            ax.set_yticks([0, 0.5, 1.0])\n",
    "            ax.set_xticks(np.arange(0, 50, 10))\n",
    "            ax.set_xticks(np.arange(0, 42, 1), minor=True)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.tick_params(labelsize=24)\n",
    "            ax.tick_params(axis=\"x\", length=0, labelsize=12)\n",
    "\n",
    "        ax = axs[2]\n",
    "        dat = contacts[n].mean(axis=0)[global_sorter[n]][i]\n",
    "        cmap = sns.light_palette(colors[i], as_cmap=True)\n",
    "        im = ax.matshow(dat, vmin=0.0, vmax=1.0, interpolation=\"nearest\", cmap=cmap)\n",
    "        ax.xaxis.tick_bottom()\n",
    "        ax.set_xticks(np.arange(0, 50, 10))\n",
    "        ax.set_xticks(np.arange(0, 42, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(0, 50, 10))\n",
    "        ax.set_yticks(np.arange(0, 42, 1), minor=True)\n",
    "        ax.set_xticklabels(np.arange(0, 50, 10))\n",
    "        if i != 0:\n",
    "            ax.set_yticklabels([])\n",
    "        ax.set_xlim(0, 41)\n",
    "        ax.set_ylim(41, 0)\n",
    "        ax.tick_params(labelsize=24)\n",
    "\n",
    "        cax = axs[3]\n",
    "        shared = cax.get_shared_x_axes()\n",
    "        for a in shared.get_siblings(cax):\n",
    "            shared.remove(a)\n",
    "        fig.colorbar(im, cax=cax, fraction=0.046, pad=0.04, orientation=\"horizontal\")\n",
    "        cax.xaxis.set_ticks(np.arange(0, 1.5, 0.5))\n",
    "        cax.set_xticklabels(np.arange(0, 1.5, 0.5))\n",
    "        cax.tick_params(labelsize=16)\n",
    "        ax.set_xticks(np.arange(0, 50, 10))\n",
    "        ax.set_xticks(np.arange(0, 42, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(0, 50, 10))\n",
    "        ax.set_yticks(np.arange(0, 42, 1), minor=True)\n",
    "        ax.set_xticklabels(np.arange(0, 50, 10))\n",
    "\n",
    "        cax.xaxis.set_ticks(np.arange(0, 1.5, 0.5))\n",
    "        cax.set_xticklabels(np.arange(0, 1.5, 0.5))\n",
    "        cax.tick_params(labelsize=16)\n",
    "    plt.savefig(\"figs/structure-ox-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/structure-ox-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinetics\n",
    "### Koopman operators\n",
    "We can look at the Koopman operators and their errors directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax, mat, title in zip(axes, (koops[n].mean(axis=0)[global_sorter[n]][:, global_sorter[n]],\n",
    "                                 koops[n].std(axis=0)[global_sorter[n]][:, global_sorter[n]]),\n",
    "                          (\"$P$\", r\"$\\sigma(P)$\")):\n",
    "    ax.matshow(mat, vmin=0.0, vmax=0.02, interpolation=\"none\", cmap=\"GnBu\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(j, i, \"{0:2.4f}\".format(mat[i, j]), ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(title, fontsize=24)\n",
    "    ax.tick_params(length=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean first passage times\n",
    "We calculate mean first-passage times from our Koopman matrix $\\mathbf{K}(\\tau)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfpts, rates = {}, {}\n",
    "for n in outsizes:\n",
    "    mfpts[n] = np.zeros((attempts, n, n))\n",
    "    rates[n] = np.zeros((attempts, n, n))\n",
    "    for i in range(attempts):\n",
    "        for u in range(n):\n",
    "            for v in range(n):\n",
    "                if u == v:\n",
    "                    continue\n",
    "                koop = renormalize(koops[n][i])\n",
    "                f = tpt(koop, [u], [v])\n",
    "                rates[n][i, u, v] = f.rate\n",
    "                mfpts[n][i, u, v] = f.mfpt * 50 * dt * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax, mat, title in zip(axes, (mfpts[n].mean(axis=0)[global_sorter[n]][:, global_sorter[n]],\n",
    "                                 mfpts[n].std(axis=0)[global_sorter[n]][:, global_sorter[n]]),\n",
    "                          (r\"$\\mathrm{MFPT}$ [µs]\", r\"$\\sigma(\\mathrm{MFPT})$\")):\n",
    "    ax.matshow(mat, vmin=0.0, vmax=60, interpolation=\"nearest\", cmap=\"GnBu\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(j, i, \"{0:2.2f}\".format(mat[i, j]), ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(title, fontsize=24)\n",
    "    ax.tick_params(length=0)\n",
    "    plt.savefig(\"figs/mfpt-ox-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/mfpt-ox-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition rates\n",
    "The transition rates are the inverse of the mean first-passage times (MFPTs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "perms = 1e6 * rates[n] / (50 * dt)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax, mat, title in zip(axes, (perms.mean(axis=0)[global_sorter[n]][:, global_sorter[n]],\n",
    "                                 perms.std(axis=0)[global_sorter[n]][:, global_sorter[n]]),\n",
    "                          [r\"$k_{ij}$ [1/ms]\", \"$\\sigma(k_{ij})$ [1/ms]\"]):\n",
    "    ax.matshow(mat, vmin=0.0, vmax=1000, interpolation=\"none\", cmap=\"GnBu\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(j, i, \"{0:2.2f}\".format(mat[i, j]), ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(title, fontsize=24)\n",
    "    ax.tick_params(length=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifetimes\n",
    "The lifetimes are dependent on the probability of transition into the same state, they are given by:\n",
    "\n",
    "$$ \\overline{t}_i = \\frac{-\\tau}{\\log K_{ii}} $$\n",
    "\n",
    "where $K_{ii}$ are the diagonal elements of the transition matrix and $\\tau$ is the model lagtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetimes = {}\n",
    "for n in outsizes:\n",
    "    lt = np.empty((attempts, n))\n",
    "    for i in range(attempts):\n",
    "        lt[i] = -lag * dt / np.log(np.diag(koops[n][i])) * 1e-3\n",
    "    lifetimes[n] = lt.mean(axis=0)[global_sorter[n]], *(np.percentile(lt, q=(2.5, 97.5), axis=0)[:, global_sorter[n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylimit = 15\n",
    "for n in outsizes:\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n), lifetimes[n][0], yerr=[lifetimes[n][0] - lifetimes[n][1],\n",
    "                                                lifetimes[n][2] - lifetimes[n][0]],\n",
    "           color=colors, capsize=8)\n",
    "    for i in range(n):\n",
    "        if lifetimes[n][2][i] < ylimit:\n",
    "            ax.text(i, lifetimes[n][2][i] + 1.0, \"{:.2f}\".format(lifetimes[n][0][i]),\n",
    "                    fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, ylimit)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(r\"$\\overline{t}_i$ [µs]\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/lifetime-ox-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/lifetime-ox-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timescales\n",
    "We just plot the relaxation timescales here for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescales = {}\n",
    "with h5py.File(\"intermediate/data.hdf5\") as read:\n",
    "    store = read[\"ox\"]\n",
    "    for n in outsizes:\n",
    "        its = np.stack(store[\"{0}/{1}/its\".format(i, n)] for i in range(attempts))[:, ::-1] * 1e-3\n",
    "        timescales[n] = (its[:, :, -2].mean(axis=0), *(np.percentile(its[:, :, -2], q=(2.5, 97.5), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    fig = plt.figure(figsize=(n - 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n - 1), timescales[n][0], yerr=[timescales[n][0] - timescales[n][1],\n",
    "                                                     timescales[n][2] - timescales[n][0]],\n",
    "           color=colors[4:], capsize=8)\n",
    "    for i in range(n - 1):\n",
    "        ax.text(i, timescales[n][2][i] + 0.2, \"{:.2f}\".format(timescales[n][0][i]),\n",
    "                fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_xticks(np.arange(n - 1))\n",
    "    ax.set_ylabel(r\"$t_i$ [µs]\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/timescales-ox-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    plt.savefig(\"figs/timescales-ox-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equilibrium distribution\n",
    "We can look at the equilibrium distributions $\\mathbf{\\pi}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    pm = pis[n].mean(axis=0)[global_sorter[n]]\n",
    "    pv = np.percentile(pis[n], q=(2.5, 97.5), axis=0)[:, global_sorter[n]]\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n), pm, yerr=[pm - pv[0], pv[1] - pm], color=colors, capsize=8)\n",
    "    for i in range(n):\n",
    "        ax.text(i, pv[1, i] + 0.05, \"{:.2f}\".format(pm[i]), fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(\"Probability\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/pops-ox-{0}.pdf\".format(n), transparent=True, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"figs/pops-ox-{0}.svg\".format(n), transparent=True, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "pm = pis[n].mean(axis=0)[global_sorter[n]]\n",
    "pv = np.percentile(pis[n], q=(2.5, 97.5), axis=0)[:, global_sorter[n]]\n",
    "for i in range(n):\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cols = [(0.8, 0.8, 0.8)] * n\n",
    "    cols[i] = colors[i]\n",
    "    ax.bar(np.arange(n), pm, yerr=[pm - pv[0], pv[1] - pm], color=cols, capsize=8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.0, 0.5, 1.0])\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(\"P\", fontsize=32, labelpad=10)\n",
    "    ax.tick_params(labelsize=32)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    ax.text(0.7, 0.9, \"{0}: {1:2.0f} %\".format(i, pm[i] * 100), fontsize=32)\n",
    "    sns.despine(ax=ax)\n",
    "    plt.savefig(\"figs/pops-fine-ox-{0}-{1}.pdf\".format(n, i), transparent=True, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"figs/pops-fine-ox-{0}-{1}.svg\".format(n, i), transparent=True, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "We can get some idea of the \"entropy\" of each state by calculating the information entropy $S_i = -\\sum_t^N \\chi_i(\\mathbf{x}_t) \\log_2(\\chi_i(\\mathbf{x}_t)) $. In some sense, this encodes ambiguity in the state assignment, or how \"wide\" the state is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = {}\n",
    "for i, n in enumerate(outsizes):\n",
    "    ent = -np.nansum(pfsn_boot[n] * np.log2(pfsn_boot[n]) / np.log2(pfsn_boot[n].shape[1]), axis=1)\n",
    "    ents[n] = np.array([ent.mean(axis=0)[global_sorter[n]],\n",
    "                        *(np.percentile(ent, (2.5, 97.5), axis=0))[:, global_sorter[n]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    fig = plt.figure(figsize=(n * 1, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(np.arange(n), ents[n][0], yerr=[ents[n][0] - ents[n][1], ents[n][2] - ents[n][0]],\n",
    "           color=colors, capsize=8)\n",
    "    for i in range(n):\n",
    "        ax.text(i, ents[n][2, i] + 0.05, \"{:.2f}\".format(ents[n][0, i]), fontsize=20, ha=\"center\", va=\"center\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_ylabel(\"Normalized entropy\", fontsize=24, labelpad=10)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    ax.tick_params(axis=\"x\", length=0, pad=10)\n",
    "    sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph\n",
    "We will now look at the model in the classic graph format. A good way of projecting the states is on the space of the two slowest time-lagged independent components (tICs), as they separate the states very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = {}\n",
    "for n in outsizes:\n",
    "    state_tic = np.einsum(\"ijk,jl->ikl\", pfsn[n], ycon[:, :2])\n",
    "    pos[n] = state_tic.mean(axis=0)[global_sorter[n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mask out transition probabilities below a certain threshold, and define the crispness as $\\mathscr{c}_i := S_i^{-1}$, i.e. a crisper state is less ambiguous in it's state assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "minflux = 3e-4\n",
    "ps = np.empty((attempts, n, n))\n",
    "for i in range(attempts):\n",
    "    p = koops[n][i][global_sorter[n]][:, global_sorter[n]].copy()\n",
    "    u, v = np.where((np.diag(pis[n][i][global_sorter[n]]) @ p) < minflux)\n",
    "#     p[u, v] = 0.0\n",
    "    ps[i, :, :] = p\n",
    "\n",
    "crisp = 1 / ents[n][0]\n",
    "crisp /= crisp.max()\n",
    "posi = pos[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just to get the arrow thickness proportional to the flux,\n",
    "# when using external software like Illustrator...\n",
    "psm = ps.mean(axis=0)\n",
    "pmin, pmax = psm[psm > 0].min(), psm[psm < 0.9].max()\n",
    "psm[(psm == 0.0) | (psm > 0.9)] = np.nan\n",
    "psm -= pmin\n",
    "psm /= pmax\n",
    "psm * 3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "fig, posi = pe.plots.plot_network(\n",
    "    ps.mean(axis=0), pos=posi, state_sizes=pis[n].mean(axis=0)[global_sorter[n]], show_frame=True, ax=ax, arrow_curvature=1.2)\n",
    "plt.close();\n",
    "\n",
    "figpadding = 0.2\n",
    "xmin = posi[:, 0].min()\n",
    "xmax = posi[:, 0].max()\n",
    "Dx = xmax - xmin\n",
    "xmin -= Dx * figpadding\n",
    "xmax += Dx * figpadding\n",
    "Dx *= 1 + figpadding\n",
    "ymin = posi[:, 0].min()\n",
    "ymax = posi[:, 0].max()\n",
    "Dy = ymax - ymin\n",
    "ymin -= Dy * figpadding\n",
    "ymax += Dy * figpadding\n",
    "Dy *= 1 + figpadding\n",
    "sizes = min(Dx, Dy) ** 2 * 0.5 * pis[n].mean(axis=0)[global_sorter[n]] / (pis[n].mean(axis=0).max() * n)\n",
    "crispness = min(Dx, Dy) ** 2 * 0.5 * crisp\n",
    "\n",
    "# We need to redraw these to be able to show the crispness\n",
    "ax.artists = []\n",
    "for i in range(n):\n",
    "    ax.add_artist(plt.Circle(posi[i], radius=0.5 * np.sqrt(0.5 * sizes[i]),\n",
    "                             facecolor=colors[i], edgecolor=colors[i], alpha=0.5))\n",
    "    ax.add_artist(plt.Circle(posi[i], radius=0.1 * np.sqrt(0.5 * crispness[i]), facecolor=colors[i]))\n",
    "ax.tick_params(labelsize=24)\n",
    "ax.set_xticks(np.arange(-2, 3, 1))\n",
    "ax.set_yticks(np.arange(-2, 3, 1))\n",
    "ax.set_xticks(np.arange(-2, 2.1, 0.1), minor=True)\n",
    "ax.set_yticks(np.arange(-2, 2.1, 0.1), minor=True)\n",
    "ax.set_xlim(-2.1, 2.1)\n",
    "ax.set_ylim(-2.1, 2.1)\n",
    "ax.set_xlabel(r\"tIC 0\", fontsize=24, labelpad=10)\n",
    "ax.set_ylabel(r\"tIC 1\", fontsize=24, labelpad=10)\n",
    "sns.despine(ax=ax)\n",
    "fig.savefig(\"figs/graph-ox-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "fig.savefig(\"figs/graph-ox-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example structures\n",
    "We extract representative structures for each state, for example just the ones with highest weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatlengths = np.array(list(itertools.chain(*lengths_ox)))\n",
    "for n in outsizes:\n",
    "    spfs = pfs[n].mean(axis=0)[:, global_sorter[n]]\n",
    "    sids = spfs.argsort(axis=0)[-50:]\n",
    "    for s in range(n):\n",
    "        trjind = []\n",
    "        for ind in sids[:, s]:\n",
    "            pdiff = flatlengths.cumsum() - ind\n",
    "            trjidx = np.where(pdiff > 0)[0][0]\n",
    "            trjnr = pdiff[trjidx]\n",
    "            trjind.append((trjidx, trjnr - 1))\n",
    "\n",
    "        frames = md.join(md.load_frame(trajs_ox[trjidx], trjnr, top=top_ox)\n",
    "                         for i, (trjidx, trjnr) in enumerate(trjind))\n",
    "        frames.save_pdb(\"structures/state-metso-{0}-{1}-top.pdb\".format(n, s))\n",
    "        np.savetxt(\"structures/state-metso-{0}-{1}-top-p.dat\".format(n, s), spfs[:, s][sids[:, s]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or by sampling randomly based on these weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 50\n",
    "flatlengths = np.array(list(itertools.chain(*lengths_ox)))\n",
    "allinds = np.arange(nframes_ox, dtype=np.int64)\n",
    "for n in outsizes:\n",
    "    spfs = pfs[n].mean(axis=0)[:, global_sorter[n]]\n",
    "    for s in range(n):\n",
    "        p = spfs[:, s] / spfs[:, s].sum()\n",
    "        sids = np.random.choice(allinds, size=nsamples, replace=False, p=p)\n",
    "        trjind = []\n",
    "        for ind in sids:\n",
    "            pdiff = flatlengths.cumsum() - ind\n",
    "            trjidx = np.where(pdiff > 0)[0][0]\n",
    "            trjnr = pdiff[trjidx]\n",
    "            trjind.append((trjidx, trjnr - 1))\n",
    "\n",
    "        frames = md.join(md.load_frame(trajs_ox[trjidx], trjnr, top=top_ox)\n",
    "                         for i, (trjidx, trjnr) in enumerate(trjind))\n",
    "        frames.save_pdb(\"structures-alt/state-metso-{0}-{1}-top.pdb\".format(n, s))\n",
    "        np.savetxt(\"structures-alt/state-metso-{0}-{1}-top-p.dat\".format(n, s), spfs[:, s][sids])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
