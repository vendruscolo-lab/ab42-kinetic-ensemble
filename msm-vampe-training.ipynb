{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:22:51.709013Z",
     "start_time": "2020-03-21T17:22:51.705336Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill params\n",
    "ratio = 0.9          # Train-Test split ratio\n",
    "attempts = 20        # Number of times to run\n",
    "width = 256\n",
    "depth = 5\n",
    "learning_rate = 5e-2\n",
    "dropout = 0.0\n",
    "regularization = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "In this notebook we set up the neural networks with VAMPNet scoring functions and train them for different output sizes and estimate errors by bootstrap aggregation. This notebook can be used with `papermill` to run all cells automatically with given parameters. We first define the imports and useful utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in five rounds of 1024 simulations each, totalling 5119 runs (one simulation failed to run) at 278 K in the $NVT$ ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T13:58:09.498622Z",
     "start_time": "2020-03-21T13:58:09.474700Z"
    }
   },
   "outputs": [],
   "source": [
    "trajs = (sorted(glob(\"trajectories/r1/traj*.xtc\")) +\n",
    "         sorted(glob(\"trajectories/r2/traj*.xtc\")) +\n",
    "         sorted(glob(\"trajectories/r3/traj*.xtc\")) +\n",
    "         sorted(glob(\"trajectories/r4/traj*.xtc\")) +\n",
    "         sorted(glob(\"trajectories/r5/traj*.xtc\")))\n",
    "top = \"trajectories/topol.gro\"\n",
    "KBT = 2.311420 # 278 K\n",
    "\n",
    "# This is only really necessary for the residues in the plots\n",
    "topo = md.load_topology(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T13:58:20.141162Z",
     "start_time": "2020-03-21T13:58:10.417427Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat = pe.coordinates.featurizer(top)\n",
    "feat.add_residue_mindist()\n",
    "inpcon = pe.coordinates.source(trajs, feat)\n",
    "\n",
    "# Uncomment for full version:\n",
    "# lengths = sort_lengths(inpcon.trajectory_lengths(), [1024, 1023, 1024, 1024, 1024])\n",
    "lengths = inpcon.trajectory_lengths()\n",
    "nframes = inpcon.trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T13:58:20.152274Z",
     "start_time": "2020-03-21T13:58:20.143214Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Trajectories: {0}\".format(len(trajs)))\n",
    "print(\"Frames: {0}\".format(nframes))\n",
    "print(\"Time: {0:5.3f} µs\".format(inpcon.trajectory_lengths().sum() * 0.00025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAMPNet\n",
    "VAMPNet[1] is composed of two lobes, one reading the system features $\\mathbf{x}$ at a timepoint $t$ and the other after some lag time $\\tau$. In this case the network reads all minimum inter-residue distances (780 values) and sends them through 5 layers with 256 nodes each. The final layer uses between 2 and 8 *softmax* outputs to yield a state assignment vector $\\chi: \\mathbb{R}^m \\to \\Delta^{n}$ where $\\Delta^{n} = \\{ s \\in \\mathbb{R}^n \\mid 0 \\le s_i \\le 1, \\sum_i^n s_i = 1 \\}$ representing the probability of a state assignment. One lobe thus transforms a system state into a state occupation probability. We can also view this value as a kind of reverse ambiguity, i.e. how sure the network is that the system is part of a certain cluster. These outputs are then used as the input for the VAMP scoring function. We use the new enhanced version with physical constraints[2], particularly the ones for positive entries and reversibility.\n",
    "\n",
    "[1] Mardt, A., Pasquali, L., Wu, H. & Noé, F. VAMPnets for deep learning of molecular kinetics. Nat Comms 1–11 (2017). doi:10.1038/s41467-017-02388-1\n",
    "\n",
    "[2] Mardt, A., Pasquali, L., Noé, F. & Wu, H. Deep learning Markov and Koopman models with physical constraints. arXiv:1912.07392 [physics] (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) and first normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T13:58:21.550489Z",
     "start_time": "2020-03-21T13:58:20.153822Z"
    }
   },
   "outputs": [],
   "source": [
    "input_flat = np.load(\"intermediate/mindist-780-mini.npy\")\n",
    "# Uncomment for full version:\n",
    "# input_data = unflatten(input_flat, lengths)\n",
    "input_data = unflatten(input_flat, [lengths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters\n",
    "To allow for a larger hyperparameter search space, we use the self-normalizing neural network approach by Klambauer *et al.* [2], thus using SELU units, `AlphaDropout` and normalized `LeCun` weight initialization. The other hyperparameters are defined at the beginning of this notebook.\n",
    "\n",
    "[2] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-Normalizing Neural Networks. arXiv.org cs.LG, (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T13:58:21.556446Z",
     "start_time": "2020-03-21T13:58:21.552134Z"
    }
   },
   "outputs": [],
   "source": [
    "activation = \"selu\"              # NN activation function\n",
    "init = \"lecun_normal\"            # NN weight initialization\n",
    "lag = 20                         # Lag time\n",
    "n_epoch = 100                    # Max. number of epochs\n",
    "n_epoch_s = 10000                # Max. number of epochs for S optimization\n",
    "n_batch = 5000                   # Training batch size\n",
    "n_dims = input_data[0].shape[1]  # Input dimension\n",
    "nres = 42                        # Number of residues\n",
    "epsilon = 1e-7                   # Floating point noise\n",
    "dt = 0.25                        # Trajectory timestep in ns\n",
    "\n",
    "outsizes = np.array([2, 3, 4, 5, 6, 7, 8])\n",
    "lags = np.array([1, 2, 5, 10, 20, 50, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "We run the training several times with different train/test splits to get an error estimate, this is referred to as bootstrap aggregating (*bagging*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:27:39.580583Z",
     "start_time": "2020-03-21T17:27:29.448265Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = DataGenerator(input_data, ratio=ratio, dt=dt, max_frames=1000)\n",
    "for i in range(attempts):\n",
    "    generator.save(\"models/model-idx-{0}.hdf5\".format(i))\n",
    "    for n in outsizes:\n",
    "        print(\"Training n={0} i={1}/{2}\".format(n, i + 1, attempts))\n",
    "        koop = KoopmanModel(n=n, network_lag=lag, verbose=1, nnargs=dict(\n",
    "            width=width, depth=depth, learning_rate=learning_rate,\n",
    "            regularization=regularization, dropout=dropout,\n",
    "            batchnorm=True, lr_factor=1e-2))\n",
    "        koop.fit(generator)\n",
    "        koop.save(\"models/model-ve-{0}-{1}.hdf5\".format(n, i))\n",
    "        del koop\n",
    "        gc.collect()\n",
    "    generator.regenerate_indices()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
