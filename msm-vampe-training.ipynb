{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill params\n",
    "ratio = 0.9             # Train-Test split ratio\n",
    "attempt = 0             # Current attempt (passed in externally)\n",
    "width = 256             # Neural net width\n",
    "depth = 5               # Neural net depth\n",
    "learning_rate = 5e-2    # VAMP2 learning rate\n",
    "dropout = 0.0           # Alpha dropout strength\n",
    "regularization = 1e-8   # L2 regularization\n",
    "epsilon = 1e-7          # VAMP epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "In this notebook we set up the neural networks with VAMPNet scoring functions and train them for different output sizes and estimate errors by bootstrap aggregation. This notebook can be used with `papermill` to run all cells automatically with given parameters. We first define the imports and useful utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories for Aβ42 (Aβ42-MetSO) were acquired in 5 (3) rounds of 1024 simulations each, totalling 5119 (3071) runs (one simulation failed to run) at 278 K in the $NVT$ ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_names = (\"red\", \"ox\")\n",
    "top, trajs = {}, {}\n",
    "trajs = {k: sorted(glob(\"trajectories/{0}/r?/traj*.xtc\".format(k))) for k in sim_names}\n",
    "top = {k: \"trajectories/{0}/topol.gro\".format(k) for k in sim_names}\n",
    "KBT = 2.311420 # 278 K\n",
    "nres = 42\n",
    "traj_rounds = {\n",
    "    \"red\": [1024, 1023, 1024, 1024, 1024],\n",
    "    \"ox\": [1024, 1024, 1023],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inpcon = {}\n",
    "for k in sim_names:\n",
    "    feat = pe.coordinates.featurizer(top[k])\n",
    "    feat.add_residue_mindist()\n",
    "    inpcon[k] = pe.coordinates.source(trajs[k], feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths, nframes = {}, {}\n",
    "for i, k in enumerate(sim_names):\n",
    "    # Switch for full data:\n",
    "    lengths[k] = [inpcon[k].trajectory_lengths()]\n",
    "#     lengths[k] = sort_lengths(inpcon[k].trajectory_lengths(), traj_rounds[k])\n",
    "    nframes[k] = inpcon[k].trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\" + \"\\t\\t\".join(sim_names))\n",
    "print(\"\\n\".join((\n",
    "    \"Trajs: \\t\\t\" + \"\\t\\t\".join(\"{0}\".format(len(trajs[k])) for k in sim_names),\n",
    "    \"Frames: \\t\" + \"\\t\\t\".join(\"{0}\".format(nframes[k]) for k in sim_names),\n",
    "    \"Time: \\t\\t\" + \"\\t\".join(\"{0:5.3f} µs\".format(inpcon[k].trajectory_lengths().sum() * 0.00025)\n",
    "                           for k in sim_names)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAMPNet\n",
    "VAMPNet[1] is composed of two lobes, one reading the system features $\\mathbf{x}$ at a timepoint $t$ and the other after some lag time $\\tau$. In this case the network reads all minimum inter-residue distances (780 values) and sends them through 5 layers with 256 nodes each. The final layer uses between 2 and 6 *softmax* outputs to yield a state assignment vector $\\chi: \\mathbb{R}^m \\to \\Delta^{n}$ where $\\Delta^{n} = \\{ s \\in \\mathbb{R}^n \\mid 0 \\le s_i \\le 1, \\sum_i^n s_i = 1 \\}$ representing the probability of a state assignment. One lobe thus transforms a system state into a state occupation probability. We can also view this value as a kind of reverse ambiguity, i.e. how sure the network is that the system is part of a certain cluster. These outputs are then used as the input for the VAMP scoring function. We use the new enhanced version with physical constraints[2], particularly the ones for positive entries and reversibility.\n",
    "\n",
    "[1] Mardt, A., Pasquali, L., Wu, H. & Noé, F. VAMPnets for deep learning of molecular kinetics. Nat Comms 1–11 (2017). doi:10.1038/s41467-017-02388-1\n",
    "\n",
    "[2] Mardt, A., Pasquali, L., Noé, F. & Wu, H. Deep learning Markov and Koopman models with physical constraints. arXiv:1912.07392 [physics] (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) and first normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sim_names:\n",
    "    filename = \"intermediate/mindist-780-{0}.npy\".format(k)\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"No mindist file for {0} ensemble, calculating from scratch...\".format(k))\n",
    "        con = np.vstack(inpcon[k].get_output())\n",
    "        np.save(filename, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_flat, input_data = {}, {}\n",
    "for k in sim_names:\n",
    "    raw = np.load(\"intermediate/mindist-780-{0}.npy\".format(k))\n",
    "    raw_mean, raw_std = raw.mean(axis=0), raw.std(axis=0)\n",
    "    input_flat[k] = (raw - raw_mean) / raw_std\n",
    "    input_data[k] = [(r - raw_mean) / raw_std for r in unflatten(raw, lengths[k])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters\n",
    "To allow for a larger hyperparameter search space, we use the self-normalizing neural network approach by Klambauer *et al.* [3], thus using SELU units, `AlphaDropout` and normalized `LeCun` weight initialization. The other hyperparameters are defined at the beginning of this notebook.\n",
    "\n",
    "[3] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-Normalizing Neural Networks. arXiv.org cs.LG, (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 20                            # Network lag time\n",
    "n_dims = input_data[k][0].shape[1]  # Input dimension\n",
    "nres = 42                           # Number of residues\n",
    "dt = 0.25                           # Trajectory timestep in ns\n",
    "steps = 6                           # CK test steps\n",
    "bs_frames = 1000000                 # Number of frames in the bootstrap sample\n",
    "\n",
    "outsizes = np.array([4, 2, 3, 5, 6])\n",
    "lags = np.array([1, 2, 5, 10, 20, 50, 100])\n",
    "\n",
    "# The oxidised model was trained without batch normalisation,\n",
    "# it shouldn't be required due to the self-normalising nature anyway.\n",
    "batchnorm = {\"red\": True, \"ox\": False}\n",
    "\n",
    "# Comment for full data:\n",
    "bs_frames = nframes\n",
    "attempts = 2\n",
    "outsizes = np.array([4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "We run the training several times with different train/test splits to get an error estimate, this is referred to as bootstrap aggregating (*bagging*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"intermediate/temp-data.hdf5\", \"a\") as write:\n",
    "    for k in (\"red\", \"ox\"):        \n",
    "        # Create HDF5 groups\n",
    "        ens = write.require_group(k)\n",
    "        att = ens.require_group(str(attempt))\n",
    "        \n",
    "        # Generate\n",
    "        generator = DataGenerator(input_data[k], ratio=ratio, dt=dt, max_frames=1000000)\n",
    "        generator.save(\"models/model-idx-{0}-{1}.hdf5\".format(k, attempt))\n",
    "        for n in outsizes:\n",
    "            print(\"Training {0} n={1} i={2}...\".format(k, n, attempt))\n",
    "            out = att.require_group(str(n))\n",
    "            koop = KoopmanModel(n=n, network_lag=lag, verbose=1, nnargs=dict(\n",
    "                width=width, depth=depth, learning_rate=learning_rate,\n",
    "                regularization=regularization, dropout=dropout,\n",
    "                batchnorm=batchnorm[k], lr_factor=5e-3))\n",
    "            koop.fit(generator)\n",
    "            koop.save(\"models/model-ve-{0}-{1}-{2}.hdf5\".format(k, n, attempt))\n",
    "            print(\"Estimating Koopman operator...\")\n",
    "            ko = out.require_dataset(\"k\", shape=(n, n), dtype=\"float32\")\n",
    "            ko[:] = koop.estimate_koopman(lag=50)\n",
    "            print(\"Estimating mu...\")\n",
    "            mu = out.require_dataset(\"mu\", shape=(koop.data.n_train,), dtype=\"float32\")\n",
    "            mu[:] = koop.mu\n",
    "            print(\"Estimating implied timescales...\")\n",
    "            its = out.require_dataset(\"its\", shape=(n - 1, len(lags)), dtype=\"float32\")\n",
    "            its[:] = koop.its(lags)\n",
    "            print(\"Performing CK-test...\")\n",
    "            cke = out.require_dataset(\"cke\", shape=(n, n, steps), dtype=\"float32\")\n",
    "            ckp = out.require_dataset(\"ckp\", shape=(n, n, steps), dtype=\"float32\")\n",
    "            cke[:], ckp[:] = koop.cktest(steps)\n",
    "            print(\"Estimating chi...\")\n",
    "            bootstrap = out.require_dataset(\"bootstrap\", shape=(koop.data.n_train, 2 * n), dtype=\"float32\")\n",
    "            bootstrap[:] = koop.transform(koop.data.trains[0])\n",
    "            full = out.require_dataset(\"full\", shape=(nframes[k], 2 * n), dtype=\"float32\")\n",
    "            full[:] = koop.transform(generator.data_flat)\n",
    "            del koop\n",
    "            gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
