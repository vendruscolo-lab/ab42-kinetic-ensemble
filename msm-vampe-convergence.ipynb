{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill params\n",
    "ratio = 0.9          # Train-Test split ratio\n",
    "attempts = 20        # Number of times to run\n",
    "width = 256\n",
    "depth = 5\n",
    "learning_rate = 5e-2\n",
    "dropout = 0.0\n",
    "regularization = 1e-8\n",
    "epsilon = 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "In this notebook we set up the neural networks with VAMPNet scoring functions and train them for different output sizes and estimate errors by bootstrap aggregation. This notebook can be used with `papermill` to run all cells automatically with given parameters. We first define the imports and useful utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in five rounds of 1024 simulations each, totalling 5119 runs (one simulation failed to run) at 278 K in the $NVT$ ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_names = (\"red\", \"ox\")\n",
    "top, trajs = {}, {}\n",
    "trajs = {k: sorted(glob(\"trajectories/{0}/r?/traj*.xtc\".format(k))) for k in sim_names}\n",
    "top = {k: \"trajectories/{0}/topol.gro\".format(k) for k in sim_names}\n",
    "KBT = 2.311420 # 278 K\n",
    "nres = 42\n",
    "traj_rounds = {\n",
    "    \"red\": [1024, 1023, 1024, 1024, 1024],\n",
    "    \"ox\": [1024, 1024, 1023],\n",
    "}\n",
    "\n",
    "# This is only really necessary for the residues in the plots\n",
    "topo = md.load_topology(top[\"red\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inpcon = {}\n",
    "for k in sim_names:\n",
    "    feat = pe.coordinates.featurizer(top[k])\n",
    "    feat.add_residue_mindist()\n",
    "    inpcon[k] = pe.coordinates.source(trajs[k], feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths, nframes = {}, {}\n",
    "for i, k in enumerate(sim_names):\n",
    "    # Switch for full version:\n",
    "    # lengths[k] = sort_lengths(inpcon[k].trajectory_lengths(), traj_rounds[k])\n",
    "    lengths[k] = [inpcon[k].trajectory_lengths()]\n",
    "    nframes[k] = inpcon[k].trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\" + \"\\t\\t\".join(sim_names))\n",
    "print(\"\\n\".join((\n",
    "    \"Trajs: \\t\\t\" + \"\\t\\t\".join(\"{0}\".format(len(trajs[k])) for k in sim_names),\n",
    "    \"Frames: \\t\" + \"\\t\\t\".join(\"{0}\".format(nframes[k]) for k in sim_names),\n",
    "    \"Time: \\t\\t\" + \"\\t\".join(\"{0:5.3f} µs\".format(inpcon[k].trajectory_lengths().sum() * 0.00025)\n",
    "                           for k in sim_names)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAMPNet\n",
    "VAMPNet[1] is composed of two lobes, one reading the system features $\\mathbf{x}$ at a timepoint $t$ and the other after some lag time $\\tau$. In this case the network reads all minimum inter-residue distances (780 values) and sends them through 5 layers with 256 nodes each. The final layer uses between 2 and 8 *softmax* outputs to yield a state assignment vector $\\chi: \\mathbb{R}^m \\to \\Delta^{n}$ where $\\Delta^{n} = \\{ s \\in \\mathbb{R}^n \\mid 0 \\le s_i \\le 1, \\sum_i^n s_i = 1 \\}$ representing the probability of a state assignment. One lobe thus transforms a system state into a state occupation probability. We can also view this value as a kind of reverse ambiguity, i.e. how sure the network is that the system is part of a certain cluster. These outputs are then used as the input for the VAMP scoring function. We use the new enhanced version with physical constraints[2], particularly the ones for positive entries and reversibility.\n",
    "\n",
    "[1] Mardt, A., Pasquali, L., Wu, H. & Noé, F. VAMPnets for deep learning of molecular kinetics. Nat Comms 1–11 (2017). doi:10.1038/s41467-017-02388-1\n",
    "\n",
    "[2] Mardt, A., Pasquali, L., Noé, F. & Wu, H. Deep learning Markov and Koopman models with physical constraints. arXiv:1912.07392 [physics] (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) and first normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sim_names:\n",
    "    filename = \"intermediate/mindist-780-{0}.npy\".format(k)\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"No mindist file for {0} ensemble, calculating from scratch...\".format(k))\n",
    "        con = np.vstack(inpcon[k].get_output())\n",
    "        np.save(filename, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_flat, input_data = {}, {}\n",
    "for k in sim_names:\n",
    "    raw = np.load(\"intermediate/mindist-780-{0}.npy\".format(k))\n",
    "    raw_mean, raw_std = raw.mean(axis=0), raw.std(axis=0)\n",
    "    input_flat[k] = (raw - raw_mean) / raw_std\n",
    "    input_data[k] = [(r - raw_mean) / raw_std for r in unflatten(raw, lengths[k])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters\n",
    "To allow for a larger hyperparameter search space, we use the self-normalizing neural network approach by Klambauer *et al.* [2], thus using SELU units, `AlphaDropout` and normalized `LeCun` weight initialization. The other hyperparameters are defined at the beginning of this notebook.\n",
    "\n",
    "[2] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-Normalizing Neural Networks. arXiv.org cs.LG, (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 20                            # Lag time\n",
    "n_dims = input_data[k][0].shape[1]  # Input dimension\n",
    "nres = 42                           # Number of residues\n",
    "dt = 0.25                           # Trajectory timestep in ns\n",
    "bs_frames = 1000000                 # Number of frames in the bootstrap sample\n",
    "\n",
    "# The oxidised model was trained without batch normalisation\n",
    "batchnorm = {\"red\": True, \"ox\": False}\n",
    "\n",
    "# Comment for full version:\n",
    "bs_frames = nframes\n",
    "attempts = 2\n",
    "outsizes = np.array([4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "We would ideally like to see how converged our ensemble is with respect to the timescales and stationary distribution given by our model. We thus build trial models with different numbers of trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "for k in (\"red\", \"ox\"):\n",
    "    filename = \"intermediate/temp-k-conv-{0}-{1}.npy\".format(k, n)\n",
    "    k_conv = np.empty((len(traj_rounds[k]), attempts, n, n))\n",
    "    for j, nt in enumerate(np.cumsum(traj_rounds[k])):\n",
    "        for i in range(attempts):\n",
    "            generator = DataGenerator(input_data[k][:nt])\n",
    "            print(\"Analysing trajs={0} n={1} i={2}...\".format(j, n, i), end=\"\\r\")\n",
    "            koop = KoopmanModel(n=n, network_lag=lag, verbose=0, nnargs=dict(\n",
    "                width=width, depth=depth, learning_rate=learning_rate,\n",
    "                regularization=regularization, dropout=dropout,\n",
    "                batchnorm=batchnorm[k], lr_factor=5e-3))\n",
    "            koop.load(\"models/model-ve-{0}-{1}-{2}.hdf5\".format(k, n, i))\n",
    "            koop.generator = generator\n",
    "            k_conv[j, i] = koop.estimate_koopman(lag=50)\n",
    "    np.save(filename, k_conv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
