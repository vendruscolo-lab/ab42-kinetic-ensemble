{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network analysis\n",
    "\n",
    "In this notebook we use the trained neural networks to predict the Koopman operators, implied timescales, calculate the Chapman-Kolmogorov test and other things. This is setup in such a way as to use an environment with only tensorflow and Keras, and no other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T14:00:12.099075Z",
     "start_time": "2020-04-04T14:00:12.085076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contains the neural net definitions and imports\n",
    "%run model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in five rounds of 1024 simulations each, totalling 5119 runs (one simulation failed to run) at 278 K in the $NVT$ ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:37:03.540674Z",
     "start_time": "2020-04-04T13:37:03.514498Z"
    }
   },
   "outputs": [],
   "source": [
    "trajs = (sorted(glob(\"trajectories/r1/traj*.xtc\")) +\n",
    "         sorted(glob(\"trajectories/r2/traj*.xtc\")) +\n",
    "         sorted(glob(\"trajectories/r3/traj*.xtc\")) +\n",
    "         sorted(glob(\"trajectories/r4/traj*.xtc\")) +\n",
    "         sorted(glob(\"trajectories/r5/traj*.xtc\")))\n",
    "top = \"trajectories/topol.gro\"\n",
    "KBT = 2.311420 # 278 K\n",
    "traj_rounds = [1024, 2047, 3071, 4095, 5119]\n",
    "nres = 42\n",
    "\n",
    "# This is only really necessary for the residues in the plots\n",
    "topo = md.load_topology(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:37:15.574427Z",
     "start_time": "2020-04-04T13:37:05.650848Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat = pe.coordinates.featurizer(top)\n",
    "feat.add_residue_mindist()\n",
    "inpcon = pe.coordinates.source(trajs, feat)\n",
    "\n",
    "# Uncomment for full version:\n",
    "# lengths = sort_lengths(inpcon.trajectory_lengths(), [1024, 1023, 1024, 1024, 1024])\n",
    "lengths = inpcon.trajectory_lengths()\n",
    "nframes = inpcon.trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:37:16.357001Z",
     "start_time": "2020-04-04T13:37:16.347658Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Trajectories: {0}\".format(len(trajs)))\n",
    "print(\"Frames: {0}\".format(nframes))\n",
    "print(\"Time: {0:5.3f} µs\".format(inpcon.trajectory_lengths().sum() * 0.00025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAMPNet\n",
    "VAMPNet[1] is composed of two lobes, one reading the system features $\\mathbf{x}$ at a timepoint $t$ and the other after some lag time $\\tau$. In this case the network reads all minimum inter-residue distances (780 values) and sends them through 5 layers with 256 nodes each. The final layer uses between 2 and 8 *softmax* outputs to yield a state assignment vector $\\chi: \\mathbb{R}^m \\to \\Delta^{n}$ where $\\Delta^{n} = \\{ s \\in \\mathbb{R}^n \\mid 0 \\le s_i \\le 1, \\sum_i^n s_i = 1 \\}$ representing the probability of a state assignment. One lobe thus transforms a system state into a state occupation probability. We can also view this value as a kind of reverse ambiguity, i.e. how sure the network is that the system is part of a certain cluster. These outputs are then used as the input for the VAMP scoring function. We use the new enhanced version with physical constraints[2], particularly the ones for positive entries and reversibility.\n",
    "\n",
    "[1] Mardt, A., Pasquali, L., Wu, H. & Noé, F. VAMPnets for deep learning of molecular kinetics. Nat Comms 1–11 (2017). doi:10.1038/s41467-017-02388-1\n",
    "\n",
    "[2] Mardt, A., Pasquali, L., Noé, F. & Wu, H. Deep learning Markov and Koopman models with physical constraints. arXiv:1912.07392 [physics] (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) for the neural network, but remove the 2nd and 3rd off-diagonals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:37:24.549249Z",
     "start_time": "2020-04-04T13:37:22.138872Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"intermediate/mindist-780-mini.npy\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing file for ensemble: {0}\".format(filename))\n",
    "    input_flat = np.load(filename)\n",
    "else:\n",
    "    print(\"No mindist file for ensemble, calculating from scratch...\")\n",
    "    input_flat = np.vstack(inpcon.get_output())\n",
    "    np.save(filename, input_flat)\n",
    "# Uncomment for full version:\n",
    "# input_data = unflatten(input_flat, lengths)\n",
    "input_data = unflatten(input_flat, [lengths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the full minimum inter-residue distances for some analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:37:27.169839Z",
     "start_time": "2020-04-04T13:37:24.550868Z"
    }
   },
   "outputs": [],
   "source": [
    "allpairs = np.asarray(list(itertools.combinations(range(nres), 2)))\n",
    "filename = \"intermediate/mindist-all-mini.npy\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing file for ensemble: {0}\".format(filename))\n",
    "    mindist_flat = np.load(filename)\n",
    "else:\n",
    "    print(\"No mindist file for ensemble, calculating from scratch...\")\n",
    "    feat = pe.coordinates.featurizer(top)\n",
    "    feat.add_residue_mindist(residue_pairs=allpairs)\n",
    "    inpmindist = pe.coordinates.source(trajs, feat)\n",
    "    mindist_flat = np.vstack(inpmindist.get_output())\n",
    "    np.save(filename, mindist_flat)\n",
    "# Uncomment for full version:\n",
    "# mindist_data = unflatten(mindist_flat, lengths)\n",
    "mindist = unflatten(mindist_flat, [lengths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters\n",
    "To allow for a larger hyperparameter search space, we use the self-normalizing neural network approach by Klambauer *et al.* [3], thus using SELU units, `AlphaDropout` and normalized `LeCun` weight initialization. The other hyperparameters are defined at the beginning of this notebook.\n",
    "\n",
    "[3] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-Normalizing Neural Networks. arXiv.org cs.LG, (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:37:27.178118Z",
     "start_time": "2020-04-04T13:37:27.171560Z"
    }
   },
   "outputs": [],
   "source": [
    "activation = \"selu\"              # NN activation function\n",
    "init = \"lecun_normal\"            # NN weight initialization\n",
    "lag = 50                         # Lag time\n",
    "n_epoch = 100                    # Max. number of epochs\n",
    "n_epoch_s = 10000                # Max. number of epochs for S optimization\n",
    "n_batch = 5000                   # Training batch size\n",
    "n_dims = input_data[0].shape[1]  # Input dimension\n",
    "nres = 42                        # Number of residues\n",
    "epsilon = 1e-7                   # Floating point noise\n",
    "dt = 0.25                        # Trajectory timestep in ns\n",
    "steps = 6                        # CK test steps\n",
    "bs_frames = 900000               # Number of frames in the bootstrap sample\n",
    "ratio = 0.9                      # Train-Test split ratio\n",
    "attempts = 20                    # Number of times to run\n",
    "width = 256                      # Layer width\n",
    "depth = 5                        # Number of layers\n",
    "learning_rate = 5e-2             # Learning rate for Chi layers\n",
    "dropout = 0.0                    # Dropout for Chi layers\n",
    "regularization = 1e-8            # L2 regularization strength for Chi layers\n",
    "epsilon = 1e-6\n",
    "\n",
    "outsizes = np.array([2, 3, 4, 5, 6, 7, 8])\n",
    "lags = np.array([1, 2, 5, 10, 20, 50, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Model validation\n",
    "We load the previously trained neural network models and calculate the implied timescales, Chapman-Kolmogorov test, and the Koopman operators. This can take a long time, as the constraint vectors have to be re-estimated for every lag time, so we save the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:37:07.537289Z",
     "start_time": "2020-04-03T13:55:48.355834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Takes ~5 days on a V100\n",
    "with h5py.File(\"intermediate/data.hdf5\", \"w\") as write:\n",
    "    for i in range(attempts):\n",
    "        att = write.create_group(str(i))\n",
    "        generator = DataGenerator.from_state(input_data, \"models/model-idx-{0}.hdf5\".format(i))\n",
    "        for n in outsizes:\n",
    "            print(\"Analysing n={0} i={1}...\".format(n, i))\n",
    "            out = att.create_group(str(n))\n",
    "            koop = KoopmanModel(n=n, network_lag=lag, verbose=0, nnargs=dict(\n",
    "                width=width, depth=depth, learning_rate=learning_rate,\n",
    "                regularization=regularization, dropout=dropout,\n",
    "                batchnorm=True, lr_factor=1e-2))\n",
    "            koop.load(\"models/model-ve-{0}-{1}.hdf5\".format(n, i))\n",
    "            koop.generator = generator\n",
    "            out.create_dataset(\"k\", data=koop.estimate_koopman(lag=50))\n",
    "            out.create_dataset(\"mu\", data=koop.mu)\n",
    "            out.create_dataset(\"its\", data=koop.its(lags))\n",
    "            ckes, ckps = koop.cktest(steps)\n",
    "            out.create_dataset(\"cke\", data=ckes)\n",
    "            out.create_dataset(\"ckp\", data=ckps)\n",
    "            out.create_dataset(\"bootstrap\", data=koop.transform(koop.data.trains[0]))\n",
    "            out.create_dataset(\"full\", data=koop.transform(generator.data_flat))\n",
    "            del ckes, ckps, koop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "We would ideally like to see how converged our ensemble is with respect to the timescales and stationary distribution given by our model. We thus build trial models with different numbers of trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:05:09.339070Z",
     "start_time": "2020-04-04T13:05:09.336491Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T11:41:40.918802Z",
     "start_time": "2020-04-04T02:40:06.535407Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"intermediate/k-conv-{0}-t-test.npy\".format(n)\n",
    "k_conv = np.empty((len(traj_rounds), attempts, n, n))\n",
    "for j, nt in enumerate(traj_rounds):\n",
    "    for i in range(attempts):\n",
    "        generator = DataGenerator(input_data[:nt])\n",
    "        print(\"Analysing trajs={0} n={1} i={2}...\".format(j, n, i), end=\"\\r\")\n",
    "        koop = KoopmanModel(n=n, network_lag=lag, verbose=0, nnargs=dict(\n",
    "            width=width, depth=depth, learning_rate=learning_rate,\n",
    "            regularization=regularization, dropout=dropout,\n",
    "            batchnorm=True, lr_factor=1e-2))\n",
    "        koop.load(\"models/model-ve-{0}-{1}.hdf5\".format(n, i))\n",
    "        koop.generator = generator\n",
    "        k_conv[j, i] = koop.estimate_koopman(lag=50)\n",
    "np.save(filename, k_conv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
