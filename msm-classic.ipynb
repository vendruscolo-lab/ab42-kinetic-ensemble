{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional model\n",
    "We build a traditional MSM using the usual tICA $\\to$ $k$-Means $\\to$ MSM $\\to$ HMM/PCCA approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gc\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "from typing import List, Tuple, Sequence\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, ticker\n",
    "from matplotlib.colors import ListedColormap\n",
    "from msmbuilder.cluster import MiniBatchKMeans, GMM, MiniBatchKMedoids, AgglomerativeClustering\n",
    "from msmtools.analysis import stationary_distribution, mfpt\n",
    "from msmtools.flux import tpt\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyemma as pe\n",
    "from scipy.linalg import eig\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "\n",
    "# Plot settings\n",
    "sns.set_palette(\"husl\", 8)\n",
    "rc(\"font\", **{\"family\": \"Helvetica\",\n",
    "              \"sans-serif\": [\"Helvetica\"]})\n",
    "rc(\"svg\", **{\"fonttype\": \"none\"})\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(source: np.ndarray, lengths: List[int]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Takes an array and returns a list of arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    source\n",
    "        Array to be unflattened.\n",
    "    lengths\n",
    "        List of integers giving the length of each subarray.\n",
    "        Must sum to the length of source.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unflat\n",
    "        List of arrays.\n",
    "    \n",
    "    \"\"\"\n",
    "    conv = []\n",
    "    lp = 0\n",
    "    for arr in lengths:\n",
    "        arrconv = []\n",
    "        for le in arr:\n",
    "            arrconv.append(source[lp:le + lp])\n",
    "            lp += le\n",
    "        conv.append(arrconv)\n",
    "    ccs = list(itertools.chain(*conv))\n",
    "    return ccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_lengths(flatlengths: Sequence[int], shapes: Sequence[int]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Takes a list of lengths and returns a list of lists of lengths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    flatlengths\n",
    "        List of lengths\n",
    "    shapes\n",
    "        List of shapes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lengths\n",
    "        List of lists of lengths\n",
    "    \n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "    i = 0\n",
    "    for n in shapes:\n",
    "        arr = []\n",
    "        for _ in range(n):\n",
    "            arr.append(flatlengths[i])\n",
    "            i += 1\n",
    "        lengths.append(arr)\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitudes(mag, step=1):\n",
    "    return np.concatenate([np.arange(1, 10, step) * (10 ** i) for i in mag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in five rounds of 1024 simulations each, totalling 5119 runs (one simulation failed to run) at 278 K in the $NVT$ ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs = sorted(glob(\"trajectories/red/r?/traj*.xtc\"))\n",
    "top = \"trajectories/red/topol.gro\"\n",
    "KBT = 2.311420 # 278 K\n",
    "traj_rounds = [1024, 2047, 3071, 4095, 5119]\n",
    "nres = 42\n",
    "dt = 0.25\n",
    "\n",
    "# This is only really necessary for the residues in the plots\n",
    "topo = md.load_topology(top)\n",
    "lags = np.array([1, 2, 5, 10, 20, 50, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pe.coordinates.featurizer(top)\n",
    "feat.add_residue_mindist()\n",
    "inpcon = pe.coordinates.source(trajs, feat)\n",
    "\n",
    "# Switch for full version:\n",
    "# lengths = sort_lengths(inpcon.trajectory_lengths(), [1024, 1023, 1024, 1024, 1024])\n",
    "lengths = [inpcon.trajectory_lengths()]\n",
    "nframes = inpcon.trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trajectories: {0}\".format(len(trajs)))\n",
    "print(\"Frames: {0}\".format(nframes))\n",
    "print(\"Time: {0:5.3f} µs\".format(inpcon.trajectory_lengths().sum() * 0.00025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpairs = np.asarray(list(itertools.combinations(range(nres), 2)))\n",
    "filename = \"intermediate/mindist-all-red.npy\"\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing file for ensemble: {0}\".format(filename))\n",
    "    mindist_flat = np.load(filename)\n",
    "else:\n",
    "    print(\"No mindist file for ensemble, calculating from scratch...\")\n",
    "    feat = pe.coordinates.featurizer(top)\n",
    "    feat.add_residue_mindist(residue_pairs=allpairs)\n",
    "    inpmindist = pe.coordinates.source(trajs, feat)\n",
    "    mindist_flat = np.vstack(inpmindist.get_output())\n",
    "    np.save(filename, mindist_flat)\n",
    "mindist = unflatten(mindist_flat, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "### TICA\n",
    "Time-lagged independent component analysis is a special case of Koopman operator estimation using a linear projection [1]. We solve the following generalized eigenvalue problem:\n",
    "\n",
    "$$ \\mathbf{C}_{01}v = \\lambda \\mathbf{C}_{00} v $$\n",
    "\n",
    "The eigenvectors encode the slowest dynamics of the system, and we use them as a convenient visualization technique.\n",
    "\n",
    "[1]\tPérez-Hernández, G., Paul, F., Giorgino, T., De Fabritiis, G. & Noé, F. Identification of slow molecular order parameters for Markov model construction. The Journal of Chemical Physics 139, 015102–14 (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticacon = pe.coordinates.tica(mindist, lag=20, dim=-1, kinetic_map=True)\n",
    "ticscon = ticacon.get_output()\n",
    "ycon = np.vstack(ticscon)\n",
    "\n",
    "print(\"tIC Dimensions: {0}\".format(ycon.shape[1]))\n",
    "print(\"Required dimensions for 90 %: {0}\".format(ticacon.cumvar[ticacon.cumvar < 0.9].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Free energy surface\n",
    "We also show the free energy surface projected onto the two slowest tICs in the form of a kernel density estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gaussian_kde(ycon[::10, :2].T)\n",
    "xmin, ymin, *_ = ycon.min(axis=0)\n",
    "xmax, ymax, *_ = ycon.max(axis=0)\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "posi = np.vstack((X.ravel(), Y.ravel()))\n",
    "Z = kernel(posi).reshape(X.shape)\n",
    "mat = np.rot90(Z.copy())\n",
    "mat[mat < 0.01] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "cmap = \"plasma\"\n",
    "F = -KBT * np.log(Z)\n",
    "F -= F.min()\n",
    "ax.contourf(X, Y, F, np.arange(0.0, 10, 1), cmap=cmap)\n",
    "ax.contour(X, Y, F, np.arange(0.0, 10, 1), cmap=cmap, linewidth=10)\n",
    "ax.tick_params(labelsize=24)\n",
    "ax.set_xlabel(r\"tIC 1\", fontsize=24, labelpad=10)\n",
    "ax.set_ylabel(r\"tIC 2\", fontsize=24, labelpad=10)\n",
    "sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter scan\n",
    "We will first scan through hyperparameters, in particular we will look at the dependence of the model on the number of tICs, the number of clusters, and the clustering algorithm itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "n_clusters = [10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "\n",
    "def cv(ntic, clust):\n",
    "    results = []\n",
    "    fulltics = [np.ascontiguousarray(tic[:, :ntic]) for tic in ticscon]\n",
    "    for n_clust in n_clusters:\n",
    "        model = pe.msm.MaximumLikelihoodMSM(lag=50)\n",
    "        for fold, (train_inds, test_inds) in enumerate(kf.split(fulltics)):\n",
    "            print(\"{0} clusters, fold {1}\".format(n_clust, fold), end=\"\\r\")\n",
    "            train = [fulltics[i] for i in train_inds]\n",
    "            test = [fulltics[i] for i in test_inds]\n",
    "\n",
    "            cluster = clust(n_clust)\n",
    "            cluster.fit([np.ascontiguousarray(t[::50]) for t in train])\n",
    "            cluster_train = cluster.predict([np.ascontiguousarray(t) for t in train])\n",
    "            cluster_test = cluster.predict([np.ascontiguousarray(t) for t in test])\n",
    "\n",
    "            model.fit(cluster_train)\n",
    "            train_score = model.score(cluster_train)\n",
    "            test_score = model.score(cluster_test)\n",
    "\n",
    "            results.append(dict(train=train_score, test=test_score, n_clust=n_clust, fold=fold))\n",
    "    return results\n",
    "\n",
    "res = {}\n",
    "for ntic in [2, 4, 8, 16, 32]:\n",
    "    res[ntic] = {}\n",
    "    for clust in [MiniBatchKMeans, MiniBatchKMedoids, GMM]:\n",
    "        print(\"{0} {1}\".format(ntic, clust.__name__))\n",
    "        try:\n",
    "            res[ntic][clust.__name__] = cv(ntic, clust)\n",
    "        except MemoryError:\n",
    "            print(\"Memory error, continuing...\")\n",
    "\n",
    "for k, v in res.items():\n",
    "    with open(\"intermediate/scan-{0}-tic.pkl\".format(k), \"wb\") as f:\n",
    "        pickle.dump(v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, if necessary\n",
    "res = {}\n",
    "for k in [2, 4, 8, 16, 32]:\n",
    "    with open(\"intermediate/scan-{0}-tic.pkl\".format(k), \"rb\") as f:\n",
    "        res[k] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[fold[\"train\"], fold[\"test\"]]\n",
    "                for ntic in res.values()\n",
    "                for kind in ntic.values()\n",
    "                for fold in kind]).reshape(5, 3, 8, 5, 2)\n",
    "\n",
    "with pd.ExcelWriter(\"intermediate/si-fig-1.xlsx\") as xls:\n",
    "    da = xr.DataArray(data=arr,\n",
    "                      dims=(\"n_tics\", \"algorithm\", \"n_clusters\", \"folds\", \"type\"),\n",
    "                      coords=dict(n_tics=[2, 4, 8, 16, 32],\n",
    "                                  algorithm=[clust.__name__ for clust in\n",
    "                                             [MiniBatchKMeans, MiniBatchKMedoids, GMM]],\n",
    "                                  n_clusters=[10, 20, 50, 100, 200, 500, 1000, 2000],\n",
    "                                  type=[\"train\", \"test\"]),\n",
    "                      name=\"Hyperparameter scan\")\n",
    "    (da.to_dataset(\"n_clusters\")\n",
    "       .to_dataframe()\n",
    "       .to_excel(xls, sheet_name=\"Hyperparameter scan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = len(res[8].keys()), len(res.keys())\n",
    "fig, axes = plt.subplots(ny, nx, figsize=(4 * nx, 4 * ny),\n",
    "                         sharex=True, sharey=True)\n",
    "for i, ktic in enumerate(sorted(res.keys())):\n",
    "    for j, kmodel in enumerate(sorted(res[ktic].keys())):\n",
    "        ax = np.atleast_2d(axes)[i, j]\n",
    "        data = pd.DataFrame(res[ktic][kmodel])\n",
    "        meds = data.groupby(\"n_clust\").median()\n",
    "        best_n = meds[\"test\"].argmax()\n",
    "        score = meds.loc[best_n, \"test\"]\n",
    "        ax.plot(meds.index, meds[\"train\"], linewidth=2, color=colors[4], label=\"Train\")\n",
    "        ax.plot(meds.index, meds[\"test\"], linewidth=2, color=colors[5], label=\"Test\")\n",
    "        ax.scatter(data[\"n_clust\"], data[\"train\"], marker=\"o\", color=colors[4], alpha=0.5, label=None)\n",
    "        ax.scatter(data[\"n_clust\"], data[\"test\"], marker=\"o\", color=colors[5], alpha=0.5, label=None)\n",
    "        ax.plot(best_n, score, marker=\"*\", color=\"black\", markersize=20)\n",
    "        ax.text(10, 1, r\"$MVC = {0:4.3}$\".format(score), fontsize=24)\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xticks(magnitudes([1, 2, 3, 4]))\n",
    "        ax.set_xlim(5, 5000)\n",
    "        if i == 0:\n",
    "            ax.set_title(kmodel, fontsize=24)\n",
    "        if i == ny - 1:\n",
    "            ax.set_xlabel(\"# Clusters\", fontsize=24)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(\"Score\", fontsize=24)\n",
    "        if j == 2:\n",
    "            ax.yaxis.set_label_position(\"right\")\n",
    "            ax.set_ylabel(\"{0} tICs\".format(ktic), fontsize=24, labelpad=20)\n",
    "        ax.set_ylim(0, 10)\n",
    "        ax.tick_params(labelsize=24)\n",
    "fig.savefig(\"figs/trad-mvcscan.pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "fig.savefig(\"figs/trad-mvcscan.svg\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a fine grained model using the best hyperparameters (*k*-Means, 200 clusters, 16 tICs) and evaluate the implied timescales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "n_clust, n_tic = 200, 16\n",
    "cluster = MiniBatchKMeans(n_clust)\n",
    "cluster.fit([ycon[::50, :n_tic]])\n",
    "dtrajs = cluster.predict([t[:, :n_tic] for t in ticscon])\n",
    "its = pe.msm.timescales_msm(dtrajs, lags=lags, n_jobs=8, errors=\"bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"intermediate/si-fig-2-a.xlsx\") as xls:\n",
    "    da = xr.DataArray(data=np.stack([its.get_timescales(), its.sample_mean, its.sample_std]),\n",
    "                      dims=(\"kind\", \"lagtimes\", \"timescales\"),\n",
    "                      coords=dict(kind=[\"timescales\", \"sample mean\", \"sample std\"]),\n",
    "                      name=\"Implied timescales\")\n",
    "    (da.to_dataset(\"lagtimes\")\n",
    "       .to_dataframe()\n",
    "       .to_excel(xls, sheet_name=\"Timescales 200 clusters\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfl = its.sample_mean - its.sample_std\n",
    "cfu = its.sample_mean + its.sample_std\n",
    "mits = its.get_timescales()\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(8):\n",
    "    ax.semilogy(its.lags * dt, mits[:, i] * dt, marker=\"o\", linewidth=1.5, color=colors[i])\n",
    "    ax.plot(its.lags * dt, its.sample_mean[:, i] * dt, marker=\"o\",\n",
    "            linewidth=1.5, color=colors[i], linestyle=\"dashed\")\n",
    "    ax.fill_between(its.lags * dt, cfl[:, i] * dt, cfu[:, i] * dt,\n",
    "                    interpolate=True, color=colors[i], alpha=0.2)\n",
    "ax.plot(its.lags * dt, its.lags * dt, color=\"k\")\n",
    "ax.fill_between(its.lags * dt, ax.get_ylim()[0] * np.ones(lags.shape[0]),\n",
    "                its.lags * dt, color=\"k\", alpha=0.2)\n",
    "ax.set_yticks(magnitudes([0, 1, 2, 3, 4, 5]))\n",
    "ax.set_ylim(1, 100000)\n",
    "sns.despine(ax=ax)\n",
    "ax.set_xlabel(r\"$\\tau$ [ns]\", fontsize=24)\n",
    "ax.set_ylabel(r\"$t_i$ [ns]\", fontsize=24)\n",
    "ax.tick_params(labelsize=24)\n",
    "fig.savefig(\"figs/its-trad-{0}-{1}-km.pdf\".format(n_clust, n_tic), bbox_inches=\"tight\", transparent=True)\n",
    "fig.savefig(\"figs/its-trad-{0}-{1}-km.svg\".format(n_clust, n_tic), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now attempt coarse-graining using Perron cluster-cluster analysis (PCCA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsizes = np.array([2, 3, 4, 5, 6])\n",
    "n_samples = 20\n",
    "its = {n: np.zeros((lags.shape[0], n - 1, n_samples)) for n in outsizes}\n",
    "for i, tau in enumerate(lags):\n",
    "    msm = pe.msm.bayesian_markov_model(dtrajs, lag=tau)\n",
    "    for n in outsizes:\n",
    "        print(\"lag={0}, n={1}\".format(tau, n), end=\"\\r\")\n",
    "        for j, idx in enumerate(np.random.randint(100, size=n_samples)):\n",
    "            pcca = msm.samples[idx].pcca(n)\n",
    "            lambdas = np.linalg.eigvals(pcca.coarse_grained_transition_matrix)[1:]\n",
    "            ts = -tau * dt / np.log(lambdas)\n",
    "            ts[(ts < 0.0) | (ts > 1e5)] = np.nan\n",
    "            its[n][i, :, j] = ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in outsizes:\n",
    "    itsm = np.nanmean(its[n], axis=-1)\n",
    "    itsl, itsh = np.nanpercentile(its[n], q=(2.5, 97.5), axis=-1)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    for i in range(n - 1):\n",
    "        ax.semilogy(lags * dt, itsm[:, i], marker=\"o\", linewidth=1.5, color=colors[i])\n",
    "        ax.plot(lags * dt, itsm[:, i], marker=\"o\",\n",
    "                linewidth=1.5, color=colors[i], linestyle=\"dashed\")\n",
    "        ax.fill_between(lags * dt, itsl[:, i], itsh[:, i],\n",
    "                        interpolate=True, color=colors[i], alpha=0.2)\n",
    "    ax.plot(lags * dt, lags * dt, color=\"k\")\n",
    "    ax.fill_between(lags * dt, ax.get_ylim()[0] * np.ones(lags.shape[0]),\n",
    "                    lags * dt, color=\"k\", alpha=0.2)\n",
    "    ax.set_yticks(magnitudes([0, 1, 2, 3, 4, 5]))\n",
    "    ax.set_ylim(1, 100000)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(r\"$\\tau$ [ns]\", fontsize=24)\n",
    "    ax.set_ylabel(r\"$t_i$ [ns]\", fontsize=24)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    fig.savefig(\"figs/its-pcca-1e5-{0}.pdf\".format(n), bbox_inches=\"tight\", transparent=True)\n",
    "    fig.savefig(\"figs/its-pcca-1e5-{0}.svg\".format(n), bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"intermediate/si-fig-2-e-f.xlsx\") as xls:\n",
    "    for n in [4, 6]:\n",
    "        da = xr.DataArray(data=its[n],\n",
    "                          dims=(\"lagtimes\", \"timescales\", \"attempts\"),\n",
    "                          name=\"Implied timescales PCCA\")\n",
    "        (da.to_dataset(\"attempts\")\n",
    "           .to_dataframe()\n",
    "           .to_excel(xls, sheet_name=\"Implied timescales PCCA {0} clusters\".format(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now attempt to build coarse-grained hidden Markov state models with different numbers of coarse-grained states and evaluate their respective timescales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its = {}\n",
    "for n in [2, 4, 6]:\n",
    "    its[n] = pe.msm.timescales_hmsm(dtrajs, n, lags=lags, n_jobs=1, errors=\"bayes\", nsamples=100, stride=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 7\n",
    "for n in [2, 4, 6]:\n",
    "    cfl, cfu = its[n].get_sample_conf()\n",
    "    mits = its[n].get_timescales()\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    for i in range(n - 1):\n",
    "        ax.semilogy(its[n].lags[:n_lags] * dt, mits[:, i][:n_lags] * dt, marker=\"o\", linewidth=1.5, color=colors[i])\n",
    "        ax.plot(its[n].lags[:n_lags] * dt, its[n].sample_mean[:, i][:n_lags] * dt, marker=\"o\",\n",
    "                linewidth=1.5, color=colors[i], linestyle=\"dashed\")\n",
    "        ax.fill_between(its[n].lags[:n_lags] * dt, cfl[:, i][:n_lags] * dt, cfu[:, i][:n_lags] * dt,\n",
    "                        interpolate=True, color=colors[i], alpha=0.2)\n",
    "    ax.plot(its[n].lags[:n_lags] * dt, its[n].lags[:n_lags] * dt, color=\"k\")\n",
    "    ax.fill_between(its[n].lags[:n_lags] * dt, ax.get_ylim()[0] * np.ones(its[n].lags[:n_lags].shape[0]),\n",
    "                    its[n].lags[:n_lags] * dt, color=\"k\", alpha=0.2)\n",
    "    ax.set_yticks(magnitudes([0, 1, 2, 3, 4, 5]))\n",
    "    ax.set_ylim(1, 100000)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(r\"$\\tau$ [ns]\", fontsize=24)\n",
    "    ax.set_ylabel(r\"$t_i$ [ns]\", fontsize=24)\n",
    "    ax.tick_params(labelsize=24)\n",
    "    fig.savefig(\"figs/its-trad-hmm-{2}-{0}-{1}-km.pdf\".format(n_clust, n_tic, n),\n",
    "                bbox_inches=\"tight\", transparent=True)\n",
    "    fig.savefig(\"figs/its-trad-hmm-{2}-{0}-{1}-km.svg\".format(n_clust, n_tic, n),\n",
    "                bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"intermediate/si-fig-2-b-c-d.xlsx\") as xls:\n",
    "    for n in [2, 4, 6]:\n",
    "        da = xr.DataArray(data=np.stack([its[n].get_timescales(), its[n].sample_mean, its[n].sample_std]),\n",
    "                          dims=(\"kind\", \"lagtimes\", \"timescales\"),\n",
    "                          coords=dict(kind=[\"timescales\", \"sample mean\", \"sample std\"]),\n",
    "                          name=\"Implied timescales\")\n",
    "        (da.to_dataset(\"lagtimes\")\n",
    "           .to_dataframe()\n",
    "           .to_excel(xls, sheet_name=\"HMM timescales {0} clusters\".format(n)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
