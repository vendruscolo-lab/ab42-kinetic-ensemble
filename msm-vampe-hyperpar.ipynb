{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:15.988980Z",
     "start_time": "2020-03-21T17:42:15.985561Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill params\n",
    "nevals = 100         # Number of hyperparameter iterations\n",
    "n = 4                # Output size\n",
    "ratio = 0.9          # Train-Test split ratio\n",
    "n_runs = 3           # Number of times to run with the same parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "In this notebook we set up the neural networks with VAMPNet scoring functions and train them for different output sizes and estimate errors by bootstrap aggregation. This notebook can be used with `papermill` to run all cells automatically with given parameters. We first define the imports and useful utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:31.759838Z",
     "start_time": "2020-03-21T17:42:29.088932Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "import os\n",
    "from typing import List, Tuple, Sequence\n",
    "import warnings\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import pyemma as pe\n",
    "from scipy.linalg import eig\n",
    "import seaborn as sns\n",
    "\n",
    "import vampnet\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Dense, Activation, Flatten, Input, BatchNormalization,\n",
    "                                     concatenate, Dropout, AlphaDropout, Layer)\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import skopt\n",
    "from skopt import space as sp\n",
    "from skopt.plots import plot_convergence, plot_objective\n",
    "\n",
    "# Plot settings\n",
    "sns.set_palette(\"husl\", 8)\n",
    "rc(\"font\", **{\"family\": \"Helvetica\",\n",
    "              \"sans-serif\": [\"Helvetica\"]})\n",
    "rc(\"svg\", **{\"fonttype\": \"none\"})\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "vamp = vampnet.VampnetTools(epsilon=1e-5)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "The version of Keras we're using unfortunately doesn't have `restore_best_weights` implemented, so I copied this from a newer version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:42.603287Z",
     "start_time": "2020-03-21T17:42:42.588545Z"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(Callback):\n",
    "    \"\"\"Stop training when a monitored quantity has stopped improving.\n",
    "    # Arguments\n",
    "        monitor: quantity to be monitored.\n",
    "        min_delta: minimum change in the monitored quantity\n",
    "            to qualify as an improvement, i.e. an absolute\n",
    "            change of less than min_delta, will count as no\n",
    "            improvement.\n",
    "        patience: number of epochs that produced the monitored\n",
    "            quantity with no improvement after which training will\n",
    "            be stopped.\n",
    "            Validation quantities may not be produced for every\n",
    "            epoch, if the validation frequency\n",
    "            (`model.fit(validation_freq=5)`) is greater than one.\n",
    "        verbose: verbosity mode.\n",
    "        mode: one of {auto, min, max}. In `min` mode,\n",
    "            training will stop when the quantity\n",
    "            monitored has stopped decreasing; in `max`\n",
    "            mode it will stop when the quantity\n",
    "            monitored has stopped increasing; in `auto`\n",
    "            mode, the direction is automatically inferred\n",
    "            from the name of the monitored quantity.\n",
    "        baseline: Baseline value for the monitored quantity to reach.\n",
    "            Training will stop if the model doesn't show improvement\n",
    "            over the baseline.\n",
    "        restore_best_weights: whether to restore model weights from\n",
    "            the epoch with the best value of the monitored quantity.\n",
    "            If False, the model weights obtained at the last step of\n",
    "            training are used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 monitor='val_loss',\n",
    "                 min_delta=0,\n",
    "                 patience=0,\n",
    "                 verbose=0,\n",
    "                 mode='auto',\n",
    "                 baseline=None,\n",
    "                 restore_best_weights=False):\n",
    "        super(EarlyStopping, self).__init__()\n",
    "\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.min_delta = min_delta\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_weights = None\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('EarlyStopping mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % mode,\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "        else:\n",
    "            if 'acc' in self.monitor:\n",
    "                self.monitor_op = np.greater\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "\n",
    "        if self.monitor_op == np.greater:\n",
    "            self.min_delta *= 1\n",
    "        else:\n",
    "            self.min_delta *= -1\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Allow instances to be re-used\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        if self.baseline is not None:\n",
    "            self.best = self.baseline\n",
    "        else:\n",
    "            self.best = np.Inf if self.monitor_op == np.less else -np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = self.get_monitor_value(logs)\n",
    "        if current is None:\n",
    "            return\n",
    "\n",
    "        if self.monitor_op(current - self.min_delta, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                if self.restore_best_weights:\n",
    "                    if self.verbose > 0:\n",
    "                        print('Restoring model weights from the end of '\n",
    "                              'the best epoch')\n",
    "                    self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
    "\n",
    "    def get_monitor_value(self, logs):\n",
    "        monitor_value = logs.get(self.monitor)\n",
    "        if monitor_value is None:\n",
    "            warnings.warn(\n",
    "                'Early stopping conditioned on metric `%s` '\n",
    "                'which is not available. Available metrics are: %s' %\n",
    "                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
    "            )\n",
    "        return monitor_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:46.342061Z",
     "start_time": "2020-03-21T17:42:46.337837Z"
    }
   },
   "outputs": [],
   "source": [
    "class CheckpointSaver(object):\n",
    "    \"\"\"\n",
    "    Save current state after each iteration with `skopt.dump`.\n",
    "    Example usage:\n",
    "        import skopt\n",
    "        checkpoint_callback = skopt.callbacks.CheckpointSaver(\"./result.pkl\")\n",
    "        skopt.gp_minimize(obj_fun, dims, callback=[checkpoint_callback])\n",
    "    Parameters\n",
    "    ----------\n",
    "    * `checkpoint_path`: location where checkpoint will be saved to;\n",
    "    * `dump_options`: options to pass on to `skopt.dump`, like `compress=9`\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path, **dump_options):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.dump_options = dump_options\n",
    "\n",
    "    def __call__(self, res):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        * `res` [`OptimizeResult`, scipy object]:\n",
    "            The optimization as a OptimizeResult object.\n",
    "        \"\"\"\n",
    "        skopt.utils.dump(res, self.checkpoint_path, **self.dump_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:49.468106Z",
     "start_time": "2020-03-21T17:42:49.463024Z"
    }
   },
   "outputs": [],
   "source": [
    "def unflatten(source: np.ndarray, lengths: List[int]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Takes an array and returns a list of arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    source\n",
    "        Array to be unflattened.\n",
    "    lengths\n",
    "        List of integers giving the length of each subarray.\n",
    "        Must sum to the length of source.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unflat\n",
    "        List of arrays.\n",
    "    \n",
    "    \"\"\"\n",
    "    conv = []\n",
    "    lp = 0\n",
    "    for arr in lengths:\n",
    "        arrconv = []\n",
    "        for le in arr:\n",
    "            arrconv.append(source[lp:le + lp])\n",
    "            lp += le\n",
    "        conv.append(arrconv)\n",
    "    ccs = list(itertools.chain(*conv))\n",
    "    return ccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:50.084140Z",
     "start_time": "2020-03-21T17:42:50.079511Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_lengths(flatlengths: Sequence[int], shapes: Sequence[int]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Takes a list of lengths and returns a list of lists of lengths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    flatlengths\n",
    "        List of lengths\n",
    "    shapes\n",
    "        List of shapes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lengths\n",
    "        List of lists of lengths\n",
    "    \n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "    i = 0\n",
    "    for n in shapes:\n",
    "        arr = []\n",
    "        for _ in range(n):\n",
    "            arr.append(flatlengths[i])\n",
    "            i += 1\n",
    "        lengths.append(arr)\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:50.555386Z",
     "start_time": "2020-03-21T17:42:50.551147Z"
    }
   },
   "outputs": [],
   "source": [
    "def triu_inverse(x: np.ndarray, n: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts flattened upper-triangular matrices into full symmetric matrices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        Flattened matrices\n",
    "    n\n",
    "        Size of the n * n matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mat\n",
    "        Array of shape (length, n, n)\n",
    "    \n",
    "    \"\"\"\n",
    "    length = x.shape[0]\n",
    "    mat = np.zeros((length, n, n))\n",
    "    a, b = np.triu_indices(n, k=1)\n",
    "    mat[:, a, b] = x\n",
    "    mat += mat.swapaxes(1, 2)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:52.164252Z",
     "start_time": "2020-03-21T17:42:52.152642Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAMPu(Layer):\n",
    "    def __init__(self, units, activation, **kwargs):\n",
    "        self.M = units\n",
    "        self.activation = activation\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.u_kernel = self.add_weight(name=\"u_var\", shape=(self.M, ), trainable=True,\n",
    "                                        initializer=tf.constant_initializer(1. / self.M))\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [self.M] * 2 + [(self.M, self.M)] * 4 + [self.M]\n",
    "    \n",
    "    def _tile(self, x, n_batch):\n",
    "        x_exp = tf.expand_dims(x, axis=0)\n",
    "        shape = x.get_shape().as_list()\n",
    "        return tf.tile(x_exp, [n_batch, *([1] * len(shape))])\n",
    "    \n",
    "    def call(self, x):\n",
    "        chi_t, chi_tau = x\n",
    "        n_batch = tf.shape(chi_t)[0]\n",
    "        norm = 1. / tf.cast(n_batch, dtype=tf.float32)\n",
    "        \n",
    "        corr_tau = norm * tf.matmul(chi_tau, chi_tau, transpose_a=True)\n",
    "        chi_mean = tf.reduce_mean(chi_tau, axis=0, keepdims=True)\n",
    "        kernel_u = tf.expand_dims(self.activation(self.u_kernel), axis=0)\n",
    "        \n",
    "        u = kernel_u / tf.reduce_sum(chi_mean * kernel_u, keepdims=True)\n",
    "        v = tf.matmul(corr_tau, u, transpose_b=True)\n",
    "        mu = norm * tf.matmul(chi_tau, u, transpose_b=True)\n",
    "        sigma = tf.matmul(chi_tau * mu, chi_tau, transpose_a=True)\n",
    "        gamma = chi_tau * tf.matmul(chi_tau, u, transpose_b=True)\n",
    "        \n",
    "        C00 = norm * tf.matmul(chi_t, chi_t, transpose_a=True)\n",
    "        C11 = norm * tf.matmul(gamma, gamma, transpose_a=True)\n",
    "        C01 = norm * tf.matmul(chi_t, gamma, transpose_a=True)\n",
    "        \n",
    "        return [\n",
    "            self._tile(var, n_batch) for var in (u, v, C00, C11, C01, sigma)\n",
    "        ] + [mu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:52.665049Z",
     "start_time": "2020-03-21T17:42:52.650520Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAMPS(Layer):\n",
    "    def __init__(self, units, activation, order=20, renorm=False, **kwargs):\n",
    "        self.M = units\n",
    "        self.activation = activation\n",
    "        self.renorm = renorm\n",
    "        self.order = order\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.S_kernel = self.add_weight(name=\"S_var\", shape=(self.M, self.M), trainable=True,\n",
    "                                        initializer=tf.constant_initializer(0.1))\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(self.M, self.M)] * 2 + [self.M] + [(self.M, self.M)]\n",
    "    \n",
    "    def call(self, x):\n",
    "        if len(x) == 5:\n",
    "            v, C00, C11, C01, sigma = x\n",
    "        else:\n",
    "            chi_t, chi_tau, u, v, C00, C11, C01, sigma = x\n",
    "            u = u[0]\n",
    "        \n",
    "        n_batch = tf.shape(v)[0]\n",
    "        norm = 1. / tf.cast(n_batch, dtype=tf.float32)\n",
    "        C00, C11, C01 = C00[0], C11[0], C01[0]\n",
    "        sigma, v = sigma[0], v[0]\n",
    "        \n",
    "        kernel_w = self.activation(self.S_kernel)\n",
    "        w1 = kernel_w + tf.transpose(kernel_w)\n",
    "        w_norm = w1 @ v\n",
    "        \n",
    "        # Numerical problems with using a high p-norm\n",
    "        if self.renorm:\n",
    "            quasi_inf_norm = lambda x: tf.reduce_max(tf.abs(x))\n",
    "            w1 = w1 / quasi_inf_norm(w_norm)\n",
    "            w_norm = w1 @ v\n",
    "        \n",
    "        w2 = (1 - tf.squeeze(w_norm)) / tf.squeeze(v)\n",
    "        S = w1 + tf.linalg.diag(w2)\n",
    "        \n",
    "        if len(x) == 8:\n",
    "            q = (norm * tf.transpose(tf.matmul(S, chi_tau, transpose_b=True))\n",
    "                 * tf.matmul(chi_tau, u, transpose_b=True))\n",
    "            probs = tf.reduce_sum(chi_t * q, axis=1)\n",
    "        \n",
    "        K = S @ sigma\n",
    "        vamp_e = tf.transpose(S) @ C00 @ S @ C11 - 2 * tf.transpose(S) @ C01\n",
    "        vamp_e_tile = tf.tile(tf.expand_dims(vamp_e, axis=0), [n_batch, 1, 1])\n",
    "        K_tile = tf.tile(tf.expand_dims(K, axis=0), [n_batch, 1, 1])\n",
    "        S_tile = tf.tile(tf.expand_dims(S, axis=0), [n_batch, 1, 1])\n",
    "        \n",
    "        if len(x) == 5:\n",
    "            return [vamp_e_tile, K_tile, tf.zeros((n_batch, self.M)), S_tile]\n",
    "        else:\n",
    "            return [vamp_e_tile, K_tile, probs, S_tile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:53.077248Z",
     "start_time": "2020-03-21T17:42:53.070846Z"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_inverse(mat):\n",
    "    \"\"\"\n",
    "    Calculates the inverse of a square matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mat\n",
    "        Square real matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    inv\n",
    "        Inverse of the matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    eigva, eigveca = np.linalg.eigh(mat)\n",
    "    inc = eigva > epsilon\n",
    "    eigv, eigvec = eigva[inc], eigveca[:, inc]\n",
    "    return eigvec @ np.diag(1. / eigv) @ eigvec.T\n",
    "\n",
    "def covariances(data):\n",
    "    \"\"\"\n",
    "    Calculates (lagged) covariances.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        Data at time t and t + tau\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    C0inv\n",
    "        Inverse covariance\n",
    "    Ctau\n",
    "        Lagged covariance\n",
    "    \n",
    "    \"\"\"\n",
    "    chil, chir = data\n",
    "    norm = 1. / chil.shape[0]\n",
    "    C0, Ctau = norm * chil.T @ chil, norm * chil.T @ chir\n",
    "    C0inv = matrix_inverse(C0)\n",
    "    return C0inv, Ctau\n",
    "\n",
    "def _compute_pi(K):\n",
    "    \"\"\"\n",
    "    Calculates the stationary distribution of a transition matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    K\n",
    "        Transition matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pi\n",
    "        Normalized stationary distribution\n",
    "    \n",
    "    \"\"\"\n",
    "    eigv, eigvec = np.linalg.eig(K.T)\n",
    "    pi_v = eigvec[:, ((eigv - 1) ** 2).argmin()]\n",
    "    return pi_v / pi_v.sum(keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:53.572699Z",
     "start_time": "2020-03-21T17:42:53.567544Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence, List, Union, Generator, Callable, Any, Dict, TypeVar, Set\n",
    "from collections import UserList\n",
    "from pathlib import Path\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "MaybeListType = Union[List[T], T]\n",
    "NNDataType = Tuple[List[np.ndarray], np.ndarray]\n",
    "MaybePathType = Union[Path, str]\n",
    "\n",
    "FRAMES, DIMENSIONS, FIRST, LAST = 0, 1, 0, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:54.091634Z",
     "start_time": "2020-03-21T17:42:54.087812Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_list(item: MaybeListType[T], cls=list) -> List[T]:\n",
    "    \"\"\"\n",
    "    Turn an object into a list, if it isn't already.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    item\n",
    "        Item to contain in a list\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List with item as only element\n",
    "    \n",
    "    \"\"\"\n",
    "    if not isinstance(item, list):\n",
    "        item = [item]\n",
    "    return cls(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:54.628966Z",
     "start_time": "2020-03-21T17:42:54.616851Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, trains: List[np.ndarray], valids: List[np.ndarray]=None,\n",
    "                 y_train: np.ndarray=None, y_valid: np.ndarray=None):\n",
    "        \"\"\"\n",
    "        DataSet - Container for training and validation data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        trains\n",
    "            List of training datasets\n",
    "        valids\n",
    "            List of validation datasets\n",
    "        y_train\n",
    "            Dummy training target data\n",
    "        y_valid\n",
    "            Dummy validation target data\n",
    "        \n",
    "        \"\"\"\n",
    "        self.trains = trains\n",
    "        self.valids = valids\n",
    "        self.y_train = y_train\n",
    "        self.y_valid = y_valid\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.n_train\n",
    "    \n",
    "    def __getitem__(self, key: int) -> \"DataSet\":\n",
    "        if isinstance(key, int):\n",
    "            data = self.__class__([t[key][None] for t in self.trains],\n",
    "                                  [t[key][None] for t in self.valids])\n",
    "        else:\n",
    "            data = self.__class__([t[key] for t in self.trains],\n",
    "                                  [t[key] for t in self.valids])\n",
    "        if self.n is not None:\n",
    "            data.n = self.n\n",
    "        return data\n",
    "    \n",
    "    @property\n",
    "    def n_train(self) -> int:\n",
    "        \"\"\"Number of training samples.\"\"\"\n",
    "        return self.trains[FIRST].shape[FRAMES]\n",
    "    \n",
    "    @property\n",
    "    def n_valid(self) -> int:\n",
    "        \"\"\"Number of validation samples.\"\"\"\n",
    "        return self.valids[FIRST].shape[FRAMES]\n",
    "    \n",
    "    @property\n",
    "    def n_dims(self) -> int:\n",
    "        \"\"\"Number of input dimensions.\"\"\"\n",
    "        return self.trains[FIRST].shape[DIMENSIONS]\n",
    "    \n",
    "    @property\n",
    "    def n(self) -> int:\n",
    "        \"\"\"Number of output dimensions.\"\"\"\n",
    "        if self.y_train is None:\n",
    "            return None\n",
    "        return self.y_train.shape[DIMENSIONS]\n",
    "    \n",
    "    @n.setter\n",
    "    def n(self, n: int):\n",
    "        self.y_train = np.zeros((self.n_train, n))\n",
    "        self.y_valid = np.zeros((self.n_valid, n))\n",
    "    \n",
    "    @property\n",
    "    def train(self) -> NNDataType:\n",
    "        \"\"\"Training and target data pair.\"\"\"\n",
    "        return self.trains, self.y_train\n",
    "    \n",
    "    @property\n",
    "    def valid(self) -> NNDataType:\n",
    "        \"\"\"Validation and target data pair.\"\"\"\n",
    "        return self.valids, self.y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:55.136527Z",
     "start_time": "2020-03-21T17:42:55.112448Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, data: MaybeListType[np.ndarray],\n",
    "                 ratio: float=0.9, dt: float=1.0, max_frames: int=None):\n",
    "        \"\"\"\n",
    "        DataGenerator - Produces data for training a Koopman model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Input data as (a list of) ndarrays with\n",
    "            frames as rows and features as columns\n",
    "        ratio\n",
    "            Train / validation split ratio\n",
    "        dt\n",
    "            Timestep of the underlying data\n",
    "        max_frames\n",
    "            The maximum number of frames to use\n",
    "        \n",
    "        \"\"\"\n",
    "        self._data = make_list(data)\n",
    "        self.ratio = ratio\n",
    "        self.dt = dt\n",
    "        self.max_frames = max_frames or self.n_points\n",
    "        \n",
    "        # Generate lag = 0 indices, we will use these for different\n",
    "        # lag times later. That way we can retrain with essentially\n",
    "        # the same data for different lag times.\n",
    "        self.regenerate_indices()\n",
    "    \n",
    "    @property\n",
    "    def data(self) -> List[np.ndarray]:\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def n_dims(self) -> int:\n",
    "        \"\"\"Number of dimensions in the input data.\"\"\"\n",
    "        return self.data[FIRST].shape[DIMENSIONS]\n",
    "    \n",
    "    @property\n",
    "    def n_points(self) -> int:\n",
    "        \"\"\"Number of frames in the input data.\"\"\"\n",
    "        return sum(self.traj_lengths)\n",
    "    \n",
    "    @property\n",
    "    def n_traj(self) -> int:\n",
    "        \"\"\"Number of trajectories in the input data.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    @property\n",
    "    def traj_lengths(self) -> int:\n",
    "        \"\"\"Length of all trajectories in the input data.\"\"\"\n",
    "        return [len(t) for t in self.data]\n",
    "    \n",
    "    @property\n",
    "    def data_flat(self) -> np.ndarray:\n",
    "        \"\"\"The flattened input data.\"\"\"\n",
    "        return np.vstack(self.data)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_state(cls, data: MaybeListType[np.ndarray],\n",
    "                   filename: MaybePathType) -> \"DataGenerator\":\n",
    "        \"\"\"\n",
    "        Creates a DataGenerator object from previously saved index data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Input data as (a list of) ndarrays with\n",
    "            frames as rows and features as columns\n",
    "        filename\n",
    "            File to load the indices from.\n",
    "        \n",
    "        \"\"\"\n",
    "        gen = cls(data)\n",
    "        gen.load(filename)\n",
    "        \n",
    "        # Check for data consistency\n",
    "        assert gen.n_traj == len(data), \"Inconsistent data lengths!\"\n",
    "        assert all(len(gen._indices[i]) == gen.traj_lengths[i]\n",
    "                   for i in range(gen.n_traj)), \"Inconsistent trajectory lengths!\"\n",
    "        return gen\n",
    "    \n",
    "    def regenerate_indices(self):\n",
    "        \"\"\"Regenerate random indices.\"\"\"\n",
    "        # We use a dict here because we might otherwise desync\n",
    "        # our indices and trajectories when generating the \n",
    "        # train and test data. This way we're sure we're\n",
    "        # accessing the correct indices.\n",
    "        self._indices = {}\n",
    "        for i, traj in enumerate(self.data):\n",
    "            inds = np.arange(traj.shape[FRAMES])\n",
    "            np.random.shuffle(inds)\n",
    "            self._indices[i] = inds\n",
    "        \n",
    "        # We will also shuffle the whole dataset to avoid\n",
    "        # preferentially sampling late round trajectories.\n",
    "        # These are more indices than we will need in practice,\n",
    "        # because the trajectories are shortened through the\n",
    "        # lag time. We will just cut out the extra ones later.\n",
    "        self._full_indices = np.random.choice(\n",
    "            np.arange(self.max_frames), size=self.max_frames, replace=False)\n",
    "    \n",
    "    def save(self, filename: MaybePathType):\n",
    "        \"\"\"\n",
    "        Save the generator state in the form of indices.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename\n",
    "            File to save the indices to.\n",
    "        \n",
    "        \"\"\"\n",
    "        with h5py.File(handle_path(filename, non_existent=True), \"w\") as write:\n",
    "            # Save the individual trajectory indices\n",
    "            inds = write.create_group(\"indices\")\n",
    "            for k, v in self._indices.items():\n",
    "                inds[str(k)] = v\n",
    "            \n",
    "            # Save the indices on a trajectory level\n",
    "            dset = write.create_dataset(\"full_indices\", data=self._full_indices)\n",
    "            dset.attrs.update(_get_serializable_attributes(self))\n",
    "    \n",
    "    def load(self, filename: MaybePathType):\n",
    "        \"\"\"\n",
    "        Load the generator state from indices.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename\n",
    "            File to load the indices from.\n",
    "        \n",
    "        \"\"\"\n",
    "        with h5py.File(handle_path(filename), \"r\") as read:\n",
    "            # Object state (ratio etc...)\n",
    "            self.__dict__.update(read[\"full_indices\"].attrs)\n",
    "            self._full_indices = read[\"full_indices\"][:]\n",
    "            \n",
    "            # All indices\n",
    "            self._indices = {int(k): v[:] for k, v in read[\"indices\"].items()}\n",
    "    \n",
    "    def _generate_indices(self, lag: int) -> Dict[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generates indices corresponding to a particular lag time.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lag\n",
    "            The lag time for data preparation\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        indices\n",
    "            Dictionary of trajectory indices with selected frames\n",
    "        \n",
    "        \"\"\"\n",
    "        indices = {}\n",
    "        for k, inds in self._indices.items():\n",
    "            max_points = inds.shape[FRAMES] - lag\n",
    "            \n",
    "            # Lag time longer than our trajectory\n",
    "            if max_points <= 0:\n",
    "                continue\n",
    "                \n",
    "            indices[k] = inds[inds < max_points]\n",
    "        return indices\n",
    "    \n",
    "    def __call__(self, n: int, lag: int) -> DataSet:\n",
    "        \"\"\"\n",
    "        Creates the data for training the neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n\n",
    "            The size of the output\n",
    "        lag\n",
    "            The lag time in steps to be used\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data\n",
    "            DataSet of training and test data\n",
    "\n",
    "        \"\"\"\n",
    "        xt_shuf = []\n",
    "        xttau_shuf = []\n",
    "        indices = self._generate_indices(lag)\n",
    "\n",
    "        for i, traj in enumerate(self.data):\n",
    "            n_points = traj.shape[FRAMES]\n",
    "\n",
    "            # We'll just skip super short trajectories for now\n",
    "            if n_points <= lag:\n",
    "                continue\n",
    "\n",
    "            xt = traj[:n_points - lag]\n",
    "            xttau = traj[lag:]\n",
    "            xt_shuf.append(xt[indices[i]])\n",
    "            xttau_shuf.append(xttau[indices[i]])\n",
    "\n",
    "        xt = np.vstack(xt_shuf).astype(np.float32)\n",
    "        xttau = np.vstack(xttau_shuf).astype(np.float32)\n",
    "\n",
    "        eff_len = min(xt.shape[FRAMES], self.max_frames)\n",
    "        train_len = int(np.floor(eff_len * self.ratio))\n",
    "        \n",
    "        # Reshuffle to remove trajectory level bias\n",
    "        inds = self._full_indices[self._full_indices < eff_len]\n",
    "        xt, xttau = xt[inds], xttau[inds]\n",
    "\n",
    "        return DataSet(\n",
    "            [xt[:train_len], xttau[:train_len]],\n",
    "            [xt[train_len:eff_len], xttau[train_len:eff_len]],\n",
    "            np.zeros((train_len, 2 * n), dtype=np.float32),\n",
    "            np.zeros((eff_len - train_len, 2 * n), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:55.619418Z",
     "start_time": "2020-03-21T17:42:55.614768Z"
    }
   },
   "outputs": [],
   "source": [
    "class KeepLast(UserList):\n",
    "    def __init__(self, data: Sequence[T]):\n",
    "        \"\"\"\n",
    "        Constructs a list that will always keep the first item.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Data to construct the list from\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data = list(reversed(data))\n",
    "    \n",
    "    def pop_first(self) -> T:\n",
    "        \"\"\"\n",
    "        Returns the first item from the list, but only deletes\n",
    "        it if there's at least one more item in the list.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        item\n",
    "            First item\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(self) < 2:\n",
    "            return self.data[LAST]\n",
    "        return self.data.pop(LAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:56.126290Z",
     "start_time": "2020-03-21T17:42:56.120108Z"
    }
   },
   "outputs": [],
   "source": [
    "def _split(data: np.ndarray, axis=LAST) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Utility function for splitting the output from two network lobes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        Array to split\n",
    "    axis\n",
    "        Axis to split along\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    split\n",
    "        2 arrays of half width\n",
    "    \n",
    "    \"\"\"\n",
    "    n = data.shape[axis] // 2\n",
    "    return [data[:, :n], data[:, n:]]\n",
    "\n",
    "def handle_path(path: MaybePathType, non_existent: bool=False) -> Path:\n",
    "    \"\"\"\n",
    "    Check path validity and return `Path` object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path\n",
    "        Filepath to be checked.\n",
    "    non_existent\n",
    "        If false, will raise an error if the path does not exist.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    path\n",
    "        The converted and existing path.\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(path, Path):\n",
    "        try:\n",
    "            path = Path(path)\n",
    "        except Exception as err:\n",
    "            message = \"Couldn't read path {0}! Original message: {1}\"\n",
    "            raise ValueError(message.format(path, err))\n",
    "    if not path.exists() and not non_existent:\n",
    "        raise IOError(\"File {0} does not exist!\".format(path))\n",
    "    if not path.parent.exists():\n",
    "        path.parent.mkdir()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:56.629292Z",
     "start_time": "2020-03-21T17:42:56.625046Z"
    }
   },
   "outputs": [],
   "source": [
    "VALIDS = {int, float, str, list}\n",
    "def _get_serializable_attributes(obj: object) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Finds all object attributes that are serializable with HDF5.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obj\n",
    "        Object to serialize\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    attributes\n",
    "        All serializable public attributes\n",
    "    \n",
    "    \"\"\"\n",
    "    return {k: v for k, v in obj.__dict__.items()\n",
    "            if any(isinstance(v, valid) for valid in VALIDS)\n",
    "            and not k.startswith(\"_\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:57.111649Z",
     "start_time": "2020-03-21T17:42:57.089024Z"
    }
   },
   "outputs": [],
   "source": [
    "class NNModel:\n",
    "    def __init__(self, model: Union[Model, Layer],\n",
    "                 loss: MaybeListType[Callable[..., Any]]=None,\n",
    "                 metric: MaybeListType[Callable[..., Any]]=None,\n",
    "                 learning_rate: MaybeListType[float]=None,\n",
    "                 batchsize: int=5000,\n",
    "                 epochs: int=100,\n",
    "                 callback: Callback=None,\n",
    "                 save_initial_weights=False,\n",
    "                 verbose: int=0):\n",
    "        \"\"\"\n",
    "        Neural network model interface class for Keras.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        loss\n",
    "            Loss function(s) to use, if more than one is specified\n",
    "            subsequent calls to `compile` or `compile_fit` will use\n",
    "            the next loss function.\n",
    "        metric\n",
    "            Fitting metric(s) to use\n",
    "        learning_rate\n",
    "            Learning rate for the Adam optimizer. Will accept\n",
    "            multiple learning rates like `loss`.\n",
    "        batchsize\n",
    "            Batchsize to use for training and testing\n",
    "        epochs\n",
    "            The maximum number of epochs while training\n",
    "        callback\n",
    "            The callback to use while training\n",
    "        save_initial_weights\n",
    "            Whether to save the initial weights for possible later restoration\n",
    "        verbose\n",
    "            The verbosity level for Keras\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        n_opt\n",
    "            The number of times `compile_fit` will be called, given\n",
    "            potentially multiple loss functions and / or learning rates.\n",
    "        \n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        self._history = []\n",
    "        \n",
    "        # Using a special list structure allows to use\n",
    "        # a varying number of compile parameters\n",
    "        self.loss = make_list(loss, cls=KeepLast)\n",
    "        self.learning_rate = make_list(learning_rate, cls=KeepLast)\n",
    "        self.n_opt = max(len(d) for d in (self.loss, self.learning_rate))\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.callback = make_list(callback)\n",
    "        self.batchsize = batchsize\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.save_initial_weights = save_initial_weights\n",
    "        self._initial_weights = self.weights if self.save_initial_weights else None\n",
    "    \n",
    "    @property\n",
    "    def weights(self) -> List[np.ndarray]:\n",
    "        \"\"\"The weights of the model layers.\"\"\"\n",
    "        return self._model.get_weights()\n",
    "    \n",
    "    @weights.setter\n",
    "    def weights(self, weights: MaybeListType[np.ndarray]):\n",
    "        self._model.set_weights(weights)\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"Reset the weights to the initialized state.\"\"\"\n",
    "        if not self.save_initial_weights:\n",
    "            raise ValueError(\"Initial weights were not saved!\")\n",
    "        self.weights = self._initial_weights\n",
    "    \n",
    "    def compile(self):\n",
    "        \"\"\"Compiles the neural network model with the parameters set at instantiation.\"\"\"\n",
    "        self._model.compile(\n",
    "            optimizer=Adam(learning_rate=self.learning_rate.pop_first(),\n",
    "                           epsilon=0.0001, clipnorm=1.0, beta_1=0.99),\n",
    "            loss=self.loss.pop_first(),\n",
    "            metrics=self.metric)\n",
    "    \n",
    "    def fit(self, train: NNDataType, valid: NNDataType, **kwargs):\n",
    "        \"\"\"\n",
    "        Trains the neural network with the specified training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train\n",
    "            Training data in the form of a list of inputs and a target output.\n",
    "        valid\n",
    "            Validation data in the form of a list of inputs and a target output.\n",
    "        kwargs\n",
    "            Additional parameters for Keras `fit` or to override object parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Pass the defaults specified in the constructor and\n",
    "        # optionally override them with explicitly passed options.\n",
    "        batchsize = train[-1].shape[0] if self.batchsize == -1 else self.batchsize\n",
    "        options = dict(batch_size=batchsize, epochs=self.epochs,\n",
    "                       verbose=self.verbose, callbacks=self.callback, shuffle=True)\n",
    "        \n",
    "        # We have to filter out None because we would like to\n",
    "        # use the Keras defaults if at all possible.\n",
    "        options = {k: v for k, v in options.items() if v is not None}\n",
    "        options.update(kwargs)\n",
    "        hist = self._model.fit(*train, validation_data=valid, **options)\n",
    "        self._history.append(hist)\n",
    "    \n",
    "    def compile_fit(self, train: NNDataType, valid: NNDataType, **kwargs):\n",
    "        \"\"\"\n",
    "        Compiles and trains the neural network with the specified training data\n",
    "        repeatedly, until all loss functions or learning rates have been used.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train\n",
    "            Training data in the form of a list of inputs and a target output.\n",
    "        valid\n",
    "            Validation data in the form of a list of inputs and a target output.\n",
    "        kwargs\n",
    "            Additional parameters for Keras `fit` or to override object parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        for _ in range(self.n_opt):\n",
    "            self.compile()\n",
    "            hist = self.fit(train, valid, **kwargs)\n",
    "            self._history.append(hist)\n",
    "        \n",
    "    def predict(self, data: MaybeListType[np.ndarray]) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Projects data through the neural network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Input data to predict\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predicted\n",
    "            Predicted data\n",
    "        \n",
    "        \"\"\"\n",
    "        # This is the case when we're predicting the state\n",
    "        # assignments of a biased trajectory\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = [data, data]\n",
    "        elif len(data) == 1:\n",
    "            data = [data[0], data[0]]\n",
    "            \n",
    "        # Specifying batchsize is crucial because\n",
    "        # we have a batch normalization layer in our model!\n",
    "        return self._model.predict(data, batch_size=self.batchsize)\n",
    "    \n",
    "    def predict_dataset(self, data: DataSet, valid: bool=False,\n",
    "                        func: Callable[..., Any]=None) -> DataSet:\n",
    "        \"\"\"\n",
    "        Projects a DataSet through the neural network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Input DataSet to predict\n",
    "        valid\n",
    "            Whether to also process any validation data in the passed data object\n",
    "        func\n",
    "            Optional function to process the raw output of the neural network\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predicted\n",
    "            Predicted data as a DataSet\n",
    "        \n",
    "        \"\"\"\n",
    "        train_pred = self.predict(data.trains)\n",
    "        valid_pred = self.predict(data.valids) if valid else None\n",
    "        \n",
    "        # We will sometimes need to process the output before\n",
    "        # forming a DataSet, e.g. splitting the network lobes\n",
    "        if func is not None:\n",
    "            train_pred = func(train_pred)\n",
    "            if valid_pred is not None:\n",
    "                valid_pred = func(valid_pred)\n",
    "        return DataSet(train_pred, valid_pred)\n",
    "        \n",
    "    def evaluate(self, data: NNDataType) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates the score by passing the output to the specified loss function.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Data to use for evaluation, typically validation data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        score\n",
    "            The loss function score\n",
    "        \n",
    "        \"\"\"\n",
    "        return self._model.evaluate(*data, batch_size=self.batchsize, verbose=self.verbose)\n",
    "    \n",
    "    # TODO: Newer Keras versions should be able to save directly to an HDF5 group\n",
    "    def save(self, group: h5py.Group):\n",
    "        \"\"\"\n",
    "        Save the model to a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename\n",
    "            Path to an HDF5 file\n",
    "        \n",
    "        \"\"\"\n",
    "        for i, weight in enumerate(self._model.get_weights()):\n",
    "            group[str(i)] = weight\n",
    "    \n",
    "    def load(self, group: h5py.Group):\n",
    "        \"\"\"\n",
    "        Load the model from a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename\n",
    "            Path to an HDF5 file\n",
    "        \n",
    "        \"\"\"\n",
    "        n_items = len(group)\n",
    "        self._model.set_weights([\n",
    "            group[str(i)] for i in range(n_items)\n",
    "        ])\n",
    "    \n",
    "    def load_weights(self, filename: MaybePathType):\n",
    "        \"\"\"\n",
    "        Load the model weights from a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename\n",
    "            Path to an HDF5 file\n",
    "        \n",
    "        \"\"\"\n",
    "        filename = handle_path(filename).as_posix()\n",
    "        self._model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:57.661323Z",
     "start_time": "2020-03-21T17:42:57.640687Z"
    }
   },
   "outputs": [],
   "source": [
    "def _build_model(n_input: int, n_output: int, learning_rate: float=1e-4,\n",
    "                 width: int=1024, depth: int=2, regularization: float=1e-8,\n",
    "                 dropout: float=0.0, verbose: int=0, batchnorm: bool=False,\n",
    "                 lr_factor: float=1e-2) -> Dict[str, NNModel]:\n",
    "    \"\"\"\n",
    "    Builds the VAMPNet model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input dimensions\n",
    "    n_output\n",
    "        Number of output dimensions\n",
    "    learning_rate\n",
    "        Learning rate for the chi model\n",
    "    width\n",
    "        Width of the layers in neurons\n",
    "    depth\n",
    "        Depth of the model in layers\n",
    "    regularization\n",
    "        L2 regularization strength per hidden layer\n",
    "    dropout\n",
    "        Dropout per hidden layer\n",
    "    verbose\n",
    "        Verbosity of the vanilla model\n",
    "    lr_factor\n",
    "        Learning rate modifier for the `all` model\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    models\n",
    "        Dictionary of all models and the u and S layers\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Input layers\n",
    "    xti = Input(shape=(n_input,))\n",
    "    xli = Input(shape=(n_input,))\n",
    "\n",
    "    # Create hidden layers\n",
    "    dense = []\n",
    "    for i in range(depth):\n",
    "        layer = Dense(\n",
    "            units=width, activation=KoopmanModel.activation,\n",
    "            kernel_regularizer=regularizers.l2(regularization),\n",
    "            kernel_initializer=KoopmanModel.initializer)\n",
    "        dense.append(layer)\n",
    "        if i <= depth - 1 and dropout > 0.0:\n",
    "            dense.append(AlphaDropout(dropout))\n",
    "\n",
    "    # Optional batch normalization\n",
    "    bn = BatchNormalization()\n",
    "    lx = bn(xti) if batchnorm else xti\n",
    "    rx = bn(xli) if batchnorm else xli\n",
    "    for i, layer in enumerate(dense):\n",
    "        lx = dense[i](lx)\n",
    "        rx = dense[i](rx)\n",
    "\n",
    "    # Output is softmax for [0, 1] interval\n",
    "    softmax = Dense(\n",
    "        units=n_output, activation=\"softmax\",\n",
    "        kernel_regularizer=regularizers.l2(0.1 * regularization),\n",
    "        kernel_initializer=KoopmanModel.initializer)\n",
    "    lx = softmax(lx)\n",
    "    rx = softmax(rx)\n",
    "\n",
    "    # Build the model\n",
    "    merged = concatenate([lx, rx])\n",
    "    models = {}\n",
    "    models[\"chi\"] = NNModel(\n",
    "        Model(inputs=[xti, xli], outputs=merged),\n",
    "        loss=[vamp._loss_VAMP_sym, vamp.loss_VAMP2_autograd],\n",
    "        metric=[vamp.metric_VAMP],\n",
    "        learning_rate=[learning_rate * f for f in (1.0, 0.02)],\n",
    "        callback=EarlyStopping(\"val_metric_VAMP\", mode=\"max\", min_delta=0.001,\n",
    "                               patience=5, restore_best_weights=True),\n",
    "        verbose=0)\n",
    "\n",
    "    # Auxiliary inputs\n",
    "    chil_in, chir_in = [Input(shape=(n_output,)) for _ in range(2)]\n",
    "    v_in = Input(shape=(n_output, 1))\n",
    "    C00_in, C01_in, C11_in, sigma_in = [\n",
    "        Input(shape=(n_output, n_output)) for _ in range(4)]\n",
    "\n",
    "    # Constraint layers\n",
    "    vlu = VAMPu(n_output, activation=tf.exp)\n",
    "    vls = VAMPS(n_output, activation=tf.exp, renorm=True)\n",
    "\n",
    "    # In / Output for full model\n",
    "    (u_out, v_out, C00_out, C11_out,\n",
    "     C01_out, sigma_out, mu_out) = vlu([lx, rx])\n",
    "    Ve_out, K_out, p_out, S_out = vls([\n",
    "        lx, rx, u_out, v_out, C00_out,\n",
    "        C11_out, C01_out, sigma_out])\n",
    "\n",
    "    # In / Output for model with only u and S\n",
    "    (u_out_b, v_out_b, C00_out_b, C11_out_b,\n",
    "     C01_out_b, sigma_out_b, _) = vlu([chil_in, chir_in])\n",
    "    Ve_out_b, K_out_b, p_out_b, S_out_b = vls([\n",
    "        chil_in, chir_in, u_out_b, v_out_b,\n",
    "        C00_out_b, C11_out_b, C01_out_b, sigma_out_b])\n",
    "\n",
    "    # In / Output for new tau prediction model\n",
    "    Ve_out_s, *_ = vls([v_in, C00_in, C11_in, C01_in, sigma_in])\n",
    "\n",
    "    # We will need these layers later, so we save them as models\n",
    "    models[\"vlu\"] = NNModel(vlu, save_initial_weights=True)\n",
    "    models[\"vls\"] = NNModel(vls, save_initial_weights=True)\n",
    "\n",
    "    # Build training models, we need to be very careful with batchsizes:\n",
    "    # https://github.com/keras-team/keras/issues/12400\n",
    "    early = EarlyStopping(\"val_loss\", patience=10, mode=\"min\", restore_best_weights=True)\n",
    "    models[\"all\"] = NNModel(\n",
    "        Model(inputs=[xti, xli], outputs=Ve_out),\n",
    "        loss=loss_vampe, learning_rate=learning_rate * lr_factor,\n",
    "        callback=early, epochs=KoopmanModel.n_epoch_aux)\n",
    "    models[\"both\"] = NNModel(\n",
    "        Model(inputs=[chil_in, chir_in], outputs=Ve_out_b),\n",
    "        loss=loss_vampe, learning_rate=5e-4,\n",
    "        callback=early, epochs=KoopmanModel.n_epoch_aux, batchsize=-1)\n",
    "    models[\"S\"] = NNModel(\n",
    "        Model(inputs=[v_in, C00_in, C11_in, C01_in, sigma_in], outputs=Ve_out_s),\n",
    "        loss=loss_vampe, learning_rate=0.1,\n",
    "        callback=early, epochs=KoopmanModel.n_epoch_aux, batchsize=-1)\n",
    "\n",
    "    # Build prediction models\n",
    "    models[\"inp\"] = NNModel(Model(inputs=[chil_in, chir_in], outputs=[\n",
    "            v_out_b, C00_out_b, C11_out_b, C01_out_b, sigma_out_b]))\n",
    "    models[\"mu\"] = NNModel(Model(inputs=[xti, xli], outputs=mu_out))\n",
    "    models[\"K\"] = NNModel(Model(inputs=[xti, xli], outputs=K_out))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:44:17.931518Z",
     "start_time": "2020-03-21T17:44:17.880875Z"
    }
   },
   "outputs": [],
   "source": [
    "class KoopmanModel:\n",
    "    activation = \"selu\"\n",
    "    initializer = \"lecun_normal\"\n",
    "    n_epoch_chi = 100\n",
    "    n_epoch_aux = 10000\n",
    "    \n",
    "    def __init__(self, n: int, network_lag: int=4, constrained: bool=True,\n",
    "                 verbose: int=0, nnargs: Dict[str, Any]=None):\n",
    "        \"\"\"\n",
    "        Provides Koopman model training methods.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n\n",
    "            Network output size\n",
    "        network_lag\n",
    "            Training lag for input frame pairs\n",
    "        constrained\n",
    "            If we're doing the constrained version for reversibility\n",
    "        verbose\n",
    "            Output verbosity\n",
    "        nnargs\n",
    "            Arguments passed to the neural network constructor\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        chi_estimated\n",
    "            True if the VAMPNet part of the model has been estimated\n",
    "        aux_estimated\n",
    "            True if the constraint part of the model has been estimated\n",
    "        generator\n",
    "            Data generator containing the full dataset\n",
    "        data\n",
    "            Data used to train and validate the neural network\n",
    "        \n",
    "        \"\"\"\n",
    "        self.n_output = n\n",
    "        self.n_input = None\n",
    "        self.network_lag = network_lag\n",
    "        self.verbose = verbose\n",
    "        self.constrained = constrained\n",
    "        self.nnargs = nnargs\n",
    "        self.chi_estimated = False\n",
    "        self.aux_estimated = False\n",
    "        \n",
    "        self._lag = network_lag\n",
    "        self._reestimated = False\n",
    "        self._models = None\n",
    "        self._chi_weights = None\n",
    "        \n",
    "        # Training and validation data\n",
    "        self._generator = None\n",
    "        self.data = None\n",
    "        \n",
    "        # Results\n",
    "        self._K = None\n",
    "        self._pi = None\n",
    "        self._mu = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, filename: MaybePathType) -> \"KoopmanModel\":\n",
    "        \"\"\"\n",
    "        Initialize from a saved model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename\n",
    "            HDF5 file with the saved model\n",
    "        \n",
    "        \"\"\"\n",
    "        koop = cls(n=-1)\n",
    "        koop.load(handle_path(filename))\n",
    "        return koop\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return \"<KoopmanModel with shape=({0},{1}), lag={2}, n={3}>\".format(\n",
    "            self.nnargs[\"width\"], self.nnargs[\"depth\"], self.network_lag, self.n_output,\n",
    "            \", estimated\" if self.chi_estimated else \"\")\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"\n",
    "        Cleanup model graph after training. Without this\n",
    "        the kernel will die due to running out of memory.\n",
    "        \n",
    "        \"\"\"\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    \n",
    "    @property\n",
    "    def is_built(self) -> bool:\n",
    "        return self._models is not None\n",
    "        \n",
    "    @property\n",
    "    def K(self) -> np.ndarray:\n",
    "        \"\"\"The estimated Koopman operator.\"\"\"\n",
    "        if self._K is None or self._reestimated:\n",
    "            self._K = self._models[\"K\"].predict(self.data.trains)[FIRST]\n",
    "        return self._K\n",
    "    \n",
    "    @property\n",
    "    def mu(self) -> np.ndarray:\n",
    "        \"\"\"The estimated mu.\"\"\"\n",
    "        if self._mu is None or self._reestimated:\n",
    "            self._mu = self._models[\"mu\"].predict(self.data.trains).flatten()\n",
    "        return self._mu\n",
    "    \n",
    "    @property\n",
    "    def pi(self) -> np.ndarray:\n",
    "        \"\"\"The estimated equilibrium distribution.\"\"\"\n",
    "        return statdist(self.K)\n",
    "    \n",
    "    # TODO Add reweighting code here\n",
    "    @pi.setter\n",
    "    def pi(self):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def dt(self) -> float:\n",
    "        \"\"\"The timestep of the underlying data.\"\"\"\n",
    "        return self.generator.dt\n",
    "    \n",
    "    @property\n",
    "    def generator(self) -> DataGenerator:\n",
    "        return self._generator\n",
    "    \n",
    "    @generator.setter\n",
    "    def generator(self, generator: DataGenerator):\n",
    "        self._generator = generator\n",
    "        self.n_input = generator.n_dims\n",
    "        self.data = self.generator(self.n_output, self.network_lag)\n",
    "    \n",
    "    @property\n",
    "    def lag(self) -> int:\n",
    "        \"\"\"The model lag time.\"\"\"\n",
    "        return self._lag\n",
    "    \n",
    "    @lag.setter\n",
    "    def lag(self, lag: int):\n",
    "        \"\"\"\n",
    "        Update the model lag time for ITS calculation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lag\n",
    "            Lag time to update the model to\n",
    "        \n",
    "        \"\"\"\n",
    "        del self.data\n",
    "        self.data = self.generator(self.n_output, lag)\n",
    "        self._models[\"vls\"].reset_weights()\n",
    "        chi_data = self._update_auxiliary_weights(\n",
    "            optimize_u=False, optimize_S=True, reset_weights=False)\n",
    "        \n",
    "        # Prepare training data\n",
    "        s_data = self._models[\"inp\"].predict_dataset(chi_data, valid=True)[FIRST]\n",
    "        s_data.n = 1\n",
    "        \n",
    "        # Train auxiliary and full model\n",
    "        self._models[\"S\"].compile_fit(s_data.train, s_data.valid)\n",
    "        self._models[\"both\"].compile_fit(chi_data.train, chi_data.valid)\n",
    "        self._lag = lag\n",
    "        \n",
    "        # Make sure we recompute any observables\n",
    "        self._reestimated = True\n",
    "    \n",
    "    def reset_lag(self):\n",
    "        \"\"\"Reset the model to the original lag time.\"\"\"\n",
    "        self.lag = self.network_lag\n",
    "    \n",
    "    def _log(self, msg: str, update: bool=False):\n",
    "        \"\"\"Log current status if verbose is set.\"\"\"\n",
    "        if self.verbose:\n",
    "            if update:\n",
    "                print(msg, end=\"\\r\")\n",
    "            else:\n",
    "                print(msg)\n",
    "    \n",
    "    def _update_auxiliary_weights(self, optimize_u: bool=True, optimize_S: bool=False,\n",
    "                                  reset_weights: bool=True) -> DataSet:\n",
    "        \"\"\"\n",
    "        Update the weights for the auxiliary model and return new output\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        optimize_u\n",
    "            Whether to optimize the u vector\n",
    "        optimize_S\n",
    "            Whether to optimize the S matrix\n",
    "        reset_weights\n",
    "            Whether to reset the weights for the vanilla VAMPNet model\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        chi\n",
    "            New training and validation assignments\n",
    "        \n",
    "        \"\"\"\n",
    "        if reset_weights:\n",
    "            self._models[\"chi\"].weights = self._chi_weights\n",
    "        \n",
    "        # Project training data\n",
    "        chi_data = self._models[\"chi\"].predict_dataset(self.data, func=_split)\n",
    "\n",
    "        # Set weights for u vector\n",
    "        C0inv, Ctau = covariances(chi_data.trains)\n",
    "        K = C0inv @ Ctau\n",
    "        if optimize_u:\n",
    "            pi = _compute_pi(K)\n",
    "            self._models[\"vlu\"].weights = [np.log(np.abs(C0inv @ pi))]\n",
    "        \n",
    "        # Optionally set weights for S matrix\n",
    "        if optimize_S:\n",
    "            *_, sigma = self._models[\"inp\"].predict(chi_data.trains)\n",
    "            sigma_inv = matrix_inverse(sigma[FIRST])\n",
    "            S_nonrev = K @ sigma_inv\n",
    "            S_rev = 0.5 * (S_nonrev + S_nonrev.T)\n",
    "            self._models[\"vls\"].weights = [np.log(np.abs(0.5 * S_rev))]\n",
    "        \n",
    "        # Project training and validation data with new weights\n",
    "        chi_data = self._models[\"chi\"].predict_dataset(self.data, valid=True, func=_split)\n",
    "        chi_data.n = 2 * self.n_output\n",
    "        return chi_data\n",
    "            \n",
    "    def train_model(self):\n",
    "        \"\"\"Train the vanilla VAMPNet model.\"\"\"\n",
    "        self._models[\"chi\"].compile_fit(self.data.train, self.data.valid)\n",
    "        self._chi_weights = self._models[\"chi\"].weights\n",
    "        self.chi_estimated = True\n",
    "    \n",
    "    def train_auxiliary_model(self):\n",
    "        \"\"\"Train the auxiliary constraint model.\"\"\"\n",
    "        \n",
    "        # Needs a trained vanilla VAMPNet\n",
    "        assert self.chi_estimated\n",
    "        \n",
    "        # Set initial weights\n",
    "        self._log(\"Setting initial auxiliary weights...\")\n",
    "        chi_data = self._update_auxiliary_weights(optimize_S=True)\n",
    "        \n",
    "        # Train auxiliary models only\n",
    "        self._log(\"Training auxiliary network...\")\n",
    "        self._models[\"both\"].compile_fit(chi_data.train, chi_data.valid)\n",
    "        \n",
    "        # Train the whole network\n",
    "        self._log(\"Training full network...\")\n",
    "        self._models[\"all\"].compile_fit(self.data.train, self.data.valid)\n",
    "        self.aux_estimated = True\n",
    "    \n",
    "    def train_loop(self):\n",
    "        \"\"\"Train the auxiliary constraint model in a loop.\"\"\"\n",
    "        # Needs a trained vanilla VAMPNet\n",
    "        assert self.chi_estimated and self.aux_estimated\n",
    "        \n",
    "        score = score_prev = 0.0\n",
    "        weights = self._models[\"all\"].weights\n",
    "        \n",
    "        self._log(\"Setting up auxiliary training loop...\")\n",
    "        \n",
    "        self._models[\"both\"].compile()\n",
    "        self._models[\"all\"].compile()\n",
    "        \n",
    "        iteration = 0\n",
    "        while score >= score_prev:\n",
    "            chi_data = self._update_auxiliary_weights(reset_weights=False)\n",
    "            self._models[\"both\"].fit(chi_data.train, chi_data.valid)\n",
    "            self._models[\"all\"].fit(self.data.train, self.data.valid)\n",
    "            score = -self._models[\"all\"].evaluate(self.data.valid)\n",
    "            \n",
    "            self._log(\"Iteration {0}: Score: {1}\".format(iteration, score))\n",
    "            \n",
    "            if score > score_prev:\n",
    "                score_prev = score\n",
    "                weights = self._models[\"all\"].weights\n",
    "            iteration += 1\n",
    "        \n",
    "        # Final fit\n",
    "        self._log(\"Performing final fit...\")\n",
    "        self._models[\"all\"].weights = weights\n",
    "        chi_data = self._update_auxiliary_weights(reset_weights=False)\n",
    "        self._models[\"both\"].fit(chi_data.train, chi_data.valid)\n",
    "\n",
    "    def save(self, filename: MaybePathType):\n",
    "        \"\"\"\n",
    "        Save the model to a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename\n",
    "            Path to an HDF5 file\n",
    "        \n",
    "        \"\"\"\n",
    "        filename = handle_path(filename, non_existent=True)\n",
    "        with h5py.File(filename, \"w\") as write:\n",
    "            \n",
    "            # We need to create a dummy dataset to be able to use `attrs`\n",
    "            par = write.create_dataset(\"parameters\", (0,))\n",
    "            parameters = _get_serializable_attributes(self)\n",
    "            for k, v in parameters.items():\n",
    "                par.attrs[k] = v\n",
    "            \n",
    "            # Save the neural net parameters\n",
    "            nn = write.create_dataset(\"nn\", (0,))\n",
    "            for k, v in self.nnargs.items():\n",
    "                nn.attrs[k] = v\n",
    "            \n",
    "            write.create_group(\"models\")\n",
    "            for k, model in self._models.items():\n",
    "                model_group = \"models/{0}\".format(k)\n",
    "                write.create_group(model_group)\n",
    "                model.save(write[model_group])\n",
    "    \n",
    "    def load(self, filename: MaybePathType):\n",
    "        \"\"\"\n",
    "        Load a model from a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename\n",
    "            Path to an HDF5 file\n",
    "        \n",
    "        \"\"\"\n",
    "        filename = handle_path(filename)\n",
    "        with h5py.File(filename, \"r\") as read:\n",
    "            # TODO: Put this into a mixin instead\n",
    "            self.__dict__.update(dict(read[\"parameters\"].attrs))\n",
    "            self.nnargs = dict(read[\"nn\"].attrs)\n",
    "            self._models = _build_model(\n",
    "                self.n_input, self.n_output, verbose=self.verbose, **self.nnargs)\n",
    "            for k, model in self._models.items():\n",
    "                model.load(read[\"models/{0}\".format(k)])\n",
    "    \n",
    "    def load_chi_weights(self, filename: MaybePathType):\n",
    "        \"\"\"\n",
    "        Load a model from a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename\n",
    "            Path to an HDF5 file\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.is_built:\n",
    "            self._models = _build_model(self.n_input, self.n_output,\n",
    "                                        verbose=self.verbose, **self.nnargs)\n",
    "        filename = handle_path(filename)\n",
    "        self._models[\"chi\"].load_weights(filename)\n",
    "        self._chi_weights = self._models[\"chi\"].weights\n",
    "        self.chi_estimated = True\n",
    "        \n",
    "    def fit(self, generator: DataGenerator):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        generator\n",
    "            Data generator function, must be callable with output size and lag time\n",
    "        \n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self._models = _build_model(self.n_input, self.n_output,\n",
    "                                    verbose=self.verbose, **self.nnargs)\n",
    "        \n",
    "        self.train_model()\n",
    "        \n",
    "        # This is the more compute-intensive part\n",
    "        if self.constrained:\n",
    "            self.train_auxiliary_model()\n",
    "            self.train_loop()\n",
    "    \n",
    "    def transform(self, data: MaybeListType[np.ndarray]) -> MaybeListType[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Transform data through the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Array(s) with input features, no lagged component required\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predicted\n",
    "            Array(s) with probabilistic state assignments\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self.chi_estimated\n",
    "        \n",
    "        # Prediction doesn't care about individual trajectories\n",
    "        if isinstance(data, list):\n",
    "            lengths = [len(d) for d in data]\n",
    "            data = np.vstack(data)\n",
    "            prob = self._models[\"chi\"].predict(data)\n",
    "            return unflatten(prob, lengths=lengths)\n",
    "        \n",
    "        return self._models[\"chi\"].predict(data)\n",
    "    \n",
    "    def fit_transform(self, generator: DataGenerator) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit and transform data through the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Data generator function, must be callable with output size, lag time,\n",
    "            and must have a `flat` attribute for the complete input dataset.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predicted\n",
    "            Array(s) with probabilistic state assignments\n",
    "        \n",
    "        \"\"\"\n",
    "        self.fit(generator)\n",
    "        return self.transform(generator.data_flat)\n",
    "    \n",
    "    def score(self) -> float:\n",
    "        \"\"\"\n",
    "        Validation score of the trained model.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        score\n",
    "            The VAMP-E score of the fully trained model\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self.chi_estimated and self.aux_estimated\n",
    "        return -self._models[\"all\"].evaluate(self.data.valid)\n",
    "    \n",
    "    def its(self, lags: Sequence[int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate implied timescales for a sequence of lag times.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lags\n",
    "            Sequence of lag times to calculate the timescales for\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        its\n",
    "            Implied timescales for different components\n",
    "            (dim 0) and lag times (dim 1).\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self.chi_estimated and self.aux_estimated\n",
    "        its = np.empty((self.n_output - 1, len(lags)))\n",
    "        for i, lag in enumerate(lags):\n",
    "            self._log(\"Computing {0}/{1}...\".format(i + 1, len(lags)), update=True)\n",
    "            \n",
    "            self.lag = lag\n",
    "            lambdas = np.linalg.eigvals(self.K)\n",
    "            lambdas = np.sort(np.abs(np.real(lambdas)))[:LAST]\n",
    "            its[:, i] = -lag * self.dt / np.log(lambdas)\n",
    "        \n",
    "        self.reset_lag()\n",
    "        return its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:58.945912Z",
     "start_time": "2020-03-21T17:42:58.942901Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_vampe(y_true, y_pred):\n",
    "    return tf.linalg.trace(y_pred[FIRST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:42:59.529652Z",
     "start_time": "2020-03-21T17:42:59.525519Z"
    }
   },
   "outputs": [],
   "source": [
    "def statdist(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the equilibrium distribution of a transition matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        Row-stochastic transition matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mu\n",
    "        Stationary distribution, i.e. the left\n",
    "        eigenvector associated with eigenvalue 1.\n",
    "    \n",
    "    \"\"\"\n",
    "    ev, evec = eig(X, left=True, right=False)\n",
    "    mu = evec.T[ev.argmax()]\n",
    "    mu /= mu.sum()\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:43:00.307680Z",
     "start_time": "2020-03-21T17:43:00.298404Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_its(its, lags, dt=1.0):\n",
    "    multi = its.ndim == 3\n",
    "    nits, nlags = its.shape[-2], its.shape[-1]\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    if multi:\n",
    "        itsm = its.mean(axis=0)\n",
    "        cfl, cfu = np.percentile(its, q=(2.5, 97.5), axis=0)\n",
    "    else:\n",
    "        itsm = its\n",
    "    \n",
    "    for i in range(nits):\n",
    "        ax.semilogy(lags * dt, itsm[i], marker=\"o\",\n",
    "                    linestyle=\"dashed\", linewidth=1.5, color=colors[i])\n",
    "        ax.plot(lags * dt, itsm[i], marker=\"o\", linewidth=1.5, color=colors[i])\n",
    "        if multi:\n",
    "            ax.fill_between(lags * dt, cfl[i], cfu[i],\n",
    "                            interpolate=True, color=colors[i], alpha=0.2)\n",
    "    ax.plot(lags * dt, lags * dt, color=\"k\")\n",
    "    ax.fill_between(lags * dt, ax.get_ylim()[0] * np.ones(len(lags)),\n",
    "                    lags * dt, color=\"k\", alpha=0.2)\n",
    "    ax.set_ylim(1, 5000)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(r\"$\\tau$ [ns]\", fontsize=24)\n",
    "    ax.set_ylabel(r\"$t_i$ [ns]\", fontsize=24)\n",
    "    ax.tick_params(labelsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in five rounds of 1024 simulations each, totalling 5119 runs (one simulation failed to run) at 278 K in the $NVT$ ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:43:01.297844Z",
     "start_time": "2020-03-21T17:43:01.271473Z"
    }
   },
   "outputs": [],
   "source": [
    "trajs = (sorted(glob(\"../../data/apo/r1/traj*.xtc\")) +\n",
    "         sorted(glob(\"../../data/apo/r2/traj*.xtc\")) +\n",
    "         sorted(glob(\"../../data/apo/r3/traj*.xtc\")) +\n",
    "         sorted(glob(\"../../data/apo/r4/traj*.xtc\")) +\n",
    "         sorted(glob(\"../../data/apo/r5/traj*.xtc\")))\n",
    "top = \"trajectories/topol.gro\"\n",
    "KBT = 2.311420 # 278 K\n",
    "\n",
    "# This is only really necessary for the residues in the plots\n",
    "topo = md.load_topology(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:43:11.929514Z",
     "start_time": "2020-03-21T17:43:01.984287Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21-03-20 17:43:02 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[0] WARNING  Using all residue pairs with schemes like closest or closest-heavy is very time consuming. Consider reducing the residue pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Obtaining file info', layout=Layout(flex='2'), max=5119.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "feat = pe.coordinates.featurizer(top)\n",
    "feat.add_residue_mindist()\n",
    "inpcon = pe.coordinates.source(trajs, feat)\n",
    "\n",
    "lengths = sort_lengths(inpcon.trajectory_lengths(), [1024, 1023, 1024, 1024, 1024])\n",
    "nframes = inpcon.trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:43:11.942357Z",
     "start_time": "2020-03-21T17:43:11.931559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectories: 5119\n",
      "Frames: 1259172\n",
      "Time: 314.793 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Trajectories: {0}\".format(len(trajs)))\n",
    "print(\"Frames: {0}\".format(nframes))\n",
    "print(\"Time: {0:5.3f} s\".format(inpcon.trajectory_lengths().sum() * 0.00025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAMPNet\n",
    "VAMPNet[1] is composed of two lobes, one reading the system features $\\mathbf{x}$ at a timepoint $t$ and the other after some lag time $\\tau$. In this case the network reads all minimum inter-residue distances (780 values) and sends them through 8 layers with 2048 nodes each. The final layer uses between 2 and 8 *softmax* outputs to yield a state assignment vector $\\chi: \\mathbb{R}^n \\to \\mathcal{S}$ where $\\mathcal{S} = \\{ s \\in \\mathbb{R}^n \\mid 0 \\le s_i \\le 1, \\sum_i^n s_i = 1 \\}$ representing the probability of a state assignment. One lobe thus transforms a system state into a state occupation probability. We can also view this value as a kind of reverse ambiguity, i.e. how sure the network is that the system is part of a certain cluster. These outputs are then used as the input for the VAMP scoring function. It is based on the idea that any approximation of the true Koopman operator will provide an underestimate of it's eigenvalues. We can therefore maximize with regards to our function, and thus find an ideal state decomposition.\n",
    "\n",
    "[1] Mardt, A., Pasquali, L., Wu, H. & No, F. VAMPnets for deep learning of molecular kinetics. Nat Comms 111 (2017). doi:10.1038/s41467-017-02388-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) and first normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:43:13.306756Z",
     "start_time": "2020-03-21T17:43:11.943978Z"
    }
   },
   "outputs": [],
   "source": [
    "input_flat = np.load(\"intermediate/mindist-780.npy\")\n",
    "input_data = unflatten(input_flat, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters\n",
    "To allow for a larger hyperparameter search space, we use the self-normalizing neural network approach by Klambauer *et al.* [2], thus using SELU units, `AlphaDropout` and normalized `LeCun` weight initialization. The other hyperparameters are defined at the beginning of this notebook.\n",
    "\n",
    "[2] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-Normalizing Neural Networks. arXiv.org cs.LG, (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:43:13.312921Z",
     "start_time": "2020-03-21T17:43:13.308512Z"
    }
   },
   "outputs": [],
   "source": [
    "activation = \"selu\"              # NN activation function\n",
    "init = \"lecun_normal\"            # NN weight initialization\n",
    "lag = 20                         # Lag time\n",
    "n_epoch = 100                    # Max. number of epochs\n",
    "n_epoch_s = 10000                # Max. number of epochs for S optimization\n",
    "n_batch = 5000                   # Training batch size\n",
    "n_dims = input_data[0].shape[1]  # Input dimension\n",
    "nres = 42                        # Number of residues\n",
    "epsilon = 1e-7                   # Floating point noise\n",
    "dt = 0.25                        # Trajectory timestep in ns\n",
    "\n",
    "outsizes = np.array([2, 3, 4, 5, 6, 7, 8])\n",
    "lags = np.array([1, 2, 5, 10, 20, 50, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "We run the training several times with different train/test splits to get an error estimate, this is referred to as bootstrap aggregating (*bagging*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:43:28.179344Z",
     "start_time": "2020-03-21T17:43:27.954758Z"
    }
   },
   "outputs": [],
   "source": [
    "generators = [DataGenerator(input_data, ratio=ratio, dt=dt, max_frames=1000000) for _ in range(n_runs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:43:37.539233Z",
     "start_time": "2020-03-21T17:43:37.530802Z"
    }
   },
   "outputs": [],
   "source": [
    "space = [\n",
    "    sp.Categorical([2, 4, 6, 8], name=\"depth\"),\n",
    "    sp.Categorical([128, 512, 1024], name=\"width\"),\n",
    "    sp.Categorical([1e-1, 1e-2, 1e-3], name=\"learning_rate\"),\n",
    "    sp.Categorical([1e-6, 1e-8, 1e-10], name=\"regularization\"),\n",
    "    sp.Categorical([0.0, 0.1], name=\"dropout\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T17:43:49.857882Z",
     "start_time": "2020-03-21T17:43:49.850619Z"
    }
   },
   "outputs": [],
   "source": [
    "@skopt.utils.use_named_args(space)\n",
    "def test_model(**space):\n",
    "    depth, width, learning_rate, regularization, dropout = (\n",
    "        space[\"depth\"], space[\"width\"], space[\"learning_rate\"],\n",
    "        space[\"regularization\"], space[\"dropout\"])\n",
    "    print(\"Parameters: {0}\".format(space))\n",
    "    width, depth = int(width), int(depth)\n",
    "    \n",
    "    scores = np.full(n_runs, np.nan)\n",
    "    for i in range(n_runs):\n",
    "        koop = KoopmanModel(n, verbose=1, network_lag=lag, nnargs=dict(\n",
    "            width=width, depth=depth, learning_rate=learning_rate,\n",
    "            regularization=regularization, dropout=dropout,\n",
    "            batchnorm=True, lr_factor=1e-2))\n",
    "        try:\n",
    "            koop.fit(generators[i])\n",
    "            scores[i] = koop.score()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            del koop\n",
    "            gc.collect()\n",
    "    \n",
    "    return np.nanmean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-21T17:44:35.295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new checkpoint file...\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Parameters: {'width': 1024, 'regularization': 1e-06, 'depth': 6, 'learning_rate': 0.001, 'dropout': 0.0}\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "Epoch 1/100\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted: OOM when allocating tensor with shape[5000,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node dense/MatMul}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[metrics/metric_VAMP/Identity/_149]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted: OOM when allocating tensor with shape[5000,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node dense/MatMul}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = \"temp-aux-0.pkl\"\n",
    "chkp = CheckpointSaver(checkpoint_file)\n",
    "if not os.path.exists(checkpoint_file):\n",
    "    print(\"Creating new checkpoint file...\")\n",
    "    res = skopt.dummy_minimize(\n",
    "        test_model,\n",
    "        space,\n",
    "        n_calls=nevals,\n",
    "        callback=[chkp],\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Checkpoint file exists, continuing optimization...\")\n",
    "    res = skopt.load(checkpoint_file)\n",
    "    res = skopt.dummy_minimize(\n",
    "        test_model,\n",
    "        space,\n",
    "        x0=res.x_iters,\n",
    "        y0=res.func_vals,\n",
    "        n_calls=nevals,\n",
    "        callback=[chkp],\n",
    "        verbose=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
