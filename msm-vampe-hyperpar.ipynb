{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill params\n",
    "nevals = 100         # Number of hyperparameter iterations\n",
    "n = 4                # Output size\n",
    "ratio = 0.9          # Train-Test split ratio\n",
    "n_runs = 3           # Number of times to run with the same parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimisation\n",
    "\n",
    "In this notebook we set up the neural networks and evaluate different hyperparameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import space as sp\n",
    "from skopt.plots import plot_convergence, plot_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointSaver(object):\n",
    "    \"\"\"\n",
    "    Save current state after each iteration with `skopt.dump`.\n",
    "    Example usage:\n",
    "        import skopt\n",
    "        checkpoint_callback = skopt.callbacks.CheckpointSaver(\"./result.pkl\")\n",
    "        skopt.gp_minimize(obj_fun, dims, callback=[checkpoint_callback])\n",
    "    Parameters\n",
    "    ----------\n",
    "    * `checkpoint_path`: location where checkpoint will be saved to;\n",
    "    * `dump_options`: options to pass on to `skopt.dump`, like `compress=9`\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path, **dump_options):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.dump_options = dump_options\n",
    "\n",
    "    def __call__(self, res):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        * `res` [`OptimizeResult`, scipy object]:\n",
    "            The optimization as a OptimizeResult object.\n",
    "        \"\"\"\n",
    "        skopt.utils.dump(res, self.checkpoint_path, **self.dump_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Trajectories\n",
    "Trajectories were acquired in five rounds of 1024 simulations each, totalling 5119 runs (one simulation failed to run) at 278 K in the $NVT$ ensemble. Postprocessing involved removing water, subsampling to 250 ps timesteps, and making molecules whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs = sorted(glob(\"trajectories/red/r?/traj*.xtc\"))\n",
    "top = \"trajectories/red/topol.gro\"\n",
    "KBT = 2.311420 # 278 K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minimum distances as features for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat = pe.coordinates.featurizer(top)\n",
    "feat.add_residue_mindist()\n",
    "inpcon = pe.coordinates.source(trajs, feat)\n",
    "\n",
    "# Switch for full version:\n",
    "# lengths = sort_lengths(inpcon.trajectory_lengths(), [1024, 1023, 1024, 1024, 1024])\n",
    "lengths = [inpcon.trajectory_lengths()]\n",
    "nframes = inpcon.trajectory_lengths().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trajectories: {0}\".format(len(trajs)))\n",
    "print(\"Frames: {0}\".format(nframes))\n",
    "print(\"Time: {0:5.3f} µs\".format(inpcon.trajectory_lengths().sum() * 0.00025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAMPNet\n",
    "VAMPNet[1] is composed of two lobes, one reading the system features $\\mathbf{x}$ at a timepoint $t$ and the other after some lag time $\\tau$. In this case the network reads all minimum inter-residue distances (780 values) and sends them through 5 layers with 256 nodes each. The final layer uses between 2 and 6 *softmax* outputs to yield a state assignment vector $\\chi: \\mathbb{R}^m \\to \\Delta^{n}$ where $\\Delta^{n} = \\{ s \\in \\mathbb{R}^n \\mid 0 \\le s_i \\le 1, \\sum_i^n s_i = 1 \\}$ representing the probability of a state assignment. One lobe thus transforms a system state into a state occupation probability. We can also view this value as a kind of reverse ambiguity, i.e. how sure the network is that the system is part of a certain cluster. These outputs are then used as the input for the VAMP scoring function. We use the new enhanced version with physical constraints[2], particularly the ones for positive entries and reversibility.\n",
    "\n",
    "[1] Mardt, A., Pasquali, L., Wu, H. & Noé, F. VAMPnets for deep learning of molecular kinetics. Nat Comms 1–11 (2017). doi:10.1038/s41467-017-02388-1\n",
    "\n",
    "[2] Mardt, A., Pasquali, L., Noé, F. & Wu, H. Deep learning Markov and Koopman models with physical constraints. arXiv:1912.07392 [physics] (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We use minimum residue distances as input ($\\frac{N(N-1)}{2}$ values, where $N$ is the number of residues) and first normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"intermediate/mindist-780-red.npy\"\n",
    "if not os.path.exists(filename):\n",
    "    print(\"No mindist file for red ensemble, calculating from scratch...\")\n",
    "    con = np.vstack(inpcon.get_output())\n",
    "    np.save(filename, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = np.load(\"intermediate/mindist-780-red.npy\")\n",
    "raw_mean, raw_std = raw.mean(axis=0), raw.std(axis=0)\n",
    "input_flat = (raw - raw_mean) / raw_std\n",
    "input_data = [(r - raw_mean) / raw_std for r in unflatten(raw, lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters\n",
    "To allow for a larger hyperparameter search space, we use the self-normalizing neural network approach by Klambauer *et al.* [2], thus using SELU units, `AlphaDropout` and normalized `LeCun` weight initialization. The other hyperparameters are defined at the beginning of this notebook.\n",
    "\n",
    "[2] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-Normalizing Neural Networks. arXiv.org cs.LG, (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 20                         # Lag time\n",
    "n_dims = input_data[0].shape[1]  # Input dimension\n",
    "nres = 42                        # Number of residues\n",
    "epsilon = 1e-7                   # Floating point noise\n",
    "dt = 0.25                        # Trajectory timestep in ns\n",
    "bs_frames = 1000000              # Number of frames to use\n",
    "\n",
    "outsizes = np.array([2, 3, 4, 5, 6])\n",
    "lags = np.array([1, 2, 5, 10, 20, 50, 100])\n",
    "\n",
    "# Comment for full version:\n",
    "bs_frames = nframes\n",
    "attempts = 2\n",
    "outsizes = np.array([4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "We run the training several times with different train/test splits to get an error estimate, this is referred to as bootstrap aggregating (*bagging*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = [DataGenerator(input_data, ratio=ratio, dt=dt, max_frames=bs_frames) for _ in range(n_runs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [\n",
    "    sp.Categorical([2, 4, 6, 8], name=\"depth\"),\n",
    "    sp.Categorical([128, 512, 1024], name=\"width\"),\n",
    "    sp.Categorical([1e-1, 1e-2, 1e-3], name=\"learning_rate\"),\n",
    "    sp.Categorical([1e-6, 1e-8, 1e-10], name=\"regularization\"),\n",
    "    sp.Categorical([0.0, 0.1], name=\"dropout\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@skopt.utils.use_named_args(space)\n",
    "def test_model(**space):\n",
    "    depth, width, learning_rate, regularization, dropout = (\n",
    "        space[\"depth\"], space[\"width\"], space[\"learning_rate\"],\n",
    "        space[\"regularization\"], space[\"dropout\"])\n",
    "    print(\"Parameters: {0}\".format(space))\n",
    "    width, depth = int(width), int(depth)\n",
    "    \n",
    "    scores = np.full(n_runs, np.nan)\n",
    "    for i in range(n_runs):\n",
    "        koop = KoopmanModel(n, verbose=1, network_lag=lag, nnargs=dict(\n",
    "            width=width, depth=depth, learning_rate=learning_rate,\n",
    "            regularization=regularization, dropout=dropout,\n",
    "            batchnorm=True, lr_factor=5e-3))\n",
    "        try:\n",
    "            koop.fit(generators[i])\n",
    "            scores[i] = koop.score()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            del koop\n",
    "            gc.collect()\n",
    "    \n",
    "    return np.nanmean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = \"hyperpar-0.pkl\"\n",
    "chkp = CheckpointSaver(checkpoint_file)\n",
    "if not os.path.exists(checkpoint_file):\n",
    "    print(\"Creating new checkpoint file...\")\n",
    "    res = skopt.dummy_minimize(\n",
    "        test_model,\n",
    "        space,\n",
    "        n_calls=nevals,\n",
    "        callback=[chkp],\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Checkpoint file exists, continuing optimization...\")\n",
    "    res = skopt.load(checkpoint_file)\n",
    "    res = skopt.dummy_minimize(\n",
    "        test_model,\n",
    "        space,\n",
    "        x0=res.x_iters,\n",
    "        y0=res.func_vals,\n",
    "        n_calls=nevals,\n",
    "        callback=[chkp],\n",
    "        verbose=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
